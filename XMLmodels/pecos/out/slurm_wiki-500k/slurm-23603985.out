wiki-500k
UUID is 2024-05-16-14-04-12
--- start training of bert1 ---
05/16/2024 14:05:55 - INFO - __main__ - Setting random seed 0
05/16/2024 14:06:04 - INFO - __main__ - Loaded training feature matrix with shape=(1779881, 2381304)
05/16/2024 14:06:05 - INFO - __main__ - Loaded training label matrix with shape=(1779881, 501070)
05/16/2024 14:06:32 - INFO - __main__ - Loaded 1779881 training sequences
05/16/2024 14:11:32 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [64, 512, 4096, 32768, 501070]
05/16/2024 14:11:32 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[64, 512, 4096, 32768]
05/16/2024 14:11:33 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=64, avr_M_nnz=64
05/16/2024 14:11:39 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
05/16/2024 14:11:39 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=1779881 truncation=128*****
05/16/2024 14:21:20 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=581.7251493930817 *****
05/16/2024 14:32:30 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23603985/tmpuixe5hi6/X_trn.pt
05/16/2024 14:32:32 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 8
05/16/2024 14:33:20 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/16/2024 14:33:20 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1779881
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/16/2024 14:33:21 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/16/2024 14:33:21 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 1779881
05/16/2024 14:33:21 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 64
05/16/2024 14:33:21 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
05/16/2024 14:33:21 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/16/2024 14:33:21 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 256
05/16/2024 14:33:21 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/16/2024 14:33:21 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 10000
Traceback (most recent call last):
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/train.py", line 564, in <module>
    do_train(args)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/train.py", line 548, in do_train
    xtf = XTransformer.train(
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/model.py", line 447, in train
    res_dict = TransformerMatcher.train(
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/matcher.py", line 1382, in train
    matcher.fine_tune_encoder(prob, val_prob=val_prob, val_csr_codes=val_csr_codes)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/matcher.py", line 1075, in fine_tune_encoder
    for batch_cnt, batch in enumerate(train_dataloader):
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 435, in __iter__
    return self._get_iterator()
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 381, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1034, in __init__
    w.start()
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
OSError: [Errno 12] Cannot allocate memory
--- start prediction of bert1 ---
Traceback (most recent call last):
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/predict.py", line 176, in <module>
    do_predict(args)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/predict.py", line 145, in do_predict
    xtf = XTransformer.load(args.model_folder)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/model.py", line 195, in load
    raise ValueError(f"load dir does not exist at: {load_dir}")
ValueError: load dir does not exist at: ./trained-models/wiki-500k/bert1
--- start evaluation of bert1 ---
Traceback (most recent call last):
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xlinear/evaluate.py", line 72, in <module>
    do_evaluation(args)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xlinear/evaluate.py", line 63, in do_evaluation
    Y_pred = smat_util.load_matrix(args.pred_path).tocsr()
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/utils/smat_util.py", line 117, in load_matrix
    mat = np.load(src)
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: './predictions/wiki-500k/2024-05-16-14-04-12/bert1'

============================= JOB FEEDBACK =============================

NodeName=uc2n517
Job ID: 23603985
Cluster: uc2
User/Group: ul_ruw26/ul_student
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 80
CPU Utilized: 13:57:10
CPU Efficiency: 26.60% of 2-04:26:40 core-walltime
Job Wall-clock time: 00:39:20
Memory Utilized: 689.08 GB
Memory Efficiency: 93.83% of 734.38 GB
