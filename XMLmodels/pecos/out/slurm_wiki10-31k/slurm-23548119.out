wiki10-31k
UUID is 2024-05-06-17-10-03
--- start training of bert ---
05/06/2024 17:10:05 - INFO - __main__ - Setting random seed 0
05/06/2024 17:10:05 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
05/06/2024 17:10:05 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
05/06/2024 17:10:05 - INFO - __main__ - Loaded 14146 training sequences
05/06/2024 17:10:10 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
05/06/2024 17:10:10 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
05/06/2024 17:10:10 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
05/06/2024 17:10:10 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
05/06/2024 17:10:10 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
05/06/2024 17:10:23 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=12.642634630203247 *****
05/06/2024 17:10:31 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23548119/tmp2ajn5kus/X_trn.pt
05/06/2024 17:10:31 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/06/2024 17:10:32 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/06/2024 17:10:32 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/06/2024 17:10:32 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/06/2024 17:10:32 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/06/2024 17:10:32 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
05/06/2024 17:10:32 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 3
05/06/2024 17:10:32 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/06/2024 17:10:32 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/06/2024 17:10:32 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/06/2024 17:10:32 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 1000
05/06/2024 17:10:53 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][    50/  1000] |   49/ 443 batches | ms/batch 320.2926 | train_loss 9.777567e-01 | lr 2.500000e-05
05/06/2024 17:11:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   100/  1000] |   99/ 443 batches | ms/batch 308.3545 | train_loss 4.654887e-01 | lr 5.000000e-05
05/06/2024 17:11:25 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   150/  1000] |  149/ 443 batches | ms/batch 309.6740 | train_loss 3.838973e-01 | lr 4.722222e-05
05/06/2024 17:11:42 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   200/  1000] |  199/ 443 batches | ms/batch 310.7721 | train_loss 3.344387e-01 | lr 4.444444e-05
05/06/2024 17:11:42 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmpxr5bpfc_ at global_step 200 ****
05/06/2024 17:11:42 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:11:59 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   250/  1000] |  249/ 443 batches | ms/batch 311.8627 | train_loss 3.050309e-01 | lr 4.166667e-05
05/06/2024 17:12:15 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   300/  1000] |  299/ 443 batches | ms/batch 312.5350 | train_loss 2.908817e-01 | lr 3.888889e-05
05/06/2024 17:12:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   350/  1000] |  349/ 443 batches | ms/batch 312.8286 | train_loss 2.749389e-01 | lr 3.611111e-05
05/06/2024 17:12:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   400/  1000] |  399/ 443 batches | ms/batch 313.1848 | train_loss 2.737701e-01 | lr 3.333333e-05
05/06/2024 17:12:48 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmpxr5bpfc_ at global_step 400 ****
05/06/2024 17:12:49 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:13:09 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   450/  1000] |    6/ 443 batches | ms/batch 307.5466 | train_loss 2.591828e-01 | lr 3.055556e-05
05/06/2024 17:13:26 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   500/  1000] |   56/ 443 batches | ms/batch 313.4382 | train_loss 2.546116e-01 | lr 2.777778e-05
05/06/2024 17:13:42 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   550/  1000] |  106/ 443 batches | ms/batch 313.7549 | train_loss 2.518338e-01 | lr 2.500000e-05
05/06/2024 17:13:59 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   600/  1000] |  156/ 443 batches | ms/batch 314.0279 | train_loss 2.548783e-01 | lr 2.222222e-05
05/06/2024 17:13:59 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmpxr5bpfc_ at global_step 600 ****
05/06/2024 17:14:00 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:14:16 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   650/  1000] |  206/ 443 batches | ms/batch 314.1574 | train_loss 2.486589e-01 | lr 1.944444e-05
05/06/2024 17:14:33 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   700/  1000] |  256/ 443 batches | ms/batch 314.3431 | train_loss 2.448264e-01 | lr 1.666667e-05
05/06/2024 17:14:49 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   750/  1000] |  306/ 443 batches | ms/batch 314.5739 | train_loss 2.384545e-01 | lr 1.388889e-05
05/06/2024 17:15:06 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   800/  1000] |  356/ 443 batches | ms/batch 314.8251 | train_loss 2.450247e-01 | lr 1.111111e-05
05/06/2024 17:15:06 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmpxr5bpfc_ at global_step 800 ****
05/06/2024 17:15:06 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:15:23 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   850/  1000] |  406/ 443 batches | ms/batch 314.6591 | train_loss 2.404745e-01 | lr 8.333333e-06
05/06/2024 17:15:43 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   900/  1000] |   13/ 443 batches | ms/batch 308.8589 | train_loss 2.379508e-01 | lr 5.555556e-06
05/06/2024 17:16:00 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   950/  1000] |   63/ 443 batches | ms/batch 314.2413 | train_loss 2.308207e-01 | lr 2.777778e-06
05/06/2024 17:16:16 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][  1000/  1000] |  113/ 443 batches | ms/batch 314.5169 | train_loss 2.338231e-01 | lr 0.000000e+00
05/06/2024 17:16:16 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmpxr5bpfc_ at global_step 1000 ****
05/06/2024 17:16:17 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:16:18 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23548119/tmpxr5bpfc_
05/06/2024 17:16:18 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
05/06/2024 17:16:18 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/06/2024 17:17:10 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
05/06/2024 17:17:10 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
05/06/2024 17:17:22 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
05/06/2024 17:17:26 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.49384985154814
05/06/2024 17:17:27 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
05/06/2024 17:17:37 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23548119/tmp2ajn5kus/X_trn.pt
05/06/2024 17:17:37 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
05/06/2024 17:18:07 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
05/06/2024 17:18:07 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/06/2024 17:18:08 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/06/2024 17:18:08 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/06/2024 17:18:08 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/06/2024 17:18:08 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/06/2024 17:18:08 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
05/06/2024 17:18:08 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 480
05/06/2024 17:18:08 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 3
05/06/2024 17:18:08 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/06/2024 17:18:08 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/06/2024 17:18:08 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/06/2024 17:18:08 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 1000
05/06/2024 17:18:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][    50/  1000] |   49/ 443 batches | ms/batch 311.0221 | train_loss 5.488455e-01 | lr 2.500000e-05
05/06/2024 17:18:46 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   100/  1000] |   99/ 443 batches | ms/batch 312.0131 | train_loss 5.054759e-01 | lr 5.000000e-05
05/06/2024 17:19:03 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   150/  1000] |  149/ 443 batches | ms/batch 312.9317 | train_loss 4.704902e-01 | lr 4.722222e-05
05/06/2024 17:19:20 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   200/  1000] |  199/ 443 batches | ms/batch 313.3038 | train_loss 4.521835e-01 | lr 4.444444e-05
05/06/2024 17:19:20 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmpl0tmk6py at global_step 200 ****
05/06/2024 17:19:21 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:19:37 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   250/  1000] |  249/ 443 batches | ms/batch 313.4895 | train_loss 4.463362e-01 | lr 4.166667e-05
05/06/2024 17:19:54 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   300/  1000] |  299/ 443 batches | ms/batch 313.8172 | train_loss 4.436512e-01 | lr 3.888889e-05
05/06/2024 17:20:11 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   350/  1000] |  349/ 443 batches | ms/batch 314.1238 | train_loss 4.402670e-01 | lr 3.611111e-05
05/06/2024 17:20:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   400/  1000] |  399/ 443 batches | ms/batch 314.5260 | train_loss 4.377776e-01 | lr 3.333333e-05
05/06/2024 17:20:27 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmpl0tmk6py at global_step 400 ****
05/06/2024 17:20:28 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:20:49 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   450/  1000] |    6/ 443 batches | ms/batch 308.9729 | train_loss 4.347412e-01 | lr 3.055556e-05
05/06/2024 17:21:06 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   500/  1000] |   56/ 443 batches | ms/batch 314.5030 | train_loss 4.307821e-01 | lr 2.777778e-05
05/06/2024 17:21:23 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   550/  1000] |  106/ 443 batches | ms/batch 314.8034 | train_loss 4.308422e-01 | lr 2.500000e-05
05/06/2024 17:21:39 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   600/  1000] |  156/ 443 batches | ms/batch 315.0978 | train_loss 4.280143e-01 | lr 2.222222e-05
05/06/2024 17:21:39 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmpl0tmk6py at global_step 600 ****
05/06/2024 17:21:40 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:21:56 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   650/  1000] |  206/ 443 batches | ms/batch 315.0631 | train_loss 4.278731e-01 | lr 1.944444e-05
05/06/2024 17:22:13 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   700/  1000] |  256/ 443 batches | ms/batch 315.1937 | train_loss 4.280375e-01 | lr 1.666667e-05
05/06/2024 17:22:30 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   750/  1000] |  306/ 443 batches | ms/batch 315.3584 | train_loss 4.263856e-01 | lr 1.388889e-05
05/06/2024 17:22:46 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   800/  1000] |  356/ 443 batches | ms/batch 315.3926 | train_loss 4.255960e-01 | lr 1.111111e-05
05/06/2024 17:22:46 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmpl0tmk6py at global_step 800 ****
05/06/2024 17:22:47 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:23:04 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   850/  1000] |  406/ 443 batches | ms/batch 315.3457 | train_loss 4.246204e-01 | lr 8.333333e-06
05/06/2024 17:23:25 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   900/  1000] |   13/ 443 batches | ms/batch 309.4732 | train_loss 4.241192e-01 | lr 5.555556e-06
05/06/2024 17:23:42 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   950/  1000] |   63/ 443 batches | ms/batch 315.0251 | train_loss 4.224146e-01 | lr 2.777778e-06
05/06/2024 17:23:59 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][  1000/  1000] |  113/ 443 batches | ms/batch 315.1866 | train_loss 4.191466e-01 | lr 0.000000e+00
05/06/2024 17:23:59 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmpl0tmk6py at global_step 1000 ****
05/06/2024 17:23:59 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:24:00 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23548119/tmpl0tmk6py
05/06/2024 17:24:01 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
05/06/2024 17:24:01 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/06/2024 17:24:55 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
05/06/2024 17:24:55 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
05/06/2024 17:25:19 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
05/06/2024 17:25:23 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=26.134949809133325
05/06/2024 17:25:26 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
05/06/2024 17:25:37 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23548119/tmp2ajn5kus/X_trn.pt
05/06/2024 17:25:37 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
05/06/2024 17:26:01 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
05/06/2024 17:26:01 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/06/2024 17:26:02 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/06/2024 17:26:02 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/06/2024 17:26:03 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/06/2024 17:26:03 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/06/2024 17:26:03 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
05/06/2024 17:26:03 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 622
05/06/2024 17:26:03 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
05/06/2024 17:26:03 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/06/2024 17:26:03 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/06/2024 17:26:03 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/06/2024 17:26:03 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
05/06/2024 17:26:24 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   400] |   49/ 443 batches | ms/batch 310.4705 | train_loss 4.409003e-01 | lr 2.500000e-05
05/06/2024 17:26:41 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   400] |   99/ 443 batches | ms/batch 311.8155 | train_loss 4.405455e-01 | lr 5.000000e-05
05/06/2024 17:26:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   400] |  149/ 443 batches | ms/batch 312.9148 | train_loss 4.430851e-01 | lr 4.166667e-05
05/06/2024 17:27:14 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   400] |  199/ 443 batches | ms/batch 313.3096 | train_loss 4.332826e-01 | lr 3.333333e-05
05/06/2024 17:27:14 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmps0jhv8ml at global_step 200 ****
05/06/2024 17:27:15 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:27:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   250/   400] |  249/ 443 batches | ms/batch 313.7111 | train_loss 4.338659e-01 | lr 2.500000e-05
05/06/2024 17:27:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   300/   400] |  299/ 443 batches | ms/batch 314.0951 | train_loss 4.260956e-01 | lr 1.666667e-05
05/06/2024 17:28:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   350/   400] |  349/ 443 batches | ms/batch 314.4424 | train_loss 4.315128e-01 | lr 8.333333e-06
05/06/2024 17:28:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   400/   400] |  399/ 443 batches | ms/batch 314.7595 | train_loss 4.317119e-01 | lr 0.000000e+00
05/06/2024 17:28:22 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmps0jhv8ml at global_step 400 ****
05/06/2024 17:28:22 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:28:23 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23548119/tmps0jhv8ml
05/06/2024 17:28:24 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.2745652481267
05/06/2024 17:28:24 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/06/2024 17:29:19 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
05/06/2024 17:29:24 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
05/06/2024 17:29:24 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
05/06/2024 17:29:24 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/06/2024 17:29:25 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/06/2024 17:29:31 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/06/2024 17:29:45 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
05/06/2024 17:31:35 - INFO - pecos.xmc.xtransformer.model - Parameters saved to ./trained-models/wiki10-31k/bert/param.json
05/06/2024 17:31:38 - INFO - pecos.xmc.xtransformer.model - Model saved to ./trained-models/wiki10-31k/bert
--- start prediction of bert ---
--- start evaluation of bert ---
--- start training of roberta ---
05/06/2024 17:33:10 - INFO - __main__ - Setting random seed 0
05/06/2024 17:33:10 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
05/06/2024 17:33:10 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
05/06/2024 17:33:10 - INFO - __main__ - Loaded 14146 training sequences
05/06/2024 17:33:14 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
05/06/2024 17:33:14 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
05/06/2024 17:33:14 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/06/2024 17:33:15 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
05/06/2024 17:33:15 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
05/06/2024 17:33:28 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=12.52266550064087 *****
05/06/2024 17:33:37 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23548119/tmp32j21epm/X_trn.pt
05/06/2024 17:33:37 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/06/2024 17:33:37 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/06/2024 17:33:37 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/06/2024 17:33:37 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/06/2024 17:33:37 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/06/2024 17:33:37 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
05/06/2024 17:33:37 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
05/06/2024 17:33:37 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/06/2024 17:33:37 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/06/2024 17:33:37 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/06/2024 17:33:37 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 600
05/06/2024 17:33:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   600] |   49/ 443 batches | ms/batch 320.3369 | train_loss 8.544826e-01 | lr 2.500000e-05
05/06/2024 17:34:13 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   600] |   99/ 443 batches | ms/batch 309.4691 | train_loss 4.327388e-01 | lr 5.000000e-05
05/06/2024 17:34:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   600] |  149/ 443 batches | ms/batch 310.8394 | train_loss 3.554073e-01 | lr 4.500000e-05
05/06/2024 17:34:46 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   600] |  199/ 443 batches | ms/batch 311.9066 | train_loss 2.995878e-01 | lr 4.000000e-05
05/06/2024 17:34:46 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmpngwenrhw at global_step 200 ****
05/06/2024 17:34:46 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:35:03 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   250/   600] |  249/ 443 batches | ms/batch 312.6323 | train_loss 2.763004e-01 | lr 3.500000e-05
05/06/2024 17:35:19 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   300/   600] |  299/ 443 batches | ms/batch 313.1007 | train_loss 2.640876e-01 | lr 3.000000e-05
05/06/2024 17:35:35 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   350/   600] |  349/ 443 batches | ms/batch 313.5121 | train_loss 2.620139e-01 | lr 2.500000e-05
05/06/2024 17:35:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   400/   600] |  399/ 443 batches | ms/batch 313.9427 | train_loss 2.576346e-01 | lr 2.000000e-05
05/06/2024 17:35:52 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmpngwenrhw at global_step 400 ****
05/06/2024 17:35:52 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:36:10 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   450/   600] |    6/ 443 batches | ms/batch 308.5116 | train_loss 2.519488e-01 | lr 1.500000e-05
05/06/2024 17:36:27 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   500/   600] |   56/ 443 batches | ms/batch 314.0973 | train_loss 2.409533e-01 | lr 1.000000e-05
05/06/2024 17:36:43 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   550/   600] |  106/ 443 batches | ms/batch 314.5353 | train_loss 2.388666e-01 | lr 5.000000e-06
05/06/2024 17:37:00 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   600/   600] |  156/ 443 batches | ms/batch 314.6659 | train_loss 2.364087e-01 | lr 0.000000e+00
05/06/2024 17:37:00 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmpngwenrhw at global_step 600 ****
05/06/2024 17:37:01 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:37:01 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23548119/tmpngwenrhw
05/06/2024 17:37:02 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
05/06/2024 17:37:02 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/06/2024 17:37:52 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
05/06/2024 17:37:52 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
05/06/2024 17:38:04 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
05/06/2024 17:38:07 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.41149441538244
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/06/2024 17:38:08 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
05/06/2024 17:38:19 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23548119/tmp32j21epm/X_trn.pt
05/06/2024 17:38:19 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
05/06/2024 17:38:46 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
05/06/2024 17:38:46 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/06/2024 17:38:46 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/06/2024 17:38:46 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/06/2024 17:38:47 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/06/2024 17:38:47 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/06/2024 17:38:47 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
05/06/2024 17:38:47 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 432
05/06/2024 17:38:47 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
05/06/2024 17:38:47 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/06/2024 17:38:47 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/06/2024 17:38:47 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/06/2024 17:38:47 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
05/06/2024 17:39:06 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   400] |   49/ 443 batches | ms/batch 311.4001 | train_loss 5.042087e-01 | lr 2.500000e-05
05/06/2024 17:39:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   400] |   99/ 443 batches | ms/batch 312.4697 | train_loss 4.215689e-01 | lr 5.000000e-05
05/06/2024 17:39:39 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   400] |  149/ 443 batches | ms/batch 313.1689 | train_loss 4.071068e-01 | lr 4.166667e-05
05/06/2024 17:39:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   400] |  199/ 443 batches | ms/batch 313.6270 | train_loss 4.016672e-01 | lr 3.333333e-05
05/06/2024 17:39:55 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmp6ch7e677 at global_step 200 ****
05/06/2024 17:39:56 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:40:13 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   250/   400] |  249/ 443 batches | ms/batch 313.8499 | train_loss 3.956839e-01 | lr 2.500000e-05
05/06/2024 17:40:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   300/   400] |  299/ 443 batches | ms/batch 314.2792 | train_loss 3.876270e-01 | lr 1.666667e-05
05/06/2024 17:40:46 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   350/   400] |  349/ 443 batches | ms/batch 314.4951 | train_loss 3.853958e-01 | lr 8.333333e-06
05/06/2024 17:41:03 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   400/   400] |  399/ 443 batches | ms/batch 314.6516 | train_loss 3.855688e-01 | lr 0.000000e+00
05/06/2024 17:41:03 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmp6ch7e677 at global_step 400 ****
05/06/2024 17:41:03 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:41:04 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23548119/tmp6ch7e677
05/06/2024 17:41:05 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
05/06/2024 17:41:05 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/06/2024 17:41:57 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
05/06/2024 17:41:57 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
05/06/2024 17:42:21 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
05/06/2024 17:42:23 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=26.172062773929024
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/06/2024 17:42:25 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
05/06/2024 17:42:36 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23548119/tmp32j21epm/X_trn.pt
05/06/2024 17:42:36 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
05/06/2024 17:43:00 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
05/06/2024 17:43:00 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/06/2024 17:43:00 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/06/2024 17:43:00 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/06/2024 17:43:01 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/06/2024 17:43:01 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/06/2024 17:43:01 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
05/06/2024 17:43:01 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 617
05/06/2024 17:43:01 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
05/06/2024 17:43:01 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/06/2024 17:43:01 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/06/2024 17:43:01 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/06/2024 17:43:01 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 200
05/06/2024 17:43:20 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   200] |   49/ 443 batches | ms/batch 310.8055 | train_loss 4.441468e-01 | lr 2.500000e-05
05/06/2024 17:43:37 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   200] |   99/ 443 batches | ms/batch 312.2772 | train_loss 4.364465e-01 | lr 5.000000e-05
05/06/2024 17:43:53 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   200] |  149/ 443 batches | ms/batch 313.2635 | train_loss 4.488150e-01 | lr 2.500000e-05
05/06/2024 17:44:10 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   200] |  199/ 443 batches | ms/batch 313.6690 | train_loss 4.397263e-01 | lr 0.000000e+00
05/06/2024 17:44:10 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmprhr9uqny at global_step 200 ****
05/06/2024 17:44:11 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:44:12 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23548119/tmprhr9uqny
05/06/2024 17:44:12 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.2244450728121
05/06/2024 17:44:12 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/06/2024 17:45:04 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
05/06/2024 17:45:09 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
05/06/2024 17:45:09 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
05/06/2024 17:45:09 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/06/2024 17:45:10 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/06/2024 17:45:16 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/06/2024 17:45:30 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
05/06/2024 17:47:19 - INFO - pecos.xmc.xtransformer.model - Parameters saved to ./trained-models/wiki10-31k/roberta/param.json
05/06/2024 17:47:22 - INFO - pecos.xmc.xtransformer.model - Model saved to ./trained-models/wiki10-31k/roberta
--- start prediction of roberta ---
mkdir: cannot create directory ‘./predictions/wiki10-31k/2024-05-06-17-10-03’: File exists
--- start evaluation of roberta ---
--- start training of xlnet ---
05/06/2024 17:49:04 - INFO - __main__ - Setting random seed 0
05/06/2024 17:49:04 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
05/06/2024 17:49:04 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
05/06/2024 17:49:05 - INFO - __main__ - Loaded 14146 training sequences
05/06/2024 17:49:09 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
05/06/2024 17:49:09 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
05/06/2024 17:49:09 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128

config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]
config.json: 100%|██████████| 760/760 [00:00<00:00, 305kB/s]

spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]
spiece.model: 100%|██████████| 798k/798k [00:00<00:00, 41.7MB/s]

tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]
tokenizer.json: 100%|██████████| 1.38M/1.38M [00:00<00:00, 14.0MB/s]

pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]
pytorch_model.bin:   2%|▏         | 10.5M/467M [00:00<00:04, 96.6MB/s]
pytorch_model.bin:   7%|▋         | 31.5M/467M [00:00<00:03, 110MB/s] 
pytorch_model.bin:  11%|█         | 52.4M/467M [00:00<00:03, 114MB/s]
pytorch_model.bin:  16%|█▌        | 73.4M/467M [00:00<00:03, 115MB/s]
pytorch_model.bin:  20%|██        | 94.4M/467M [00:00<00:03, 116MB/s]
pytorch_model.bin:  25%|██▍       | 115M/467M [00:01<00:03, 117MB/s] 
pytorch_model.bin:  29%|██▉       | 136M/467M [00:01<00:02, 117MB/s]
pytorch_model.bin:  34%|███▎      | 157M/467M [00:01<00:02, 117MB/s]
pytorch_model.bin:  38%|███▊      | 178M/467M [00:01<00:02, 117MB/s]
pytorch_model.bin:  43%|████▎     | 199M/467M [00:01<00:02, 117MB/s]
pytorch_model.bin:  47%|████▋     | 220M/467M [00:01<00:02, 117MB/s]
pytorch_model.bin:  52%|█████▏    | 241M/467M [00:02<00:01, 117MB/s]
pytorch_model.bin:  56%|█████▌    | 262M/467M [00:02<00:01, 117MB/s]
pytorch_model.bin:  61%|██████    | 283M/467M [00:02<00:01, 117MB/s]
pytorch_model.bin:  65%|██████▌   | 304M/467M [00:02<00:01, 117MB/s]
pytorch_model.bin:  70%|██████▉   | 325M/467M [00:02<00:01, 117MB/s]
pytorch_model.bin:  74%|███████▍  | 346M/467M [00:02<00:01, 117MB/s]
pytorch_model.bin:  79%|███████▊  | 367M/467M [00:03<00:00, 117MB/s]
pytorch_model.bin:  83%|████████▎ | 388M/467M [00:03<00:00, 117MB/s]
pytorch_model.bin:  88%|████████▊ | 409M/467M [00:03<00:00, 117MB/s]
pytorch_model.bin:  92%|█████████▏| 430M/467M [00:03<00:00, 117MB/s]
pytorch_model.bin:  97%|█████████▋| 451M/467M [00:03<00:00, 117MB/s]
pytorch_model.bin: 100%|██████████| 467M/467M [00:03<00:00, 117MB/s]
pytorch_model.bin: 100%|██████████| 467M/467M [00:04<00:00, 117MB/s]
05/06/2024 17:49:15 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
05/06/2024 17:49:15 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
05/06/2024 17:49:34 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=19.47140884399414 *****
05/06/2024 17:49:44 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23548119/tmppfk52qva/X_trn.pt
05/06/2024 17:49:44 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/06/2024 17:49:44 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/06/2024 17:49:44 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/06/2024 17:49:45 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/06/2024 17:49:45 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/06/2024 17:49:45 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
05/06/2024 17:49:45 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
05/06/2024 17:49:45 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/06/2024 17:49:45 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/06/2024 17:49:45 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/06/2024 17:49:45 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
05/06/2024 17:50:15 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   400] |   49/ 443 batches | ms/batch 532.6382 | train_loss 7.699232e-01 | lr 2.500000e-05
05/06/2024 17:50:41 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   400] |   99/ 443 batches | ms/batch 505.7448 | train_loss 4.283351e-01 | lr 5.000000e-05
05/06/2024 17:51:07 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   400] |  149/ 443 batches | ms/batch 507.4981 | train_loss 3.379400e-01 | lr 4.166667e-05
05/06/2024 17:51:34 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   400] |  199/ 443 batches | ms/batch 508.4107 | train_loss 3.060830e-01 | lr 3.333333e-05
05/06/2024 17:51:34 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmprp3mzyzb at global_step 200 ****
05/06/2024 17:51:34 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:52:01 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   250/   400] |  249/ 443 batches | ms/batch 508.4087 | train_loss 2.870467e-01 | lr 2.500000e-05
05/06/2024 17:52:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   300/   400] |  299/ 443 batches | ms/batch 509.2898 | train_loss 2.807996e-01 | lr 1.666667e-05
05/06/2024 17:52:53 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   350/   400] |  349/ 443 batches | ms/batch 509.7632 | train_loss 2.743175e-01 | lr 8.333333e-06
05/06/2024 17:53:20 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   400/   400] |  399/ 443 batches | ms/batch 510.1438 | train_loss 2.750024e-01 | lr 0.000000e+00
05/06/2024 17:53:20 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmprp3mzyzb at global_step 400 ****
05/06/2024 17:53:20 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:53:21 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23548119/tmprp3mzyzb
05/06/2024 17:53:21 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
05/06/2024 17:53:21 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/06/2024 17:54:44 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
05/06/2024 17:54:45 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
05/06/2024 17:54:56 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
05/06/2024 17:55:00 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.425137848154954
05/06/2024 17:55:02 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
05/06/2024 17:55:13 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23548119/tmppfk52qva/X_trn.pt
05/06/2024 17:55:13 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
05/06/2024 17:55:40 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
05/06/2024 17:55:40 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/06/2024 17:55:40 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/06/2024 17:55:40 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/06/2024 17:55:41 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/06/2024 17:55:41 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/06/2024 17:55:41 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
05/06/2024 17:55:41 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 432
05/06/2024 17:55:41 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
05/06/2024 17:55:41 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/06/2024 17:55:41 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/06/2024 17:55:41 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/06/2024 17:55:41 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
05/06/2024 17:56:11 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   400] |   49/ 443 batches | ms/batch 509.3345 | train_loss 6.570713e-01 | lr 2.500000e-05
05/06/2024 17:56:39 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   400] |   99/ 443 batches | ms/batch 538.1146 | train_loss 5.210952e-01 | lr 5.000000e-05
05/06/2024 17:57:06 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   400] |  149/ 443 batches | ms/batch 509.2277 | train_loss 4.750749e-01 | lr 4.166667e-05
05/06/2024 17:57:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   400] |  199/ 443 batches | ms/batch 510.4270 | train_loss 4.548158e-01 | lr 3.333333e-05
05/06/2024 17:57:32 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmp175mb_v9 at global_step 200 ****
05/06/2024 17:57:33 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:57:59 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   250/   400] |  249/ 443 batches | ms/batch 510.6969 | train_loss 4.449889e-01 | lr 2.500000e-05
05/06/2024 17:58:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   300/   400] |  299/ 443 batches | ms/batch 511.0789 | train_loss 4.339489e-01 | lr 1.666667e-05
05/06/2024 17:58:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   350/   400] |  349/ 443 batches | ms/batch 511.4661 | train_loss 4.295174e-01 | lr 8.333333e-06
05/06/2024 17:59:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   400/   400] |  399/ 443 batches | ms/batch 584.9950 | train_loss 4.277387e-01 | lr 0.000000e+00
05/06/2024 17:59:23 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmp175mb_v9 at global_step 400 ****
05/06/2024 17:59:24 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 17:59:25 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23548119/tmp175mb_v9
05/06/2024 17:59:25 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
05/06/2024 17:59:25 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/06/2024 18:00:50 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
05/06/2024 18:00:50 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
05/06/2024 18:01:14 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
05/06/2024 18:01:17 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=26.24211791319101
05/06/2024 18:01:19 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
05/06/2024 18:01:31 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23548119/tmppfk52qva/X_trn.pt
05/06/2024 18:01:31 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
05/06/2024 18:01:57 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
05/06/2024 18:01:57 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/06/2024 18:01:57 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/06/2024 18:01:58 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/06/2024 18:01:58 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/06/2024 18:01:58 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/06/2024 18:01:58 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
05/06/2024 18:01:58 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 648
05/06/2024 18:01:58 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
05/06/2024 18:01:58 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/06/2024 18:01:58 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/06/2024 18:01:58 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/06/2024 18:01:58 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 200
05/06/2024 18:02:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   200] |   49/ 443 batches | ms/batch 507.8667 | train_loss 5.318606e-01 | lr 2.500000e-05
05/06/2024 18:02:56 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   200] |   99/ 443 batches | ms/batch 507.0458 | train_loss 5.174603e-01 | lr 5.000000e-05
05/06/2024 18:03:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   200] |  149/ 443 batches | ms/batch 507.5104 | train_loss 5.159419e-01 | lr 2.500000e-05
05/06/2024 18:03:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   200] |  199/ 443 batches | ms/batch 508.3698 | train_loss 5.045804e-01 | lr 0.000000e+00
05/06/2024 18:03:48 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23548119/tmpxeoiu5t5 at global_step 200 ****
05/06/2024 18:03:49 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/06/2024 18:03:50 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23548119/tmpxeoiu5t5
05/06/2024 18:03:51 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.0601583486498
05/06/2024 18:03:51 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/06/2024 18:05:15 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
05/06/2024 18:05:20 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
05/06/2024 18:05:20 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
05/06/2024 18:05:20 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/06/2024 18:05:22 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/06/2024 18:05:27 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/06/2024 18:05:42 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
05/06/2024 18:07:35 - INFO - pecos.xmc.xtransformer.model - Parameters saved to ./trained-models/wiki10-31k/xlnet/param.json
05/06/2024 18:07:38 - INFO - pecos.xmc.xtransformer.model - Model saved to ./trained-models/wiki10-31k/xlnet
--- start prediction of xlnet ---
mkdir: cannot create directory ‘./predictions/wiki10-31k/2024-05-06-17-10-03’: File exists
--- start evaluation of xlnet ---

============================= JOB FEEDBACK =============================

NodeName=uc2n904
Job ID: 23548119
Cluster: uc2
User/Group: ul_ruw26/ul_student
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 16
CPU Utilized: 04:33:34
CPU Efficiency: 28.32% of 16:06:08 core-walltime
Job Wall-clock time: 01:00:23
Memory Utilized: 25.25 GB
Memory Efficiency: 64.64% of 39.06 GB
