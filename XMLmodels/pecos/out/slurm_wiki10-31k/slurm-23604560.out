UUID is 2024-05-16-14-57-29
--- start training of bert ---
05/16/2024 14:58:53 - INFO - __main__ - Setting random seed 0
05/16/2024 14:58:54 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
05/16/2024 14:58:54 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
05/16/2024 14:58:55 - INFO - __main__ - Loaded 14146 training sequences
05/16/2024 14:59:00 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
05/16/2024 14:59:00 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
05/16/2024 14:59:00 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
05/16/2024 14:59:06 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
05/16/2024 14:59:06 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
05/16/2024 14:59:18 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=11.53936505317688 *****
05/16/2024 14:59:27 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23604560/tmpchciyb5e/X_trn.pt
05/16/2024 14:59:27 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/16/2024 14:59:55 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/16/2024 14:59:55 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/16/2024 14:59:56 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/16/2024 14:59:56 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/16/2024 14:59:56 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
05/16/2024 14:59:56 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 3
05/16/2024 14:59:56 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/16/2024 14:59:56 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/16/2024 14:59:56 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/16/2024 14:59:56 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 1000
05/16/2024 15:00:24 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][    50/  1000] |   49/ 443 batches | ms/batch 479.9730 | train_loss 9.777567e-01 | lr 2.500000e-05
05/16/2024 15:00:40 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   100/  1000] |   99/ 443 batches | ms/batch 309.1610 | train_loss 4.654887e-01 | lr 5.000000e-05
05/16/2024 15:00:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   150/  1000] |  149/ 443 batches | ms/batch 310.7256 | train_loss 3.838973e-01 | lr 4.722222e-05
05/16/2024 15:01:13 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   200/  1000] |  199/ 443 batches | ms/batch 312.5078 | train_loss 3.344387e-01 | lr 4.444444e-05
05/16/2024 15:01:13 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmp50zwm256 at global_step 200 ****
05/16/2024 15:01:14 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:01:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   250/  1000] |  249/ 443 batches | ms/batch 313.6790 | train_loss 3.050309e-01 | lr 4.166667e-05
05/16/2024 15:01:47 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   300/  1000] |  299/ 443 batches | ms/batch 314.6474 | train_loss 2.908817e-01 | lr 3.888889e-05
05/16/2024 15:02:03 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   350/  1000] |  349/ 443 batches | ms/batch 316.4497 | train_loss 2.749389e-01 | lr 3.611111e-05
05/16/2024 15:02:20 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   400/  1000] |  399/ 443 batches | ms/batch 318.5485 | train_loss 2.737701e-01 | lr 3.333333e-05
05/16/2024 15:02:20 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmp50zwm256 at global_step 400 ****
05/16/2024 15:02:20 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:02:40 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   450/  1000] |    6/ 443 batches | ms/batch 314.4860 | train_loss 2.591828e-01 | lr 3.055556e-05
05/16/2024 15:02:57 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   500/  1000] |   56/ 443 batches | ms/batch 320.0991 | train_loss 2.546116e-01 | lr 2.777778e-05
05/16/2024 15:03:13 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   550/  1000] |  106/ 443 batches | ms/batch 319.7487 | train_loss 2.518338e-01 | lr 2.500000e-05
05/16/2024 15:03:30 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   600/  1000] |  156/ 443 batches | ms/batch 318.6866 | train_loss 2.548783e-01 | lr 2.222222e-05
05/16/2024 15:03:30 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmp50zwm256 at global_step 600 ****
05/16/2024 15:03:31 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:03:47 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   650/  1000] |  206/ 443 batches | ms/batch 318.6771 | train_loss 2.486589e-01 | lr 1.944444e-05
05/16/2024 15:04:04 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   700/  1000] |  256/ 443 batches | ms/batch 318.7831 | train_loss 2.448264e-01 | lr 1.666667e-05
05/16/2024 15:04:21 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   750/  1000] |  306/ 443 batches | ms/batch 318.9167 | train_loss 2.384545e-01 | lr 1.388889e-05
05/16/2024 15:04:37 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   800/  1000] |  356/ 443 batches | ms/batch 318.7142 | train_loss 2.450247e-01 | lr 1.111111e-05
05/16/2024 15:04:37 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmp50zwm256 at global_step 800 ****
05/16/2024 15:04:38 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:04:54 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   850/  1000] |  406/ 443 batches | ms/batch 318.4259 | train_loss 2.404745e-01 | lr 8.333333e-06
05/16/2024 15:05:14 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   900/  1000] |   13/ 443 batches | ms/batch 312.3813 | train_loss 2.379508e-01 | lr 5.555556e-06
05/16/2024 15:05:30 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   950/  1000] |   63/ 443 batches | ms/batch 318.1262 | train_loss 2.308207e-01 | lr 2.777778e-06
05/16/2024 15:05:47 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][  1000/  1000] |  113/ 443 batches | ms/batch 318.2934 | train_loss 2.338231e-01 | lr 0.000000e+00
05/16/2024 15:05:47 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmp50zwm256 at global_step 1000 ****
05/16/2024 15:05:48 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:05:48 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23604560/tmp50zwm256
05/16/2024 15:05:49 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
05/16/2024 15:05:49 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/16/2024 15:06:40 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
05/16/2024 15:06:41 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
05/16/2024 15:06:50 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
05/16/2024 15:06:53 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=22.38675243885197
05/16/2024 15:06:54 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
05/16/2024 15:07:04 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23604560/tmpchciyb5e/X_trn.pt
05/16/2024 15:07:04 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
05/16/2024 15:07:32 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
05/16/2024 15:07:32 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/16/2024 15:07:32 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/16/2024 15:07:32 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/16/2024 15:07:33 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/16/2024 15:07:33 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/16/2024 15:07:33 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
05/16/2024 15:07:33 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 544
05/16/2024 15:07:33 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 3
05/16/2024 15:07:33 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/16/2024 15:07:33 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/16/2024 15:07:33 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/16/2024 15:07:33 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 1000
05/16/2024 15:07:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][    50/  1000] |   49/ 443 batches | ms/batch 325.4742 | train_loss 5.222815e-01 | lr 2.500000e-05
05/16/2024 15:08:11 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   100/  1000] |   99/ 443 batches | ms/batch 316.4195 | train_loss 4.913870e-01 | lr 5.000000e-05
05/16/2024 15:08:28 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   150/  1000] |  149/ 443 batches | ms/batch 318.3245 | train_loss 4.655468e-01 | lr 4.722222e-05
05/16/2024 15:08:45 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   200/  1000] |  199/ 443 batches | ms/batch 318.9711 | train_loss 4.507756e-01 | lr 4.444444e-05
05/16/2024 15:08:45 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmpwu9a_yj_ at global_step 200 ****
05/16/2024 15:08:46 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:09:03 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   250/  1000] |  249/ 443 batches | ms/batch 319.3605 | train_loss 4.455424e-01 | lr 4.166667e-05
05/16/2024 15:09:20 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   300/  1000] |  299/ 443 batches | ms/batch 320.0193 | train_loss 4.457161e-01 | lr 3.888889e-05
05/16/2024 15:09:37 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   350/  1000] |  349/ 443 batches | ms/batch 319.6426 | train_loss 4.411390e-01 | lr 3.611111e-05
05/16/2024 15:09:54 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   400/  1000] |  399/ 443 batches | ms/batch 319.2866 | train_loss 4.371752e-01 | lr 3.333333e-05
05/16/2024 15:09:54 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmpwu9a_yj_ at global_step 400 ****
05/16/2024 15:09:54 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:10:15 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   450/  1000] |    6/ 443 batches | ms/batch 313.0046 | train_loss 4.372251e-01 | lr 3.055556e-05
05/16/2024 15:10:32 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   500/  1000] |   56/ 443 batches | ms/batch 318.2973 | train_loss 4.307827e-01 | lr 2.777778e-05
05/16/2024 15:10:48 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   550/  1000] |  106/ 443 batches | ms/batch 318.5853 | train_loss 4.333611e-01 | lr 2.500000e-05
05/16/2024 15:11:05 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   600/  1000] |  156/ 443 batches | ms/batch 318.6520 | train_loss 4.302737e-01 | lr 2.222222e-05
05/16/2024 15:11:05 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmpwu9a_yj_ at global_step 600 ****
05/16/2024 15:11:06 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:11:23 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   650/  1000] |  206/ 443 batches | ms/batch 318.3309 | train_loss 4.306035e-01 | lr 1.944444e-05
05/16/2024 15:11:39 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   700/  1000] |  256/ 443 batches | ms/batch 318.8030 | train_loss 4.317064e-01 | lr 1.666667e-05
05/16/2024 15:11:56 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   750/  1000] |  306/ 443 batches | ms/batch 318.6295 | train_loss 4.279905e-01 | lr 1.388889e-05
05/16/2024 15:12:13 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   800/  1000] |  356/ 443 batches | ms/batch 318.2545 | train_loss 4.279113e-01 | lr 1.111111e-05
05/16/2024 15:12:13 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmpwu9a_yj_ at global_step 800 ****
05/16/2024 15:12:14 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:12:30 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   850/  1000] |  406/ 443 batches | ms/batch 318.1132 | train_loss 4.278188e-01 | lr 8.333333e-06
05/16/2024 15:12:51 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   900/  1000] |   13/ 443 batches | ms/batch 312.2150 | train_loss 4.254891e-01 | lr 5.555556e-06
05/16/2024 15:13:07 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   950/  1000] |   63/ 443 batches | ms/batch 317.9176 | train_loss 4.264040e-01 | lr 2.777778e-06
05/16/2024 15:13:24 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][  1000/  1000] |  113/ 443 batches | ms/batch 317.8868 | train_loss 4.246180e-01 | lr 0.000000e+00
05/16/2024 15:13:24 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmpwu9a_yj_ at global_step 1000 ****
05/16/2024 15:13:25 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:13:26 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23604560/tmpwu9a_yj_
05/16/2024 15:13:26 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
05/16/2024 15:13:26 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/16/2024 15:14:20 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
05/16/2024 15:14:20 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
05/16/2024 15:14:44 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
05/16/2024 15:14:46 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=25.738795419199775
05/16/2024 15:14:48 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
05/16/2024 15:14:58 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23604560/tmpchciyb5e/X_trn.pt
05/16/2024 15:14:58 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
05/16/2024 15:15:22 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
05/16/2024 15:15:22 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/16/2024 15:15:22 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/16/2024 15:15:22 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/16/2024 15:15:23 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/16/2024 15:15:23 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/16/2024 15:15:23 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
05/16/2024 15:15:23 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 619
05/16/2024 15:15:23 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
05/16/2024 15:15:23 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/16/2024 15:15:23 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/16/2024 15:15:23 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/16/2024 15:15:23 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
05/16/2024 15:15:43 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   400] |   49/ 443 batches | ms/batch 312.6463 | train_loss 4.475666e-01 | lr 2.500000e-05
05/16/2024 15:16:00 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   400] |   99/ 443 batches | ms/batch 313.8557 | train_loss 4.464749e-01 | lr 5.000000e-05
05/16/2024 15:16:17 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   400] |  149/ 443 batches | ms/batch 316.3908 | train_loss 4.512181e-01 | lr 4.166667e-05
05/16/2024 15:16:34 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   400] |  199/ 443 batches | ms/batch 317.2450 | train_loss 4.392463e-01 | lr 3.333333e-05
05/16/2024 15:16:34 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmpw9jc9blo at global_step 200 ****
05/16/2024 15:16:34 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:16:51 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   250/   400] |  249/ 443 batches | ms/batch 318.3046 | train_loss 4.396152e-01 | lr 2.500000e-05
05/16/2024 15:17:08 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   300/   400] |  299/ 443 batches | ms/batch 319.0952 | train_loss 4.332347e-01 | lr 1.666667e-05
05/16/2024 15:17:25 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   350/   400] |  349/ 443 batches | ms/batch 319.0638 | train_loss 4.379734e-01 | lr 8.333333e-06
05/16/2024 15:17:42 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   400/   400] |  399/ 443 batches | ms/batch 319.0789 | train_loss 4.378251e-01 | lr 0.000000e+00
05/16/2024 15:17:42 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmpw9jc9blo at global_step 400 ****
05/16/2024 15:17:43 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:17:44 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23604560/tmpw9jc9blo
05/16/2024 15:17:44 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.27074791460484
05/16/2024 15:17:44 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/16/2024 15:18:39 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
05/16/2024 15:18:44 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
05/16/2024 15:18:44 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
05/16/2024 15:18:44 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/16/2024 15:18:45 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/16/2024 15:18:50 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/16/2024 15:19:05 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
05/16/2024 15:20:55 - INFO - pecos.xmc.xtransformer.model - Parameters saved to ./trained-models/wiki10-31k/bert/param.json
05/16/2024 15:20:58 - INFO - pecos.xmc.xtransformer.model - Model saved to ./trained-models/wiki10-31k/bert
--- start prediction of bert ---
--- start evaluation of bert ---
--- start training of roberta ---
05/16/2024 15:22:35 - INFO - __main__ - Setting random seed 0
05/16/2024 15:22:35 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
05/16/2024 15:22:35 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
05/16/2024 15:22:36 - INFO - __main__ - Loaded 14146 training sequences
05/16/2024 15:22:40 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
05/16/2024 15:22:40 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
05/16/2024 15:22:40 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/16/2024 15:22:45 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
05/16/2024 15:22:45 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
05/16/2024 15:22:59 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=13.876189708709717 *****
05/16/2024 15:23:08 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23604560/tmp6qw2llwg/X_trn.pt
05/16/2024 15:23:08 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/16/2024 15:23:08 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/16/2024 15:23:08 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/16/2024 15:23:08 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/16/2024 15:23:08 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/16/2024 15:23:08 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
05/16/2024 15:23:08 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
05/16/2024 15:23:08 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/16/2024 15:23:08 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/16/2024 15:23:08 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/16/2024 15:23:08 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 600
05/16/2024 15:23:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   600] |   49/ 443 batches | ms/batch 327.2249 | train_loss 8.544826e-01 | lr 2.500000e-05
05/16/2024 15:23:45 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   600] |   99/ 443 batches | ms/batch 311.3410 | train_loss 4.327388e-01 | lr 5.000000e-05
05/16/2024 15:24:01 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   600] |  149/ 443 batches | ms/batch 313.0410 | train_loss 3.554073e-01 | lr 4.500000e-05
05/16/2024 15:24:18 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   600] |  199/ 443 batches | ms/batch 314.7796 | train_loss 2.995878e-01 | lr 4.000000e-05
05/16/2024 15:24:18 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmp_kuepfi2 at global_step 200 ****
05/16/2024 15:24:19 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:24:35 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   250/   600] |  249/ 443 batches | ms/batch 316.7574 | train_loss 2.763004e-01 | lr 3.500000e-05
05/16/2024 15:24:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   300/   600] |  299/ 443 batches | ms/batch 317.6362 | train_loss 2.640876e-01 | lr 3.000000e-05
05/16/2024 15:25:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   350/   600] |  349/ 443 batches | ms/batch 318.9339 | train_loss 2.620139e-01 | lr 2.500000e-05
05/16/2024 15:25:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   400/   600] |  399/ 443 batches | ms/batch 320.4762 | train_loss 2.576346e-01 | lr 2.000000e-05
05/16/2024 15:25:26 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmp_kuepfi2 at global_step 400 ****
05/16/2024 15:25:26 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:25:45 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   450/   600] |    6/ 443 batches | ms/batch 314.1688 | train_loss 2.519488e-01 | lr 1.500000e-05
05/16/2024 15:26:02 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   500/   600] |   56/ 443 batches | ms/batch 319.7459 | train_loss 2.409533e-01 | lr 1.000000e-05
05/16/2024 15:26:19 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   550/   600] |  106/ 443 batches | ms/batch 320.2384 | train_loss 2.388666e-01 | lr 5.000000e-06
05/16/2024 15:26:36 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   600/   600] |  156/ 443 batches | ms/batch 319.7526 | train_loss 2.364087e-01 | lr 0.000000e+00
05/16/2024 15:26:36 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmp_kuepfi2 at global_step 600 ****
05/16/2024 15:26:36 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:26:37 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23604560/tmp_kuepfi2
05/16/2024 15:26:37 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
05/16/2024 15:26:37 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/16/2024 15:27:29 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
05/16/2024 15:27:29 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
05/16/2024 15:27:41 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
05/16/2024 15:27:44 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.41149441538244
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/16/2024 15:27:45 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
05/16/2024 15:27:56 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23604560/tmp6qw2llwg/X_trn.pt
05/16/2024 15:27:56 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
05/16/2024 15:28:23 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
05/16/2024 15:28:23 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/16/2024 15:28:23 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/16/2024 15:28:23 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/16/2024 15:28:24 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/16/2024 15:28:24 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/16/2024 15:28:24 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
05/16/2024 15:28:24 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 432
05/16/2024 15:28:24 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
05/16/2024 15:28:24 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/16/2024 15:28:24 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/16/2024 15:28:24 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/16/2024 15:28:24 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
05/16/2024 15:28:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   400] |   49/ 443 batches | ms/batch 316.3973 | train_loss 5.042087e-01 | lr 2.500000e-05
05/16/2024 15:29:01 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   400] |   99/ 443 batches | ms/batch 317.1173 | train_loss 4.215689e-01 | lr 5.000000e-05
05/16/2024 15:29:17 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   400] |  149/ 443 batches | ms/batch 318.4427 | train_loss 4.071068e-01 | lr 4.166667e-05
05/16/2024 15:29:34 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   400] |  199/ 443 batches | ms/batch 320.0256 | train_loss 4.016672e-01 | lr 3.333333e-05
05/16/2024 15:29:34 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmpk1897j0s at global_step 200 ****
05/16/2024 15:29:35 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:29:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   250/   400] |  249/ 443 batches | ms/batch 320.8636 | train_loss 3.956839e-01 | lr 2.500000e-05
05/16/2024 15:30:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   300/   400] |  299/ 443 batches | ms/batch 321.0695 | train_loss 3.876270e-01 | lr 1.666667e-05
05/16/2024 15:30:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   350/   400] |  349/ 443 batches | ms/batch 320.7422 | train_loss 3.853958e-01 | lr 8.333333e-06
05/16/2024 15:30:43 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   400/   400] |  399/ 443 batches | ms/batch 320.8167 | train_loss 3.855688e-01 | lr 0.000000e+00
05/16/2024 15:30:43 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmpk1897j0s at global_step 400 ****
05/16/2024 15:30:44 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:30:44 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23604560/tmpk1897j0s
05/16/2024 15:30:45 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
05/16/2024 15:30:45 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/16/2024 15:31:38 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
05/16/2024 15:31:39 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
05/16/2024 15:32:03 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
05/16/2024 15:32:05 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=26.172062773929024
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/16/2024 15:32:07 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
05/16/2024 15:32:19 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23604560/tmp6qw2llwg/X_trn.pt
05/16/2024 15:32:19 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
05/16/2024 15:32:43 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
05/16/2024 15:32:43 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/16/2024 15:32:43 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/16/2024 15:32:43 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/16/2024 15:32:44 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/16/2024 15:32:44 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/16/2024 15:32:44 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
05/16/2024 15:32:44 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 617
05/16/2024 15:32:44 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
05/16/2024 15:32:44 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/16/2024 15:32:44 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/16/2024 15:32:44 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/16/2024 15:32:44 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 200
05/16/2024 15:33:04 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   200] |   49/ 443 batches | ms/batch 314.1243 | train_loss 4.441468e-01 | lr 2.500000e-05
05/16/2024 15:33:21 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   200] |   99/ 443 batches | ms/batch 316.3440 | train_loss 4.364465e-01 | lr 5.000000e-05
05/16/2024 15:33:38 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   200] |  149/ 443 batches | ms/batch 318.4462 | train_loss 4.488150e-01 | lr 2.500000e-05
05/16/2024 15:33:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   200] |  199/ 443 batches | ms/batch 320.2329 | train_loss 4.397263e-01 | lr 0.000000e+00
05/16/2024 15:33:55 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmpbr6k8fu9 at global_step 200 ****
05/16/2024 15:33:56 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:33:57 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23604560/tmpbr6k8fu9
05/16/2024 15:33:57 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.2244450728121
05/16/2024 15:33:57 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/16/2024 15:34:52 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
05/16/2024 15:34:57 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
05/16/2024 15:34:57 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
05/16/2024 15:34:57 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/16/2024 15:34:58 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/16/2024 15:35:03 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/16/2024 15:35:18 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
05/16/2024 15:37:10 - INFO - pecos.xmc.xtransformer.model - Parameters saved to ./trained-models/wiki10-31k/roberta/param.json
05/16/2024 15:37:13 - INFO - pecos.xmc.xtransformer.model - Model saved to ./trained-models/wiki10-31k/roberta
--- start prediction of roberta ---
--- start evaluation of roberta ---
--- start training of xlnet ---
05/16/2024 15:38:46 - INFO - __main__ - Setting random seed 0
05/16/2024 15:38:46 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
05/16/2024 15:38:46 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
05/16/2024 15:38:46 - INFO - __main__ - Loaded 14146 training sequences
05/16/2024 15:38:50 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
05/16/2024 15:38:50 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
05/16/2024 15:38:50 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
05/16/2024 15:38:52 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
05/16/2024 15:38:52 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
05/16/2024 15:39:12 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=19.752591848373413 *****
05/16/2024 15:39:21 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23604560/tmpad3uom8y/X_trn.pt
05/16/2024 15:39:21 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/16/2024 15:39:22 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/16/2024 15:39:22 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/16/2024 15:39:22 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/16/2024 15:39:22 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/16/2024 15:39:22 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
05/16/2024 15:39:22 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
05/16/2024 15:39:22 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/16/2024 15:39:22 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/16/2024 15:39:22 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/16/2024 15:39:22 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
05/16/2024 15:39:53 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   400] |   49/ 443 batches | ms/batch 536.7496 | train_loss 7.699232e-01 | lr 2.500000e-05
05/16/2024 15:40:19 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   400] |   99/ 443 batches | ms/batch 509.2060 | train_loss 4.283351e-01 | lr 5.000000e-05
05/16/2024 15:40:45 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   400] |  149/ 443 batches | ms/batch 510.9235 | train_loss 3.379400e-01 | lr 4.166667e-05
05/16/2024 15:41:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   400] |  199/ 443 batches | ms/batch 513.1423 | train_loss 3.060830e-01 | lr 3.333333e-05
05/16/2024 15:41:12 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmpesc5696y at global_step 200 ****
05/16/2024 15:41:12 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:41:39 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   250/   400] |  249/ 443 batches | ms/batch 515.0209 | train_loss 2.870467e-01 | lr 2.500000e-05
05/16/2024 15:42:06 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   300/   400] |  299/ 443 batches | ms/batch 516.7711 | train_loss 2.807996e-01 | lr 1.666667e-05
05/16/2024 15:42:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   350/   400] |  349/ 443 batches | ms/batch 516.0544 | train_loss 2.743175e-01 | lr 8.333333e-06
05/16/2024 15:42:59 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   400/   400] |  399/ 443 batches | ms/batch 516.2906 | train_loss 2.750024e-01 | lr 0.000000e+00
05/16/2024 15:42:59 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmpesc5696y at global_step 400 ****
05/16/2024 15:43:00 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:43:01 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23604560/tmpesc5696y
05/16/2024 15:43:01 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
05/16/2024 15:43:01 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/16/2024 15:44:24 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
05/16/2024 15:44:25 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
05/16/2024 15:44:36 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
05/16/2024 15:44:40 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.425137848154954
05/16/2024 15:44:42 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
05/16/2024 15:44:54 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23604560/tmpad3uom8y/X_trn.pt
05/16/2024 15:44:54 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
05/16/2024 15:45:20 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
05/16/2024 15:45:20 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/16/2024 15:45:20 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/16/2024 15:45:20 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/16/2024 15:45:21 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/16/2024 15:45:21 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/16/2024 15:45:21 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
05/16/2024 15:45:21 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 432
05/16/2024 15:45:21 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
05/16/2024 15:45:21 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/16/2024 15:45:21 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/16/2024 15:45:21 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/16/2024 15:45:21 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
05/16/2024 15:45:51 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   400] |   49/ 443 batches | ms/batch 511.8662 | train_loss 6.570713e-01 | lr 2.500000e-05
05/16/2024 15:46:18 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   400] |   99/ 443 batches | ms/batch 512.9846 | train_loss 5.210952e-01 | lr 5.000000e-05
05/16/2024 15:46:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   400] |  149/ 443 batches | ms/batch 516.2850 | train_loss 4.750749e-01 | lr 4.166667e-05
05/16/2024 15:47:11 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   400] |  199/ 443 batches | ms/batch 516.9375 | train_loss 4.548158e-01 | lr 3.333333e-05
05/16/2024 15:47:11 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmpyf3ei1j3 at global_step 200 ****
05/16/2024 15:47:12 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:47:39 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   250/   400] |  249/ 443 batches | ms/batch 515.9409 | train_loss 4.449889e-01 | lr 2.500000e-05
05/16/2024 15:48:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   300/   400] |  299/ 443 batches | ms/batch 514.1770 | train_loss 4.339489e-01 | lr 1.666667e-05
05/16/2024 15:48:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   350/   400] |  349/ 443 batches | ms/batch 513.9567 | train_loss 4.295174e-01 | lr 8.333333e-06
05/16/2024 15:48:59 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   400/   400] |  399/ 443 batches | ms/batch 514.1379 | train_loss 4.277387e-01 | lr 0.000000e+00
05/16/2024 15:48:59 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmpyf3ei1j3 at global_step 400 ****
05/16/2024 15:48:59 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:49:00 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23604560/tmpyf3ei1j3
05/16/2024 15:49:01 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
05/16/2024 15:49:01 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/16/2024 15:50:27 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
05/16/2024 15:50:28 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
05/16/2024 15:50:54 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
05/16/2024 15:50:58 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=26.24211791319101
05/16/2024 15:51:03 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
05/16/2024 15:51:16 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23604560/tmpad3uom8y/X_trn.pt
05/16/2024 15:51:16 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
05/16/2024 15:51:42 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
05/16/2024 15:51:42 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 1
05/16/2024 15:51:43 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
05/16/2024 15:51:43 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/16/2024 15:51:43 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
05/16/2024 15:51:43 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
05/16/2024 15:51:43 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
05/16/2024 15:51:43 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 648
05/16/2024 15:51:43 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
05/16/2024 15:51:43 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
05/16/2024 15:51:43 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 32
05/16/2024 15:51:43 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
05/16/2024 15:51:43 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 200
05/16/2024 15:52:14 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   200] |   49/ 443 batches | ms/batch 515.4794 | train_loss 5.318606e-01 | lr 2.500000e-05
05/16/2024 15:52:41 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   200] |   99/ 443 batches | ms/batch 511.8556 | train_loss 5.174603e-01 | lr 5.000000e-05
05/16/2024 15:53:08 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   200] |  149/ 443 batches | ms/batch 513.2114 | train_loss 5.159419e-01 | lr 2.500000e-05
05/16/2024 15:53:35 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   200] |  199/ 443 batches | ms/batch 516.8004 | train_loss 5.045804e-01 | lr 0.000000e+00
05/16/2024 15:53:35 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23604560/tmps1erxg45 at global_step 200 ****
05/16/2024 15:53:36 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
05/16/2024 15:53:37 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23604560/tmps1erxg45
05/16/2024 15:53:37 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.0601583486498
05/16/2024 15:53:37 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
05/16/2024 15:55:04 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
05/16/2024 15:55:10 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
05/16/2024 15:55:10 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
05/16/2024 15:55:10 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/16/2024 15:55:11 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/16/2024 15:55:16 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
05/16/2024 15:55:31 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
05/16/2024 15:57:24 - INFO - pecos.xmc.xtransformer.model - Parameters saved to ./trained-models/wiki10-31k/xlnet/param.json
05/16/2024 15:57:27 - INFO - pecos.xmc.xtransformer.model - Model saved to ./trained-models/wiki10-31k/xlnet
--- start prediction of xlnet ---
--- start evaluation of xlnet ---

============================= JOB FEEDBACK =============================

NodeName=uc2n910
Job ID: 23604560
Cluster: uc2
User/Group: ul_ruw26/ul_student
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 16
CPU Utilized: 04:32:35
CPU Efficiency: 27.20% of 16:42:08 core-walltime
Job Wall-clock time: 01:02:38
Memory Utilized: 23.66 GB
Memory Efficiency: 60.58% of 39.06 GB
