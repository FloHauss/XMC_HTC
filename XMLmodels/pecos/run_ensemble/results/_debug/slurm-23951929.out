------- Ensemble run at 2024-08-12 19:44:06 for amazon-670k ----------
08/12/2024 19:44:55 - INFO - __main__ - Setting random seed 0
08/12/2024 19:44:57 - INFO - __main__ - Loaded training feature matrix with shape=(490449, 135909)
08/12/2024 19:44:58 - INFO - __main__ - Loaded training label matrix with shape=(490449, 670091)
08/12/2024 19:44:59 - INFO - __main__ - Loaded 490449 training sequences
08/12/2024 19:45:13 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 32768, 670091]
08/12/2024 19:45:13 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 32768]
08/12/2024 19:45:13 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
08/12/2024 19:45:14 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
08/12/2024 19:45:14 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=490449 truncation=128*****
08/12/2024 19:45:59 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=45.68718457221985 *****
08/12/2024 19:46:24 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23951929/tmpbiwxjbf6/X_trn.pt
08/12/2024 19:46:25 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/12/2024 19:46:25 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/12/2024 19:46:25 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=490449
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/12/2024 19:46:26 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/12/2024 19:46:26 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 490449
08/12/2024 19:46:26 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
08/12/2024 19:46:26 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
08/12/2024 19:46:26 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/12/2024 19:46:26 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/12/2024 19:46:26 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/12/2024 19:46:26 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 4000
08/12/2024 19:46:43 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/  4000] |   49/7664 batches | ms/batch 223.5895 | train_loss 9.122515e-01 | lr 2.500000e-05
08/12/2024 19:46:53 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/  4000] |   99/7664 batches | ms/batch 191.3954 | train_loss 2.161752e-01 | lr 5.000000e-05
08/12/2024 19:47:04 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/  4000] |  149/7664 batches | ms/batch 195.1236 | train_loss 1.727486e-01 | lr 7.500000e-05
08/12/2024 19:47:14 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/  4000] |  199/7664 batches | ms/batch 193.1350 | train_loss 1.573389e-01 | lr 1.000000e-04
08/12/2024 19:47:25 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   250/  4000] |  249/7664 batches | ms/batch 191.9943 | train_loss 1.308312e-01 | lr 9.868421e-05
08/12/2024 19:47:35 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   300/  4000] |  299/7664 batches | ms/batch 193.6414 | train_loss 1.131381e-01 | lr 9.736842e-05
08/12/2024 19:47:45 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   350/  4000] |  349/7664 batches | ms/batch 193.7474 | train_loss 1.030426e-01 | lr 9.605263e-05
08/12/2024 19:47:56 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   400/  4000] |  399/7664 batches | ms/batch 193.8174 | train_loss 9.667281e-02 | lr 9.473684e-05
08/12/2024 19:48:07 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   450/  4000] |  449/7664 batches | ms/batch 192.6669 | train_loss 9.213393e-02 | lr 9.342105e-05
08/12/2024 19:48:17 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   500/  4000] |  499/7664 batches | ms/batch 194.0654 | train_loss 9.022655e-02 | lr 9.210526e-05
08/12/2024 19:48:28 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   550/  4000] |  549/7664 batches | ms/batch 194.6204 | train_loss 8.762111e-02 | lr 9.078947e-05
08/12/2024 19:48:39 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   600/  4000] |  599/7664 batches | ms/batch 194.3100 | train_loss 8.541055e-02 | lr 8.947368e-05
08/12/2024 19:48:49 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   650/  4000] |  649/7664 batches | ms/batch 192.9172 | train_loss 8.278435e-02 | lr 8.815789e-05
08/12/2024 19:49:00 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   700/  4000] |  699/7664 batches | ms/batch 194.5347 | train_loss 7.947467e-02 | lr 8.684211e-05
08/12/2024 19:49:10 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   750/  4000] |  749/7664 batches | ms/batch 194.4876 | train_loss 7.695602e-02 | lr 8.552632e-05
08/12/2024 19:49:21 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   800/  4000] |  799/7664 batches | ms/batch 194.8898 | train_loss 7.858161e-02 | lr 8.421053e-05
08/12/2024 19:49:31 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   850/  4000] |  849/7664 batches | ms/batch 194.4718 | train_loss 7.690330e-02 | lr 8.289474e-05
08/12/2024 19:49:42 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   900/  4000] |  899/7664 batches | ms/batch 193.2182 | train_loss 7.838354e-02 | lr 8.157895e-05
08/12/2024 19:49:53 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   950/  4000] |  949/7664 batches | ms/batch 194.5475 | train_loss 7.689615e-02 | lr 8.026316e-05
08/12/2024 19:50:03 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1000/  4000] |  999/7664 batches | ms/batch 194.4786 | train_loss 7.433572e-02 | lr 7.894737e-05
08/12/2024 19:50:03 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23951929/tmpafhzwgyb at global_step 1000 ****
08/12/2024 19:50:04 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/12/2024 19:50:14 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1050/  4000] | 1049/7664 batches | ms/batch 194.4354 | train_loss 7.599803e-02 | lr 7.763158e-05
08/12/2024 19:50:25 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1100/  4000] | 1099/7664 batches | ms/batch 193.3975 | train_loss 7.500569e-02 | lr 7.631579e-05
08/12/2024 19:50:35 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1150/  4000] | 1149/7664 batches | ms/batch 194.7369 | train_loss 7.439935e-02 | lr 7.500000e-05
08/12/2024 19:50:46 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1200/  4000] | 1199/7664 batches | ms/batch 194.8127 | train_loss 7.392085e-02 | lr 7.368421e-05
08/12/2024 19:50:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1250/  4000] | 1249/7664 batches | ms/batch 194.7767 | train_loss 7.367238e-02 | lr 7.236842e-05
08/12/2024 19:51:07 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1300/  4000] | 1299/7664 batches | ms/batch 193.6673 | train_loss 7.282255e-02 | lr 7.105263e-05
08/12/2024 19:51:18 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1350/  4000] | 1349/7664 batches | ms/batch 195.2721 | train_loss 7.240625e-02 | lr 6.973684e-05
08/12/2024 19:51:28 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1400/  4000] | 1399/7664 batches | ms/batch 195.0012 | train_loss 7.307297e-02 | lr 6.842105e-05
08/12/2024 19:51:39 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1450/  4000] | 1449/7664 batches | ms/batch 195.1176 | train_loss 7.043418e-02 | lr 6.710526e-05
08/12/2024 19:51:49 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1500/  4000] | 1499/7664 batches | ms/batch 194.9114 | train_loss 7.081600e-02 | lr 6.578947e-05
08/12/2024 19:52:00 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1550/  4000] | 1549/7664 batches | ms/batch 193.5868 | train_loss 7.244379e-02 | lr 6.447368e-05
08/12/2024 19:52:10 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1600/  4000] | 1599/7664 batches | ms/batch 195.0099 | train_loss 7.089321e-02 | lr 6.315789e-05
08/12/2024 19:52:21 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1650/  4000] | 1649/7664 batches | ms/batch 195.2227 | train_loss 7.066222e-02 | lr 6.184211e-05
08/12/2024 19:52:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1700/  4000] | 1699/7664 batches | ms/batch 195.3657 | train_loss 6.810873e-02 | lr 6.052632e-05
08/12/2024 19:52:42 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1750/  4000] | 1749/7664 batches | ms/batch 194.5174 | train_loss 7.035554e-02 | lr 5.921053e-05
08/12/2024 19:52:53 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1800/  4000] | 1799/7664 batches | ms/batch 195.8917 | train_loss 6.726271e-02 | lr 5.789474e-05
08/12/2024 19:53:04 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1850/  4000] | 1849/7664 batches | ms/batch 196.0693 | train_loss 6.867731e-02 | lr 5.657895e-05
08/12/2024 19:53:14 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1900/  4000] | 1899/7664 batches | ms/batch 195.7319 | train_loss 6.919418e-02 | lr 5.526316e-05
08/12/2024 19:53:25 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1950/  4000] | 1949/7664 batches | ms/batch 194.4374 | train_loss 6.859755e-02 | lr 5.394737e-05
08/12/2024 19:53:35 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2000/  4000] | 1999/7664 batches | ms/batch 196.1576 | train_loss 6.992275e-02 | lr 5.263158e-05
08/12/2024 19:53:35 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23951929/tmpafhzwgyb at global_step 2000 ****
08/12/2024 19:53:36 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/12/2024 19:53:47 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2050/  4000] | 2049/7664 batches | ms/batch 196.2997 | train_loss 6.690615e-02 | lr 5.131579e-05
08/12/2024 19:53:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2100/  4000] | 2099/7664 batches | ms/batch 195.8729 | train_loss 6.789897e-02 | lr 5.000000e-05
08/12/2024 19:54:08 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2150/  4000] | 2149/7664 batches | ms/batch 195.5207 | train_loss 6.625768e-02 | lr 4.868421e-05
08/12/2024 19:54:18 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2200/  4000] | 2199/7664 batches | ms/batch 194.1224 | train_loss 6.764431e-02 | lr 4.736842e-05
08/12/2024 19:54:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2250/  4000] | 2249/7664 batches | ms/batch 195.2228 | train_loss 6.645710e-02 | lr 4.605263e-05
08/12/2024 19:54:40 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2300/  4000] | 2299/7664 batches | ms/batch 195.2493 | train_loss 6.598305e-02 | lr 4.473684e-05
08/12/2024 19:54:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2350/  4000] | 2349/7664 batches | ms/batch 195.2879 | train_loss 6.681735e-02 | lr 4.342105e-05
08/12/2024 19:55:01 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2400/  4000] | 2399/7664 batches | ms/batch 193.9618 | train_loss 6.647220e-02 | lr 4.210526e-05
08/12/2024 19:55:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2450/  4000] | 2449/7664 batches | ms/batch 195.1458 | train_loss 6.728555e-02 | lr 4.078947e-05
08/12/2024 19:55:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2500/  4000] | 2499/7664 batches | ms/batch 195.2922 | train_loss 6.551027e-02 | lr 3.947368e-05
08/12/2024 19:55:33 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2550/  4000] | 2549/7664 batches | ms/batch 195.4864 | train_loss 6.639115e-02 | lr 3.815789e-05
08/12/2024 19:55:43 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2600/  4000] | 2599/7664 batches | ms/batch 194.4081 | train_loss 6.614954e-02 | lr 3.684211e-05
08/12/2024 19:55:54 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2650/  4000] | 2649/7664 batches | ms/batch 195.5294 | train_loss 6.643193e-02 | lr 3.552632e-05
08/12/2024 19:56:04 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2700/  4000] | 2699/7664 batches | ms/batch 195.6585 | train_loss 6.605154e-02 | lr 3.421053e-05
08/12/2024 19:56:15 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2750/  4000] | 2749/7664 batches | ms/batch 195.4265 | train_loss 6.544717e-02 | lr 3.289474e-05
08/12/2024 19:56:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2800/  4000] | 2799/7664 batches | ms/batch 194.9753 | train_loss 6.563752e-02 | lr 3.157895e-05
08/12/2024 19:56:36 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2850/  4000] | 2849/7664 batches | ms/batch 194.1330 | train_loss 6.448456e-02 | lr 3.026316e-05
08/12/2024 19:56:47 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2900/  4000] | 2899/7664 batches | ms/batch 195.4899 | train_loss 6.368757e-02 | lr 2.894737e-05
08/12/2024 19:56:58 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2950/  4000] | 2949/7664 batches | ms/batch 195.5447 | train_loss 6.502616e-02 | lr 2.763158e-05
08/12/2024 19:57:08 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3000/  4000] | 2999/7664 batches | ms/batch 195.3832 | train_loss 6.488572e-02 | lr 2.631579e-05
08/12/2024 19:57:08 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23951929/tmpafhzwgyb at global_step 3000 ****
08/12/2024 19:57:09 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/12/2024 19:57:19 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3050/  4000] | 3049/7664 batches | ms/batch 193.9689 | train_loss 6.535273e-02 | lr 2.500000e-05
08/12/2024 19:57:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3100/  4000] | 3099/7664 batches | ms/batch 195.2649 | train_loss 6.413666e-02 | lr 2.368421e-05
08/12/2024 19:57:40 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3150/  4000] | 3149/7664 batches | ms/batch 195.4409 | train_loss 6.286958e-02 | lr 2.236842e-05
08/12/2024 19:57:51 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3200/  4000] | 3199/7664 batches | ms/batch 195.6085 | train_loss 6.240768e-02 | lr 2.105263e-05
08/12/2024 19:58:02 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3250/  4000] | 3249/7664 batches | ms/batch 193.9192 | train_loss 6.480138e-02 | lr 1.973684e-05
08/12/2024 19:58:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3300/  4000] | 3299/7664 batches | ms/batch 195.4722 | train_loss 6.481003e-02 | lr 1.842105e-05
08/12/2024 19:58:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3350/  4000] | 3349/7664 batches | ms/batch 195.3791 | train_loss 6.458102e-02 | lr 1.710526e-05
08/12/2024 19:58:34 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3400/  4000] | 3399/7664 batches | ms/batch 195.7822 | train_loss 6.397798e-02 | lr 1.578947e-05
08/12/2024 19:58:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3450/  4000] | 3449/7664 batches | ms/batch 195.3018 | train_loss 6.224542e-02 | lr 1.447368e-05
08/12/2024 19:58:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3500/  4000] | 3499/7664 batches | ms/batch 194.3508 | train_loss 6.414476e-02 | lr 1.315789e-05
08/12/2024 19:59:06 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3550/  4000] | 3549/7664 batches | ms/batch 195.6136 | train_loss 6.244636e-02 | lr 1.184211e-05
08/12/2024 19:59:16 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3600/  4000] | 3599/7664 batches | ms/batch 196.0582 | train_loss 6.036677e-02 | lr 1.052632e-05
08/12/2024 19:59:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3650/  4000] | 3649/7664 batches | ms/batch 196.3141 | train_loss 6.268162e-02 | lr 9.210526e-06
08/12/2024 19:59:38 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3700/  4000] | 3699/7664 batches | ms/batch 194.5035 | train_loss 6.168512e-02 | lr 7.894737e-06
08/12/2024 19:59:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3750/  4000] | 3749/7664 batches | ms/batch 195.9993 | train_loss 6.311998e-02 | lr 6.578947e-06
08/12/2024 19:59:59 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3800/  4000] | 3799/7664 batches | ms/batch 195.9151 | train_loss 6.336958e-02 | lr 5.263158e-06
08/12/2024 20:00:10 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3850/  4000] | 3849/7664 batches | ms/batch 196.0780 | train_loss 6.128364e-02 | lr 3.947368e-06
08/12/2024 20:00:20 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3900/  4000] | 3899/7664 batches | ms/batch 194.7331 | train_loss 6.406201e-02 | lr 2.631579e-06
08/12/2024 20:00:31 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3950/  4000] | 3949/7664 batches | ms/batch 195.7765 | train_loss 6.243852e-02 | lr 1.315789e-06
08/12/2024 20:00:41 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4000/  4000] | 3999/7664 batches | ms/batch 195.7806 | train_loss 6.142710e-02 | lr 0.000000e+00
08/12/2024 20:00:41 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23951929/tmpafhzwgyb at global_step 4000 ****
08/12/2024 20:00:42 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/12/2024 20:00:43 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23951929/tmpafhzwgyb
08/12/2024 20:00:43 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([490449, 128])) in OVA mode
08/12/2024 20:00:43 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=490449
08/12/2024 20:10:48 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/12/2024 20:10:58 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/12/2024 20:12:59 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/12/2024 20:13:11 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=5.101201144257609
08/12/2024 20:13:12 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
08/12/2024 20:13:41 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23951929/tmpbiwxjbf6/X_trn.pt
08/12/2024 20:13:41 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/12/2024 20:17:21 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/12/2024 20:17:21 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/12/2024 20:17:21 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/12/2024 20:17:21 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=490449
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/12/2024 20:17:39 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/12/2024 20:17:39 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 490449
08/12/2024 20:17:39 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
08/12/2024 20:17:39 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 176
08/12/2024 20:17:39 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
08/12/2024 20:17:39 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/12/2024 20:17:39 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/12/2024 20:17:39 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/12/2024 20:17:39 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 4000
Traceback (most recent call last):
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/train.py", line 564, in <module>
    do_train(args)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/train.py", line 548, in do_train
    xtf = XTransformer.train(
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/model.py", line 447, in train
    res_dict = TransformerMatcher.train(
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/matcher.py", line 1390, in train
    matcher.fine_tune_encoder(prob, val_prob=val_prob, val_csr_codes=val_csr_codes)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/matcher.py", line 1130, in fine_tune_encoder
    torch.nn.utils.clip_grad_norm_(
  File "/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py", line 55, in clip_grad_norm_
    norms.extend(torch._foreach_norm(grads, norm_type))
NotImplementedError: Could not run 'aten::_foreach_norm.Scalar' with arguments from the 'SparseCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_foreach_norm.Scalar' is only available for these backends: [CPU, CUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

CPU: registered at aten/src/ATen/RegisterCPU.cpp:31188 [kernel]
CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44143 [kernel]
BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]
Functionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]
Named: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]
ZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]
AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
Tracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:17079 [kernel]
AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]
AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]
FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]
FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]
Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]
PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]
PreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]
PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]

Traceback (most recent call last):
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/predict.py", line 176, in <module>
    do_predict(args)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/predict.py", line 145, in do_predict
    xtf = XTransformer.load(args.model_folder)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/model.py", line 196, in load
    text_encoder = TransformerMatcher.load(os.path.join(load_dir, "text_encoder"))
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/matcher.py", line 386, in load
    raise ValueError(f"text_encoder does not exist at {encoder_dir}")
ValueError: text_encoder does not exist at models/amazon-670k/bert1/text_encoder/text_encoder
08/12/2024 20:18:10 - INFO - __main__ - Setting random seed 0
08/12/2024 20:18:12 - INFO - __main__ - Loaded training feature matrix with shape=(490449, 135909)
08/12/2024 20:18:12 - INFO - __main__ - Loaded training label matrix with shape=(490449, 670091)
08/12/2024 20:18:13 - INFO - __main__ - Loaded 490449 training sequences
08/12/2024 20:18:26 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 32768, 670091]
08/12/2024 20:18:26 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 32768]
08/12/2024 20:18:26 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
08/12/2024 20:18:29 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
08/12/2024 20:18:29 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=490449 truncation=128*****
08/12/2024 20:19:14 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=45.821088552474976 *****
08/12/2024 20:19:39 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23951929/tmpgo8tvttd/X_trn.pt
08/12/2024 20:19:40 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/12/2024 20:19:40 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/12/2024 20:19:40 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=490449
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/12/2024 20:19:40 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/12/2024 20:19:40 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 490449
08/12/2024 20:19:40 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
08/12/2024 20:19:40 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
08/12/2024 20:19:40 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/12/2024 20:19:40 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/12/2024 20:19:40 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/12/2024 20:19:40 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 4000
08/12/2024 20:19:58 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/  4000] |   49/7664 batches | ms/batch 225.8317 | train_loss 9.112216e-01 | lr 2.500000e-05
08/12/2024 20:20:08 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/  4000] |   99/7664 batches | ms/batch 191.4151 | train_loss 2.169015e-01 | lr 5.000000e-05
08/12/2024 20:20:18 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/  4000] |  149/7664 batches | ms/batch 193.8081 | train_loss 1.723510e-01 | lr 7.500000e-05
08/12/2024 20:20:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/  4000] |  199/7664 batches | ms/batch 193.5653 | train_loss 1.571649e-01 | lr 1.000000e-04
08/12/2024 20:20:39 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   250/  4000] |  249/7664 batches | ms/batch 192.0302 | train_loss 1.330314e-01 | lr 9.868421e-05
08/12/2024 20:20:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   300/  4000] |  299/7664 batches | ms/batch 193.5826 | train_loss 1.148972e-01 | lr 9.736842e-05
08/12/2024 20:21:00 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   350/  4000] |  349/7664 batches | ms/batch 194.1480 | train_loss 1.039105e-01 | lr 9.605263e-05
08/12/2024 20:21:11 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   400/  4000] |  399/7664 batches | ms/batch 194.0020 | train_loss 9.774382e-02 | lr 9.473684e-05
08/12/2024 20:21:21 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   450/  4000] |  449/7664 batches | ms/batch 192.8717 | train_loss 9.246226e-02 | lr 9.342105e-05
08/12/2024 20:21:31 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   500/  4000] |  499/7664 batches | ms/batch 194.5243 | train_loss 9.074108e-02 | lr 9.210526e-05
08/12/2024 20:21:42 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   550/  4000] |  549/7664 batches | ms/batch 194.5263 | train_loss 8.737285e-02 | lr 9.078947e-05
08/12/2024 20:21:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   600/  4000] |  599/7664 batches | ms/batch 194.9564 | train_loss 8.584941e-02 | lr 8.947368e-05
08/12/2024 20:22:03 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   650/  4000] |  649/7664 batches | ms/batch 193.4700 | train_loss 8.354950e-02 | lr 8.815789e-05
08/12/2024 20:22:14 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   700/  4000] |  699/7664 batches | ms/batch 195.4747 | train_loss 8.041503e-02 | lr 8.684211e-05
08/12/2024 20:22:24 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   750/  4000] |  749/7664 batches | ms/batch 195.3397 | train_loss 7.774785e-02 | lr 8.552632e-05
08/12/2024 20:22:35 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   800/  4000] |  799/7664 batches | ms/batch 195.9599 | train_loss 7.900577e-02 | lr 8.421053e-05
08/12/2024 20:22:45 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   850/  4000] |  849/7664 batches | ms/batch 195.4594 | train_loss 7.686418e-02 | lr 8.289474e-05
08/12/2024 20:22:56 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   900/  4000] |  899/7664 batches | ms/batch 194.5063 | train_loss 7.839671e-02 | lr 8.157895e-05
08/12/2024 20:23:07 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   950/  4000] |  949/7664 batches | ms/batch 196.2214 | train_loss 7.753819e-02 | lr 8.026316e-05
08/12/2024 20:23:17 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1000/  4000] |  999/7664 batches | ms/batch 195.7413 | train_loss 7.532821e-02 | lr 7.894737e-05
08/12/2024 20:23:17 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23951929/tmpvqy704_f at global_step 1000 ****
08/12/2024 20:23:18 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/12/2024 20:23:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1050/  4000] | 1049/7664 batches | ms/batch 196.3661 | train_loss 7.646599e-02 | lr 7.763158e-05
08/12/2024 20:23:39 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1100/  4000] | 1099/7664 batches | ms/batch 194.5590 | train_loss 7.486120e-02 | lr 7.631579e-05
08/12/2024 20:23:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1150/  4000] | 1149/7664 batches | ms/batch 196.4860 | train_loss 7.484861e-02 | lr 7.500000e-05
08/12/2024 20:24:00 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1200/  4000] | 1199/7664 batches | ms/batch 196.3230 | train_loss 7.450227e-02 | lr 7.368421e-05
08/12/2024 20:24:11 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1250/  4000] | 1249/7664 batches | ms/batch 196.4903 | train_loss 7.361147e-02 | lr 7.236842e-05
08/12/2024 20:24:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1300/  4000] | 1299/7664 batches | ms/batch 194.6135 | train_loss 7.443648e-02 | lr 7.105263e-05
08/12/2024 20:24:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1350/  4000] | 1349/7664 batches | ms/batch 196.0791 | train_loss 7.304943e-02 | lr 6.973684e-05
08/12/2024 20:24:43 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1400/  4000] | 1399/7664 batches | ms/batch 195.8109 | train_loss 7.346522e-02 | lr 6.842105e-05
08/12/2024 20:24:53 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1450/  4000] | 1449/7664 batches | ms/batch 196.1054 | train_loss 7.148139e-02 | lr 6.710526e-05
08/12/2024 20:25:04 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1500/  4000] | 1499/7664 batches | ms/batch 195.8186 | train_loss 7.143461e-02 | lr 6.578947e-05
08/12/2024 20:25:14 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1550/  4000] | 1549/7664 batches | ms/batch 194.3975 | train_loss 7.134429e-02 | lr 6.447368e-05
08/12/2024 20:25:25 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1600/  4000] | 1599/7664 batches | ms/batch 196.0210 | train_loss 7.076521e-02 | lr 6.315789e-05
08/12/2024 20:25:36 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1650/  4000] | 1649/7664 batches | ms/batch 195.8996 | train_loss 7.093555e-02 | lr 6.184211e-05
08/12/2024 20:25:46 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1700/  4000] | 1699/7664 batches | ms/batch 195.9619 | train_loss 6.844366e-02 | lr 6.052632e-05
08/12/2024 20:25:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1750/  4000] | 1749/7664 batches | ms/batch 194.4211 | train_loss 7.111757e-02 | lr 5.921053e-05
08/12/2024 20:26:08 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1800/  4000] | 1799/7664 batches | ms/batch 195.8912 | train_loss 6.830221e-02 | lr 5.789474e-05
08/12/2024 20:26:18 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1850/  4000] | 1849/7664 batches | ms/batch 196.2199 | train_loss 6.839294e-02 | lr 5.657895e-05
08/12/2024 20:26:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1900/  4000] | 1899/7664 batches | ms/batch 195.9354 | train_loss 6.890541e-02 | lr 5.526316e-05
08/12/2024 20:26:39 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1950/  4000] | 1949/7664 batches | ms/batch 194.5026 | train_loss 6.831208e-02 | lr 5.394737e-05
08/12/2024 20:26:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2000/  4000] | 1999/7664 batches | ms/batch 197.8495 | train_loss 7.035463e-02 | lr 5.263158e-05
08/12/2024 20:26:50 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23951929/tmpvqy704_f at global_step 2000 ****
08/12/2024 20:26:50 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/12/2024 20:27:01 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2050/  4000] | 2049/7664 batches | ms/batch 195.9010 | train_loss 6.809264e-02 | lr 5.131579e-05
08/12/2024 20:27:11 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2100/  4000] | 2099/7664 batches | ms/batch 195.4970 | train_loss 6.763146e-02 | lr 5.000000e-05
08/12/2024 20:27:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2150/  4000] | 2149/7664 batches | ms/batch 195.6369 | train_loss 6.687079e-02 | lr 4.868421e-05
08/12/2024 20:27:33 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2200/  4000] | 2199/7664 batches | ms/batch 194.1960 | train_loss 6.807742e-02 | lr 4.736842e-05
08/12/2024 20:27:43 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2250/  4000] | 2249/7664 batches | ms/batch 195.8738 | train_loss 6.714399e-02 | lr 4.605263e-05
08/12/2024 20:27:54 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2300/  4000] | 2299/7664 batches | ms/batch 196.1582 | train_loss 6.714922e-02 | lr 4.473684e-05
08/12/2024 20:28:04 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2350/  4000] | 2349/7664 batches | ms/batch 195.7625 | train_loss 6.692419e-02 | lr 4.342105e-05
08/12/2024 20:28:15 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2400/  4000] | 2399/7664 batches | ms/batch 194.8669 | train_loss 6.756771e-02 | lr 4.210526e-05
08/12/2024 20:28:25 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2450/  4000] | 2449/7664 batches | ms/batch 196.0392 | train_loss 6.773027e-02 | lr 4.078947e-05
08/12/2024 20:28:36 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2500/  4000] | 2499/7664 batches | ms/batch 195.7753 | train_loss 6.606300e-02 | lr 3.947368e-05
08/12/2024 20:28:47 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2550/  4000] | 2549/7664 batches | ms/batch 195.9622 | train_loss 6.664926e-02 | lr 3.815789e-05
08/12/2024 20:28:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2600/  4000] | 2599/7664 batches | ms/batch 194.3968 | train_loss 6.688935e-02 | lr 3.684211e-05
08/12/2024 20:29:08 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2650/  4000] | 2649/7664 batches | ms/batch 195.7259 | train_loss 6.645128e-02 | lr 3.552632e-05
08/12/2024 20:29:18 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2700/  4000] | 2699/7664 batches | ms/batch 196.1843 | train_loss 6.663090e-02 | lr 3.421053e-05
08/12/2024 20:29:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2750/  4000] | 2749/7664 batches | ms/batch 195.8305 | train_loss 6.517967e-02 | lr 3.289474e-05
08/12/2024 20:29:40 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2800/  4000] | 2799/7664 batches | ms/batch 196.4220 | train_loss 6.580363e-02 | lr 3.157895e-05
08/12/2024 20:29:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2850/  4000] | 2849/7664 batches | ms/batch 194.8955 | train_loss 6.527578e-02 | lr 3.026316e-05
08/12/2024 20:30:01 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2900/  4000] | 2899/7664 batches | ms/batch 196.1498 | train_loss 6.390313e-02 | lr 2.894737e-05
08/12/2024 20:30:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2950/  4000] | 2949/7664 batches | ms/batch 196.1425 | train_loss 6.591393e-02 | lr 2.763158e-05
08/12/2024 20:30:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3000/  4000] | 2999/7664 batches | ms/batch 196.5254 | train_loss 6.404861e-02 | lr 2.631579e-05
08/12/2024 20:30:22 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23951929/tmpvqy704_f at global_step 3000 ****
08/12/2024 20:30:23 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/12/2024 20:30:33 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3050/  4000] | 3049/7664 batches | ms/batch 194.4114 | train_loss 6.559200e-02 | lr 2.500000e-05
08/12/2024 20:30:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3100/  4000] | 3099/7664 batches | ms/batch 195.5932 | train_loss 6.373536e-02 | lr 2.368421e-05
08/12/2024 20:30:54 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3150/  4000] | 3149/7664 batches | ms/batch 195.6144 | train_loss 6.362545e-02 | lr 2.236842e-05
08/12/2024 20:31:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3200/  4000] | 3199/7664 batches | ms/batch 195.8937 | train_loss 6.202856e-02 | lr 2.105263e-05
08/12/2024 20:31:15 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3250/  4000] | 3249/7664 batches | ms/batch 194.3822 | train_loss 6.431960e-02 | lr 1.973684e-05
08/12/2024 20:31:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3300/  4000] | 3299/7664 batches | ms/batch 196.1295 | train_loss 6.401962e-02 | lr 1.842105e-05
08/12/2024 20:31:37 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3350/  4000] | 3349/7664 batches | ms/batch 195.8688 | train_loss 6.583191e-02 | lr 1.710526e-05
08/12/2024 20:31:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3400/  4000] | 3399/7664 batches | ms/batch 196.0674 | train_loss 6.426073e-02 | lr 1.578947e-05
08/12/2024 20:31:58 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3450/  4000] | 3449/7664 batches | ms/batch 195.6451 | train_loss 6.294380e-02 | lr 1.447368e-05
08/12/2024 20:32:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3500/  4000] | 3499/7664 batches | ms/batch 194.4373 | train_loss 6.402828e-02 | lr 1.315789e-05
08/12/2024 20:32:19 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3550/  4000] | 3549/7664 batches | ms/batch 195.9137 | train_loss 6.301396e-02 | lr 1.184211e-05
08/12/2024 20:32:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3600/  4000] | 3599/7664 batches | ms/batch 195.8951 | train_loss 6.076866e-02 | lr 1.052632e-05
08/12/2024 20:32:40 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3650/  4000] | 3649/7664 batches | ms/batch 195.9457 | train_loss 6.263485e-02 | lr 9.210526e-06
08/12/2024 20:32:51 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3700/  4000] | 3699/7664 batches | ms/batch 194.2442 | train_loss 6.243993e-02 | lr 7.894737e-06
08/12/2024 20:33:02 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3750/  4000] | 3749/7664 batches | ms/batch 195.5568 | train_loss 6.284771e-02 | lr 6.578947e-06
08/12/2024 20:33:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3800/  4000] | 3799/7664 batches | ms/batch 195.9242 | train_loss 6.301536e-02 | lr 5.263158e-06
08/12/2024 20:33:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3850/  4000] | 3849/7664 batches | ms/batch 195.5161 | train_loss 6.246012e-02 | lr 3.947368e-06
08/12/2024 20:33:33 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3900/  4000] | 3899/7664 batches | ms/batch 194.5327 | train_loss 6.297294e-02 | lr 2.631579e-06
08/12/2024 20:33:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3950/  4000] | 3949/7664 batches | ms/batch 195.9425 | train_loss 6.260537e-02 | lr 1.315789e-06
08/12/2024 20:33:54 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4000/  4000] | 3999/7664 batches | ms/batch 197.3841 | train_loss 6.185049e-02 | lr 0.000000e+00
08/12/2024 20:33:54 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23951929/tmpvqy704_f at global_step 4000 ****
08/12/2024 20:33:55 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/12/2024 20:33:56 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23951929/tmpvqy704_f
08/12/2024 20:33:56 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([490449, 128])) in OVA mode
08/12/2024 20:33:56 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=490449
08/12/2024 20:44:05 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/12/2024 20:44:15 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/12/2024 20:46:03 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/12/2024 20:46:17 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=5.1052464170586545
08/12/2024 20:46:18 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
08/12/2024 20:46:49 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23951929/tmpgo8tvttd/X_trn.pt
08/12/2024 20:46:49 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/12/2024 20:50:30 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/12/2024 20:50:30 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/12/2024 20:50:31 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/12/2024 20:50:31 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=490449
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/12/2024 20:50:49 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/12/2024 20:50:49 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 490449
08/12/2024 20:50:49 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
08/12/2024 20:50:49 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 192
08/12/2024 20:50:49 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
08/12/2024 20:50:49 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/12/2024 20:50:49 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/12/2024 20:50:49 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/12/2024 20:50:49 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 4000
Traceback (most recent call last):
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/train.py", line 564, in <module>
    do_train(args)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/train.py", line 548, in do_train
    xtf = XTransformer.train(
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/model.py", line 447, in train
    res_dict = TransformerMatcher.train(
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/matcher.py", line 1390, in train
    matcher.fine_tune_encoder(prob, val_prob=val_prob, val_csr_codes=val_csr_codes)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/matcher.py", line 1130, in fine_tune_encoder
    torch.nn.utils.clip_grad_norm_(
  File "/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py", line 55, in clip_grad_norm_
    norms.extend(torch._foreach_norm(grads, norm_type))
NotImplementedError: Could not run 'aten::_foreach_norm.Scalar' with arguments from the 'SparseCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_foreach_norm.Scalar' is only available for these backends: [CPU, CUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

CPU: registered at aten/src/ATen/RegisterCPU.cpp:31188 [kernel]
CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44143 [kernel]
BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]
Functionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]
Named: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]
ZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]
AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
Tracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:17079 [kernel]
AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]
AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]
FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]
FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]
Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]
PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]
PreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]
PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]

Traceback (most recent call last):
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/predict.py", line 176, in <module>
    do_predict(args)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/predict.py", line 145, in do_predict
    xtf = XTransformer.load(args.model_folder)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/model.py", line 196, in load
    text_encoder = TransformerMatcher.load(os.path.join(load_dir, "text_encoder"))
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/matcher.py", line 386, in load
    raise ValueError(f"text_encoder does not exist at {encoder_dir}")
ValueError: text_encoder does not exist at models/amazon-670k/bert2/text_encoder/text_encoder
08/12/2024 20:51:12 - INFO - __main__ - Setting random seed 0
08/12/2024 20:51:14 - INFO - __main__ - Loaded training feature matrix with shape=(490449, 135909)
08/12/2024 20:51:15 - INFO - __main__ - Loaded training label matrix with shape=(490449, 670091)
08/12/2024 20:51:16 - INFO - __main__ - Loaded 490449 training sequences
08/12/2024 20:51:28 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 32768, 670091]
08/12/2024 20:51:28 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 32768]
08/12/2024 20:51:28 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
08/12/2024 20:51:29 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
08/12/2024 20:51:29 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=490449 truncation=128*****
08/12/2024 20:52:14 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=44.49038887023926 *****
08/12/2024 20:52:37 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23951929/tmp820e5pq1/X_trn.pt
08/12/2024 20:52:38 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/12/2024 20:52:39 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/12/2024 20:52:39 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=490449
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/12/2024 20:52:39 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/12/2024 20:52:39 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 490449
08/12/2024 20:52:39 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
08/12/2024 20:52:39 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
08/12/2024 20:52:39 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/12/2024 20:52:39 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/12/2024 20:52:39 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/12/2024 20:52:39 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 4000
08/12/2024 20:52:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/  4000] |   49/7664 batches | ms/batch 223.0104 | train_loss 9.160960e-01 | lr 2.500000e-05
08/12/2024 20:53:06 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/  4000] |   99/7664 batches | ms/batch 191.6534 | train_loss 2.184783e-01 | lr 5.000000e-05
08/12/2024 20:53:16 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/  4000] |  149/7664 batches | ms/batch 193.2134 | train_loss 1.733325e-01 | lr 7.500000e-05
08/12/2024 20:53:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/  4000] |  199/7664 batches | ms/batch 193.9956 | train_loss 1.590602e-01 | lr 1.000000e-04
08/12/2024 20:53:37 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   250/  4000] |  249/7664 batches | ms/batch 192.2707 | train_loss 1.347657e-01 | lr 9.868421e-05
08/12/2024 20:53:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   300/  4000] |  299/7664 batches | ms/batch 194.4958 | train_loss 1.168916e-01 | lr 9.736842e-05
08/12/2024 20:53:58 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   350/  4000] |  349/7664 batches | ms/batch 194.3734 | train_loss 1.047543e-01 | lr 9.605263e-05
08/12/2024 20:54:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   400/  4000] |  399/7664 batches | ms/batch 194.3215 | train_loss 9.776142e-02 | lr 9.473684e-05
08/12/2024 20:54:19 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   450/  4000] |  449/7664 batches | ms/batch 193.0745 | train_loss 9.402736e-02 | lr 9.342105e-05
08/12/2024 20:54:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   500/  4000] |  499/7664 batches | ms/batch 194.8103 | train_loss 9.076331e-02 | lr 9.210526e-05
08/12/2024 20:54:40 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   550/  4000] |  549/7664 batches | ms/batch 194.9165 | train_loss 8.904116e-02 | lr 9.078947e-05
08/12/2024 20:54:51 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   600/  4000] |  599/7664 batches | ms/batch 195.0233 | train_loss 8.663018e-02 | lr 8.947368e-05
08/12/2024 20:55:02 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   650/  4000] |  649/7664 batches | ms/batch 193.5510 | train_loss 8.463834e-02 | lr 8.815789e-05
08/12/2024 20:55:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   700/  4000] |  699/7664 batches | ms/batch 195.1825 | train_loss 8.106690e-02 | lr 8.684211e-05
08/12/2024 20:55:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   750/  4000] |  749/7664 batches | ms/batch 195.1163 | train_loss 7.871968e-02 | lr 8.552632e-05
08/12/2024 20:55:33 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   800/  4000] |  799/7664 batches | ms/batch 195.5312 | train_loss 7.853749e-02 | lr 8.421053e-05
08/12/2024 20:55:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   850/  4000] |  849/7664 batches | ms/batch 195.1744 | train_loss 7.869183e-02 | lr 8.289474e-05
08/12/2024 20:55:54 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   900/  4000] |  899/7664 batches | ms/batch 193.6949 | train_loss 7.935061e-02 | lr 8.157895e-05
08/12/2024 20:56:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   950/  4000] |  949/7664 batches | ms/batch 195.1033 | train_loss 7.713596e-02 | lr 8.026316e-05
08/12/2024 20:56:16 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1000/  4000] |  999/7664 batches | ms/batch 195.4138 | train_loss 7.513462e-02 | lr 7.894737e-05
08/12/2024 20:56:16 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23951929/tmp05zorz1v at global_step 1000 ****
08/12/2024 20:56:16 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/12/2024 20:56:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1050/  4000] | 1049/7664 batches | ms/batch 195.2189 | train_loss 7.789634e-02 | lr 7.763158e-05
08/12/2024 20:56:37 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1100/  4000] | 1099/7664 batches | ms/batch 194.0010 | train_loss 7.554615e-02 | lr 7.631579e-05
08/12/2024 20:56:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1150/  4000] | 1149/7664 batches | ms/batch 195.4813 | train_loss 7.527565e-02 | lr 7.500000e-05
08/12/2024 20:56:58 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1200/  4000] | 1199/7664 batches | ms/batch 195.3555 | train_loss 7.472258e-02 | lr 7.368421e-05
08/12/2024 20:57:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1250/  4000] | 1249/7664 batches | ms/batch 195.7292 | train_loss 7.383678e-02 | lr 7.236842e-05
08/12/2024 20:57:20 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1300/  4000] | 1299/7664 batches | ms/batch 194.3694 | train_loss 7.305356e-02 | lr 7.105263e-05
08/12/2024 20:57:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1350/  4000] | 1349/7664 batches | ms/batch 195.6212 | train_loss 7.266246e-02 | lr 6.973684e-05
08/12/2024 20:57:41 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1400/  4000] | 1399/7664 batches | ms/batch 195.6841 | train_loss 7.305307e-02 | lr 6.842105e-05
08/12/2024 20:57:51 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1450/  4000] | 1449/7664 batches | ms/batch 195.4167 | train_loss 7.194798e-02 | lr 6.710526e-05
08/12/2024 20:58:02 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1500/  4000] | 1499/7664 batches | ms/batch 194.8645 | train_loss 7.138120e-02 | lr 6.578947e-05
08/12/2024 20:58:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1550/  4000] | 1549/7664 batches | ms/batch 193.6894 | train_loss 7.173536e-02 | lr 6.447368e-05
08/12/2024 20:58:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1600/  4000] | 1599/7664 batches | ms/batch 195.2184 | train_loss 7.099931e-02 | lr 6.315789e-05
08/12/2024 20:58:33 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1650/  4000] | 1649/7664 batches | ms/batch 195.0002 | train_loss 7.042861e-02 | lr 6.184211e-05
08/12/2024 20:58:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1700/  4000] | 1699/7664 batches | ms/batch 195.3740 | train_loss 6.927696e-02 | lr 6.052632e-05
08/12/2024 20:58:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1750/  4000] | 1749/7664 batches | ms/batch 193.7724 | train_loss 7.170707e-02 | lr 5.921053e-05
08/12/2024 20:59:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1800/  4000] | 1799/7664 batches | ms/batch 195.0344 | train_loss 6.753330e-02 | lr 5.789474e-05
08/12/2024 20:59:16 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1850/  4000] | 1849/7664 batches | ms/batch 195.4577 | train_loss 6.916566e-02 | lr 5.657895e-05
08/12/2024 20:59:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1900/  4000] | 1899/7664 batches | ms/batch 195.3402 | train_loss 6.951204e-02 | lr 5.526316e-05
08/12/2024 20:59:37 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1950/  4000] | 1949/7664 batches | ms/batch 194.3567 | train_loss 6.841279e-02 | lr 5.394737e-05
08/12/2024 20:59:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2000/  4000] | 1999/7664 batches | ms/batch 195.9379 | train_loss 7.015569e-02 | lr 5.263158e-05
08/12/2024 20:59:48 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23951929/tmp05zorz1v at global_step 2000 ****
08/12/2024 20:59:48 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/12/2024 20:59:59 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2050/  4000] | 2049/7664 batches | ms/batch 195.4301 | train_loss 6.881862e-02 | lr 5.131579e-05
08/12/2024 21:00:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2100/  4000] | 2099/7664 batches | ms/batch 195.5845 | train_loss 6.856169e-02 | lr 5.000000e-05
08/12/2024 21:00:20 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2150/  4000] | 2149/7664 batches | ms/batch 195.1312 | train_loss 6.669378e-02 | lr 4.868421e-05
08/12/2024 21:00:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2200/  4000] | 2199/7664 batches | ms/batch 193.8150 | train_loss 6.684559e-02 | lr 4.736842e-05
08/12/2024 21:00:41 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2250/  4000] | 2249/7664 batches | ms/batch 195.4443 | train_loss 6.763455e-02 | lr 4.605263e-05
08/12/2024 21:00:51 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2300/  4000] | 2299/7664 batches | ms/batch 195.2258 | train_loss 6.731623e-02 | lr 4.473684e-05
08/12/2024 21:01:02 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2350/  4000] | 2349/7664 batches | ms/batch 195.6262 | train_loss 6.646059e-02 | lr 4.342105e-05
08/12/2024 21:01:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2400/  4000] | 2399/7664 batches | ms/batch 194.0601 | train_loss 6.624505e-02 | lr 4.210526e-05
08/12/2024 21:01:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2450/  4000] | 2449/7664 batches | ms/batch 195.2971 | train_loss 6.769777e-02 | lr 4.078947e-05
08/12/2024 21:01:33 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2500/  4000] | 2499/7664 batches | ms/batch 195.4922 | train_loss 6.537698e-02 | lr 3.947368e-05
08/12/2024 21:01:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2550/  4000] | 2549/7664 batches | ms/batch 195.4217 | train_loss 6.646623e-02 | lr 3.815789e-05
08/12/2024 21:01:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2600/  4000] | 2599/7664 batches | ms/batch 193.9152 | train_loss 6.646071e-02 | lr 3.684211e-05
08/12/2024 21:02:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2650/  4000] | 2649/7664 batches | ms/batch 195.6199 | train_loss 6.638672e-02 | lr 3.552632e-05
08/12/2024 21:02:16 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2700/  4000] | 2699/7664 batches | ms/batch 195.4152 | train_loss 6.607315e-02 | lr 3.421053e-05
08/12/2024 21:02:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2750/  4000] | 2749/7664 batches | ms/batch 195.8876 | train_loss 6.445704e-02 | lr 3.289474e-05
08/12/2024 21:02:37 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2800/  4000] | 2799/7664 batches | ms/batch 195.9021 | train_loss 6.495519e-02 | lr 3.157895e-05
08/12/2024 21:02:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2850/  4000] | 2849/7664 batches | ms/batch 194.5586 | train_loss 6.515759e-02 | lr 3.026316e-05
08/12/2024 21:02:58 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2900/  4000] | 2899/7664 batches | ms/batch 195.9883 | train_loss 6.507975e-02 | lr 2.894737e-05
08/12/2024 21:03:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2950/  4000] | 2949/7664 batches | ms/batch 195.9741 | train_loss 6.528772e-02 | lr 2.763158e-05
08/12/2024 21:03:19 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3000/  4000] | 2999/7664 batches | ms/batch 195.8871 | train_loss 6.438941e-02 | lr 2.631579e-05
08/12/2024 21:03:19 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23951929/tmp05zorz1v at global_step 3000 ****
08/12/2024 21:03:20 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/12/2024 21:03:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3050/  4000] | 3049/7664 batches | ms/batch 194.5072 | train_loss 6.481222e-02 | lr 2.500000e-05
08/12/2024 21:03:41 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3100/  4000] | 3099/7664 batches | ms/batch 195.9179 | train_loss 6.458099e-02 | lr 2.368421e-05
08/12/2024 21:03:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3150/  4000] | 3149/7664 batches | ms/batch 195.8402 | train_loss 6.263146e-02 | lr 2.236842e-05
08/12/2024 21:04:02 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3200/  4000] | 3199/7664 batches | ms/batch 196.1594 | train_loss 6.212358e-02 | lr 2.105263e-05
08/12/2024 21:04:13 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3250/  4000] | 3249/7664 batches | ms/batch 194.4893 | train_loss 6.489061e-02 | lr 1.973684e-05
08/12/2024 21:04:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3300/  4000] | 3299/7664 batches | ms/batch 195.8929 | train_loss 6.360353e-02 | lr 1.842105e-05
08/12/2024 21:04:34 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3350/  4000] | 3349/7664 batches | ms/batch 196.0891 | train_loss 6.557987e-02 | lr 1.710526e-05
08/12/2024 21:04:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3400/  4000] | 3399/7664 batches | ms/batch 195.8307 | train_loss 6.402145e-02 | lr 1.578947e-05
08/12/2024 21:04:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3450/  4000] | 3449/7664 batches | ms/batch 196.1280 | train_loss 6.314721e-02 | lr 1.447368e-05
08/12/2024 21:05:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3500/  4000] | 3499/7664 batches | ms/batch 194.6812 | train_loss 6.409447e-02 | lr 1.315789e-05
08/12/2024 21:05:16 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3550/  4000] | 3549/7664 batches | ms/batch 195.9598 | train_loss 6.295344e-02 | lr 1.184211e-05
08/12/2024 21:05:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3600/  4000] | 3599/7664 batches | ms/batch 196.4245 | train_loss 6.055257e-02 | lr 1.052632e-05
08/12/2024 21:05:37 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3650/  4000] | 3649/7664 batches | ms/batch 196.1412 | train_loss 6.271381e-02 | lr 9.210526e-06
08/12/2024 21:05:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3700/  4000] | 3699/7664 batches | ms/batch 195.0359 | train_loss 6.236150e-02 | lr 7.894737e-06
08/12/2024 21:05:59 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3750/  4000] | 3749/7664 batches | ms/batch 196.6847 | train_loss 6.276457e-02 | lr 6.578947e-06
08/12/2024 21:06:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3800/  4000] | 3799/7664 batches | ms/batch 196.8341 | train_loss 6.298410e-02 | lr 5.263158e-06
08/12/2024 21:06:20 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3850/  4000] | 3849/7664 batches | ms/batch 196.6517 | train_loss 6.210714e-02 | lr 3.947368e-06
08/12/2024 21:06:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3900/  4000] | 3899/7664 batches | ms/batch 195.2842 | train_loss 6.380392e-02 | lr 2.631579e-06
08/12/2024 21:06:41 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3950/  4000] | 3949/7664 batches | ms/batch 196.4175 | train_loss 6.233302e-02 | lr 1.315789e-06
08/12/2024 21:06:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4000/  4000] | 3999/7664 batches | ms/batch 195.8634 | train_loss 6.171091e-02 | lr 0.000000e+00
08/12/2024 21:06:52 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23951929/tmp05zorz1v at global_step 4000 ****
08/12/2024 21:06:52 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/12/2024 21:06:53 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23951929/tmp05zorz1v
08/12/2024 21:06:53 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([490449, 128])) in OVA mode
08/12/2024 21:06:53 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=490449
08/12/2024 21:17:03 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/12/2024 21:17:13 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/12/2024 21:19:11 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/12/2024 21:19:24 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=5.101227650581406
08/12/2024 21:19:24 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
08/12/2024 21:19:53 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23951929/tmp820e5pq1/X_trn.pt
08/12/2024 21:19:53 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/12/2024 21:23:41 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/12/2024 21:23:41 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/12/2024 21:23:41 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/12/2024 21:23:41 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=490449
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/12/2024 21:23:59 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/12/2024 21:23:59 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 490449
08/12/2024 21:23:59 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
08/12/2024 21:23:59 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 176
08/12/2024 21:23:59 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
08/12/2024 21:23:59 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/12/2024 21:23:59 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/12/2024 21:23:59 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/12/2024 21:23:59 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 4000
Traceback (most recent call last):
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/train.py", line 564, in <module>
    do_train(args)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/train.py", line 548, in do_train
    xtf = XTransformer.train(
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/model.py", line 447, in train
    res_dict = TransformerMatcher.train(
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/matcher.py", line 1390, in train
    matcher.fine_tune_encoder(prob, val_prob=val_prob, val_csr_codes=val_csr_codes)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/matcher.py", line 1130, in fine_tune_encoder
    torch.nn.utils.clip_grad_norm_(
  File "/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py", line 55, in clip_grad_norm_
    norms.extend(torch._foreach_norm(grads, norm_type))
NotImplementedError: Could not run 'aten::_foreach_norm.Scalar' with arguments from the 'SparseCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_foreach_norm.Scalar' is only available for these backends: [CPU, CUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

CPU: registered at aten/src/ATen/RegisterCPU.cpp:31188 [kernel]
CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44143 [kernel]
BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]
Functionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]
Named: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]
ZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]
AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]
Tracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:17079 [kernel]
AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]
AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]
FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]
FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]
Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]
PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]
PreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]
PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]

Traceback (most recent call last):
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ul/ul_student/ul_ruw26/miniconda3/envs/xr_transformer_env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/predict.py", line 176, in <module>
    do_predict(args)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/predict.py", line 145, in do_predict
    xtf = XTransformer.load(args.model_folder)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/model.py", line 196, in load
    text_encoder = TransformerMatcher.load(os.path.join(load_dir, "text_encoder"))
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/xmc/xtransformer/matcher.py", line 386, in load
    raise ValueError(f"text_encoder does not exist at {encoder_dir}")
ValueError: text_encoder does not exist at models/amazon-670k/bert3/text_encoder/text_encoder
Traceback (most recent call last):
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/run_ensemble/./ensemble_evaluate.py", line 73, in <module>
    do_evaluation(args)
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/run_ensemble/./ensemble_evaluate.py", line 59, in do_evaluation
    Y_pred = [sorted_csr(load_matrix(pp).tocsr()) for pp in args.pred_path]
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/run_ensemble/./ensemble_evaluate.py", line 59, in <listcomp>
    Y_pred = [sorted_csr(load_matrix(pp).tocsr()) for pp in args.pred_path]
  File "/pfs/data5/home/ul/ul_student/ul_ruw26/XMC_HTC/XMLmodels/pecos/pecos/utils/smat_util.py", line 117, in load_matrix
    mat = np.load(src)
  File "/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'models/amazon-670k/bert1/Pt.npz'

============================= JOB FEEDBACK =============================

NodeName=uc2n913
Job ID: 23951929
Cluster: uc2
User/Group: ul_ruw26/ul_student
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 32
CPU Utilized: 10:50:20
CPU Efficiency: 20.17% of 2-05:43:28 core-walltime
Job Wall-clock time: 01:40:44
Memory Utilized: 42.70 GB
Memory Efficiency: 29.15% of 146.48 GB
