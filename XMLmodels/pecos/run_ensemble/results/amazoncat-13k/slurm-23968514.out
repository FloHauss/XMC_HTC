------- Ensemble run at 2024-08-26 17:47:53 for amazoncat-13k ----------
08/26/2024 17:49:24 - INFO - __main__ - Setting random seed 0
08/26/2024 17:49:26 - INFO - __main__ - Loaded training feature matrix with shape=(1186239, 203882)
08/26/2024 17:49:26 - INFO - __main__ - Loaded training label matrix with shape=(1186239, 13330)
08/26/2024 17:49:29 - INFO - __main__ - Loaded 1186239 training sequences
08/26/2024 17:49:32 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 1024, 13330]
08/26/2024 17:49:32 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 1024, 13330]
08/26/2024 17:49:32 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
08/26/2024 17:49:37 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
08/26/2024 17:49:37 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=1186239 truncation=256*****
08/26/2024 17:51:39 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=121.4843361377716 *****
08/26/2024 17:52:52 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23968514/tmp0cyepx0v/X_trn.pt
08/26/2024 17:52:56 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/26/2024 17:53:15 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/26/2024 17:53:15 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/26/2024 17:53:16 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/26/2024 17:53:16 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 1186239
08/26/2024 17:53:16 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
08/26/2024 17:53:16 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
08/26/2024 17:53:16 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/26/2024 17:53:16 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/26/2024 17:53:16 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/26/2024 17:53:16 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 5000
08/26/2024 17:56:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   500/  5000] |  499/18535 batches | ms/batch 377.3834 | train_loss 3.351100e-01 | lr 2.500000e-05
08/26/2024 18:00:02 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1000/  5000] |  999/18535 batches | ms/batch 358.7953 | train_loss 6.372401e-02 | lr 5.000000e-05
08/26/2024 18:03:15 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1500/  5000] | 1499/18535 batches | ms/batch 358.2967 | train_loss 4.614919e-02 | lr 4.375000e-05
08/26/2024 18:06:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2000/  5000] | 1999/18535 batches | ms/batch 358.0793 | train_loss 4.059318e-02 | lr 3.750000e-05
08/26/2024 18:09:39 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2500/  5000] | 2499/18535 batches | ms/batch 358.0350 | train_loss 3.752303e-02 | lr 3.125000e-05
08/26/2024 18:12:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3000/  5000] | 2999/18535 batches | ms/batch 361.2915 | train_loss 3.580604e-02 | lr 2.500000e-05
08/26/2024 18:16:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3500/  5000] | 3499/18535 batches | ms/batch 360.4673 | train_loss 3.415933e-02 | lr 1.875000e-05
08/26/2024 18:19:17 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4000/  5000] | 3999/18535 batches | ms/batch 359.7085 | train_loss 3.318854e-02 | lr 1.250000e-05
08/26/2024 18:22:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4500/  5000] | 4499/18535 batches | ms/batch 360.4345 | train_loss 3.233852e-02 | lr 6.250000e-06
08/26/2024 18:25:42 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  5000/  5000] | 4999/18535 batches | ms/batch 360.3062 | train_loss 3.109883e-02 | lr 0.000000e+00
08/26/2024 18:25:42 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmpuno7f5xt at global_step 5000 ****
08/26/2024 18:25:43 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/26/2024 18:25:45 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968514/tmpuno7f5xt
08/26/2024 18:25:46 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([1186239, 256])) in OVA mode
08/26/2024 18:25:46 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
08/26/2024 19:09:00 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/26/2024 19:09:26 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/26/2024 19:12:59 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/26/2024 19:13:37 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=1024, avr_M_nnz=50.002184214142346
08/26/2024 19:13:41 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
08/26/2024 19:15:14 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23968514/tmp0cyepx0v/X_trn.pt
08/26/2024 19:15:14 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/26/2024 19:42:18 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/26/2024 19:42:18 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/26/2024 19:42:18 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/26/2024 19:42:20 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/26/2024 19:43:11 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/26/2024 19:43:11 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 1186239
08/26/2024 19:43:11 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 1024
08/26/2024 19:43:11 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 440
08/26/2024 19:43:11 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
08/26/2024 19:43:11 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/26/2024 19:43:11 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/26/2024 19:43:11 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/26/2024 19:43:11 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 10000
08/26/2024 19:46:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   500/ 10000] |  499/18535 batches | ms/batch 364.7837 | train_loss 1.331941e-01 | lr 2.500000e-05
08/26/2024 19:50:08 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1000/ 10000] |  999/18535 batches | ms/batch 364.1163 | train_loss 1.128429e-01 | lr 5.000000e-05
08/26/2024 19:53:25 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1500/ 10000] | 1499/18535 batches | ms/batch 364.2849 | train_loss 1.095118e-01 | lr 4.722222e-05
08/26/2024 19:56:45 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2000/ 10000] | 1999/18535 batches | ms/batch 367.8473 | train_loss 1.082347e-01 | lr 4.444444e-05
08/26/2024 20:00:03 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2500/ 10000] | 2499/18535 batches | ms/batch 365.4768 | train_loss 1.074842e-01 | lr 4.166667e-05
08/26/2024 20:03:20 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3000/ 10000] | 2999/18535 batches | ms/batch 362.6110 | train_loss 1.071029e-01 | lr 3.888889e-05
08/26/2024 20:06:36 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3500/ 10000] | 3499/18535 batches | ms/batch 361.0950 | train_loss 1.063307e-01 | lr 3.611111e-05
08/26/2024 20:09:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4000/ 10000] | 3999/18535 batches | ms/batch 362.2652 | train_loss 1.060798e-01 | lr 3.333333e-05
08/26/2024 20:13:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4500/ 10000] | 4499/18535 batches | ms/batch 362.2619 | train_loss 1.058658e-01 | lr 3.055556e-05
08/26/2024 20:16:25 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  5000/ 10000] | 4999/18535 batches | ms/batch 362.7810 | train_loss 1.054427e-01 | lr 2.777778e-05
08/26/2024 20:16:25 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmp_jy20s7u at global_step 5000 ****
08/26/2024 20:16:26 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/26/2024 20:19:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  5500/ 10000] | 5499/18535 batches | ms/batch 378.9824 | train_loss 1.052971e-01 | lr 2.500000e-05
08/26/2024 20:23:08 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  6000/ 10000] | 5999/18535 batches | ms/batch 361.6152 | train_loss 1.049449e-01 | lr 2.222222e-05
08/26/2024 20:26:24 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  6500/ 10000] | 6499/18535 batches | ms/batch 361.3461 | train_loss 1.049000e-01 | lr 1.944444e-05
08/26/2024 20:29:41 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  7000/ 10000] | 6999/18535 batches | ms/batch 362.5144 | train_loss 1.047784e-01 | lr 1.666667e-05
08/26/2024 20:32:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  7500/ 10000] | 7499/18535 batches | ms/batch 361.7543 | train_loss 1.043705e-01 | lr 1.388889e-05
08/26/2024 20:36:13 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  8000/ 10000] | 7999/18535 batches | ms/batch 362.0676 | train_loss 1.040641e-01 | lr 1.111111e-05
08/26/2024 20:39:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  8500/ 10000] | 8499/18535 batches | ms/batch 361.8461 | train_loss 1.040623e-01 | lr 8.333333e-06
08/26/2024 20:42:46 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  9000/ 10000] | 8999/18535 batches | ms/batch 362.1005 | train_loss 1.038458e-01 | lr 5.555556e-06
08/26/2024 20:46:03 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  9500/ 10000] | 9499/18535 batches | ms/batch 361.8798 | train_loss 1.037472e-01 | lr 2.777778e-06
08/26/2024 20:49:21 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][ 10000/ 10000] | 9999/18535 batches | ms/batch 364.8589 | train_loss 1.036373e-01 | lr 0.000000e+00
08/26/2024 20:49:21 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmp_jy20s7u at global_step 10000 ****
08/26/2024 20:49:21 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/26/2024 20:49:24 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968514/tmp_jy20s7u
08/26/2024 20:49:26 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((1186239, 1024)) with avr_nnz=400.0
08/26/2024 20:49:26 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
08/26/2024 21:33:36 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/26/2024 21:34:00 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/26/2024 21:43:35 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/26/2024 21:44:14 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=13330, avr_M_nnz=50.04579094094866
08/26/2024 21:44:18 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
08/26/2024 21:45:52 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23968514/tmp0cyepx0v/X_trn.pt
08/26/2024 21:45:52 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/26/2024 22:07:57 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/26/2024 22:07:57 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/26/2024 22:07:57 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/26/2024 22:08:00 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/26/2024 22:09:02 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/26/2024 22:09:02 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 1186239
08/26/2024 22:09:02 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 13330
08/26/2024 22:09:02 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 781
08/26/2024 22:09:02 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
08/26/2024 22:09:02 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/26/2024 22:09:02 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/26/2024 22:09:02 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/26/2024 22:09:02 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 30000
08/26/2024 22:12:42 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   500/ 30000] |  499/18535 batches | ms/batch 366.3191 | train_loss 1.885429e-01 | lr 2.500000e-05
08/26/2024 22:16:01 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  1000/ 30000] |  999/18535 batches | ms/batch 364.8404 | train_loss 1.849713e-01 | lr 5.000000e-05
08/26/2024 22:19:20 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  1500/ 30000] | 1499/18535 batches | ms/batch 365.1652 | train_loss 1.841213e-01 | lr 4.913793e-05
08/26/2024 22:22:40 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  2000/ 30000] | 1999/18535 batches | ms/batch 364.4086 | train_loss 1.827279e-01 | lr 4.827586e-05
08/26/2024 22:25:59 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  2500/ 30000] | 2499/18535 batches | ms/batch 363.6469 | train_loss 1.822983e-01 | lr 4.741379e-05
08/26/2024 22:29:18 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  3000/ 30000] | 2999/18535 batches | ms/batch 364.0429 | train_loss 1.818787e-01 | lr 4.655172e-05
08/26/2024 22:32:36 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  3500/ 30000] | 3499/18535 batches | ms/batch 364.1434 | train_loss 1.814204e-01 | lr 4.568966e-05
08/26/2024 22:35:56 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  4000/ 30000] | 3999/18535 batches | ms/batch 364.7959 | train_loss 1.811659e-01 | lr 4.482759e-05
08/26/2024 22:39:14 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  4500/ 30000] | 4499/18535 batches | ms/batch 363.9597 | train_loss 1.809560e-01 | lr 4.396552e-05
08/26/2024 22:42:33 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  5000/ 30000] | 4999/18535 batches | ms/batch 363.8655 | train_loss 1.808685e-01 | lr 4.310345e-05
08/26/2024 22:45:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  5500/ 30000] | 5499/18535 batches | ms/batch 363.7546 | train_loss 1.804287e-01 | lr 4.224138e-05
08/26/2024 22:49:17 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  6000/ 30000] | 5999/18535 batches | ms/batch 375.7614 | train_loss 1.801622e-01 | lr 4.137931e-05
08/26/2024 22:53:01 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  6500/ 30000] | 6499/18535 batches | ms/batch 413.9565 | train_loss 1.801738e-01 | lr 4.051724e-05
08/26/2024 22:56:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  7000/ 30000] | 6999/18535 batches | ms/batch 372.7231 | train_loss 1.798492e-01 | lr 3.965517e-05
08/26/2024 22:59:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  7500/ 30000] | 7499/18535 batches | ms/batch 369.1458 | train_loss 1.796341e-01 | lr 3.879310e-05
08/26/2024 23:03:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  8000/ 30000] | 7999/18535 batches | ms/batch 399.9527 | train_loss 1.795737e-01 | lr 3.793103e-05
08/26/2024 23:07:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  8500/ 30000] | 8499/18535 batches | ms/batch 403.9153 | train_loss 1.794009e-01 | lr 3.706897e-05
08/26/2024 23:10:43 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  9000/ 30000] | 8999/18535 batches | ms/batch 400.2962 | train_loss 1.792106e-01 | lr 3.620690e-05
08/26/2024 23:14:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  9500/ 30000] | 9499/18535 batches | ms/batch 401.6787 | train_loss 1.791433e-01 | lr 3.534483e-05
08/26/2024 23:17:56 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 10000/ 30000] | 9999/18535 batches | ms/batch 381.6797 | train_loss 1.791485e-01 | lr 3.448276e-05
08/26/2024 23:17:56 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmpc8szzwt0 at global_step 10000 ****
08/26/2024 23:17:59 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/26/2024 23:21:33 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 10500/ 30000] | 10499/18535 batches | ms/batch 391.3459 | train_loss 1.788656e-01 | lr 3.362069e-05
08/26/2024 23:25:01 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 11000/ 30000] | 10999/18535 batches | ms/batch 383.7213 | train_loss 1.789795e-01 | lr 3.275862e-05
08/26/2024 23:28:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 11500/ 30000] | 11499/18535 batches | ms/batch 369.0177 | train_loss 1.786913e-01 | lr 3.189655e-05
08/26/2024 23:31:45 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 12000/ 30000] | 11999/18535 batches | ms/batch 368.5985 | train_loss 1.785155e-01 | lr 3.103448e-05
08/26/2024 23:35:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 12500/ 30000] | 12499/18535 batches | ms/batch 365.2400 | train_loss 1.786182e-01 | lr 3.017241e-05
08/26/2024 23:38:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 13000/ 30000] | 12999/18535 batches | ms/batch 366.8161 | train_loss 1.784848e-01 | lr 2.931034e-05
08/26/2024 23:41:49 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 13500/ 30000] | 13499/18535 batches | ms/batch 373.3469 | train_loss 1.782990e-01 | lr 2.844828e-05
08/26/2024 23:45:11 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 14000/ 30000] | 13999/18535 batches | ms/batch 368.3423 | train_loss 1.783075e-01 | lr 2.758621e-05
08/26/2024 23:48:33 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 14500/ 30000] | 14499/18535 batches | ms/batch 366.9921 | train_loss 1.781606e-01 | lr 2.672414e-05
08/26/2024 23:51:54 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 15000/ 30000] | 14999/18535 batches | ms/batch 367.2462 | train_loss 1.782073e-01 | lr 2.586207e-05
08/26/2024 23:55:17 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 15500/ 30000] | 15499/18535 batches | ms/batch 372.3580 | train_loss 1.781552e-01 | lr 2.500000e-05
08/26/2024 23:58:49 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 16000/ 30000] | 15999/18535 batches | ms/batch 386.9411 | train_loss 1.779164e-01 | lr 2.413793e-05
08/27/2024 00:02:11 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 16500/ 30000] | 16499/18535 batches | ms/batch 371.2484 | train_loss 1.778327e-01 | lr 2.327586e-05
08/27/2024 00:05:35 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 17000/ 30000] | 16999/18535 batches | ms/batch 371.5842 | train_loss 1.778511e-01 | lr 2.241379e-05
08/27/2024 00:09:02 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 17500/ 30000] | 17499/18535 batches | ms/batch 380.3579 | train_loss 1.777396e-01 | lr 2.155172e-05
08/27/2024 00:12:34 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 18000/ 30000] | 17999/18535 batches | ms/batch 389.0233 | train_loss 1.776210e-01 | lr 2.068966e-05
08/27/2024 00:15:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 18500/ 30000] | 18499/18535 batches | ms/batch 370.2617 | train_loss 1.775621e-01 | lr 1.982759e-05
08/27/2024 00:19:52 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 19000/ 30000] |  464/18535 batches | ms/batch 385.4041 | train_loss 1.765381e-01 | lr 1.896552e-05
08/27/2024 00:23:25 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 19500/ 30000] |  964/18535 batches | ms/batch 391.9248 | train_loss 1.765010e-01 | lr 1.810345e-05
08/27/2024 00:26:55 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 20000/ 30000] | 1464/18535 batches | ms/batch 384.1894 | train_loss 1.764559e-01 | lr 1.724138e-05
08/27/2024 00:26:55 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmpc8szzwt0 at global_step 20000 ****
08/27/2024 00:26:57 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/27/2024 00:30:28 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 20500/ 30000] | 1964/18535 batches | ms/batch 386.7404 | train_loss 1.763696e-01 | lr 1.637931e-05
08/27/2024 00:33:50 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 21000/ 30000] | 2464/18535 batches | ms/batch 369.0264 | train_loss 1.763051e-01 | lr 1.551724e-05
08/27/2024 00:37:12 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 21500/ 30000] | 2964/18535 batches | ms/batch 369.3063 | train_loss 1.762368e-01 | lr 1.465517e-05
08/27/2024 00:40:35 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 22000/ 30000] | 3464/18535 batches | ms/batch 372.0803 | train_loss 1.762351e-01 | lr 1.379310e-05
08/27/2024 00:43:59 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 22500/ 30000] | 3964/18535 batches | ms/batch 372.8141 | train_loss 1.762256e-01 | lr 1.293103e-05
08/27/2024 00:47:23 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 23000/ 30000] | 4464/18535 batches | ms/batch 373.2899 | train_loss 1.762877e-01 | lr 1.206897e-05
08/27/2024 00:50:49 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 23500/ 30000] | 4964/18535 batches | ms/batch 376.5302 | train_loss 1.761164e-01 | lr 1.120690e-05
08/27/2024 00:54:13 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 24000/ 30000] | 5464/18535 batches | ms/batch 373.9172 | train_loss 1.759625e-01 | lr 1.034483e-05
08/27/2024 00:57:41 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 24500/ 30000] | 5964/18535 batches | ms/batch 377.4947 | train_loss 1.759666e-01 | lr 9.482759e-06
08/27/2024 01:01:03 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 25000/ 30000] | 6464/18535 batches | ms/batch 370.9516 | train_loss 1.760539e-01 | lr 8.620690e-06
08/27/2024 01:04:26 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 25500/ 30000] | 6964/18535 batches | ms/batch 370.4071 | train_loss 1.758437e-01 | lr 7.758621e-06
08/27/2024 01:07:57 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 26000/ 30000] | 7464/18535 batches | ms/batch 385.4628 | train_loss 1.758666e-01 | lr 6.896552e-06
08/27/2024 01:11:28 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 26500/ 30000] | 7964/18535 batches | ms/batch 386.8722 | train_loss 1.758106e-01 | lr 6.034483e-06
08/27/2024 01:15:07 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 27000/ 30000] | 8464/18535 batches | ms/batch 401.2911 | train_loss 1.757850e-01 | lr 5.172414e-06
08/27/2024 01:18:40 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 27500/ 30000] | 8964/18535 batches | ms/batch 392.7235 | train_loss 1.756433e-01 | lr 4.310345e-06
08/27/2024 01:22:06 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 28000/ 30000] | 9464/18535 batches | ms/batch 377.6445 | train_loss 1.755473e-01 | lr 3.448276e-06
08/27/2024 01:25:42 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 28500/ 30000] | 9964/18535 batches | ms/batch 396.1283 | train_loss 1.756288e-01 | lr 2.586207e-06
08/27/2024 01:29:17 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 29000/ 30000] | 10464/18535 batches | ms/batch 395.3473 | train_loss 1.755198e-01 | lr 1.724138e-06
08/27/2024 01:32:52 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 29500/ 30000] | 10964/18535 batches | ms/batch 396.6294 | train_loss 1.755356e-01 | lr 8.620690e-07
08/27/2024 01:36:16 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 30000/ 30000] | 11464/18535 batches | ms/batch 372.2667 | train_loss 1.755920e-01 | lr 0.000000e+00
08/27/2024 01:36:16 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmpc8szzwt0 at global_step 30000 ****
08/27/2024 01:36:19 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/27/2024 01:36:22 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968514/tmpc8szzwt0
08/27/2024 01:36:29 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((1186239, 13330)) with avr_nnz=650.6143104382844
08/27/2024 01:36:29 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
08/27/2024 02:21:36 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(1186239, 204650)
08/27/2024 02:21:42 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [128, 1024, 13330]
08/27/2024 02:21:42 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
08/27/2024 02:21:44 - INFO - pecos.xmc.base - Training Layer 0 of 3 Layers in HierarchicalMLModel, neg_mining=tfn..
08/27/2024 02:23:44 - INFO - pecos.xmc.base - Training Layer 1 of 3 Layers in HierarchicalMLModel, neg_mining=tfn..
08/27/2024 02:24:57 - INFO - pecos.xmc.base - Training Layer 2 of 3 Layers in HierarchicalMLModel, neg_mining=tfn+man..
08/27/2024 02:48:45 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/amazoncat-13k/bert/param.json
08/27/2024 02:48:49 - INFO - pecos.xmc.xtransformer.model - Model saved to models/amazoncat-13k/bert
08/27/2024 03:03:18 - INFO - __main__ - Setting random seed 0
08/27/2024 03:03:20 - INFO - __main__ - Loaded training feature matrix with shape=(1186239, 203882)
08/27/2024 03:03:20 - INFO - __main__ - Loaded training label matrix with shape=(1186239, 13330)
08/27/2024 03:03:22 - INFO - __main__ - Loaded 1186239 training sequences
08/27/2024 03:03:26 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 1024, 13330]
08/27/2024 03:03:26 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 1024, 13330]
08/27/2024 03:03:26 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/27/2024 03:03:31 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
08/27/2024 03:03:31 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=1186239 truncation=256*****
08/27/2024 03:05:42 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=131.19023847579956 *****
08/27/2024 03:07:02 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23968514/tmp6lm02apt/X_trn.pt
08/27/2024 03:07:04 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/27/2024 03:07:05 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/27/2024 03:07:05 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/27/2024 03:07:05 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/27/2024 03:07:05 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 1186239
08/27/2024 03:07:05 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
08/27/2024 03:07:05 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
08/27/2024 03:07:05 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/27/2024 03:07:05 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/27/2024 03:07:05 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/27/2024 03:07:05 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 5000
08/27/2024 03:10:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   500/  5000] |  499/18535 batches | ms/batch 366.4797 | train_loss 2.753723e-01 | lr 2.500000e-05
08/27/2024 03:13:42 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1000/  5000] |  999/18535 batches | ms/batch 363.4960 | train_loss 5.542161e-02 | lr 5.000000e-05
08/27/2024 03:16:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1500/  5000] | 1499/18535 batches | ms/batch 363.6983 | train_loss 4.605262e-02 | lr 4.375000e-05
08/27/2024 03:20:07 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2000/  5000] | 1999/18535 batches | ms/batch 363.7017 | train_loss 4.120553e-02 | lr 3.750000e-05
08/27/2024 03:23:20 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2500/  5000] | 2499/18535 batches | ms/batch 363.1850 | train_loss 3.941423e-02 | lr 3.125000e-05
08/27/2024 03:26:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3000/  5000] | 2999/18535 batches | ms/batch 364.2719 | train_loss 3.769305e-02 | lr 2.500000e-05
08/27/2024 03:29:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3500/  5000] | 3499/18535 batches | ms/batch 364.6045 | train_loss 3.619906e-02 | lr 1.875000e-05
08/27/2024 03:32:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4000/  5000] | 3999/18535 batches | ms/batch 364.6295 | train_loss 3.456640e-02 | lr 1.250000e-05
08/27/2024 03:36:10 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4500/  5000] | 4499/18535 batches | ms/batch 366.2266 | train_loss 3.425918e-02 | lr 6.250000e-06
08/27/2024 03:39:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  5000/  5000] | 4999/18535 batches | ms/batch 364.3688 | train_loss 3.303792e-02 | lr 0.000000e+00
08/27/2024 03:39:23 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmp6jm18ezc at global_step 5000 ****
08/27/2024 03:39:25 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/27/2024 03:39:26 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968514/tmp6jm18ezc
08/27/2024 03:39:27 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([1186239, 256])) in OVA mode
08/27/2024 03:39:27 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
08/27/2024 04:21:58 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/27/2024 04:22:21 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/27/2024 04:26:01 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/27/2024 04:26:39 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=1024, avr_M_nnz=50.00252394332002
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/27/2024 04:26:45 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
08/27/2024 04:28:26 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23968514/tmp6lm02apt/X_trn.pt
08/27/2024 04:28:27 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/27/2024 04:52:21 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/27/2024 04:52:21 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/27/2024 04:52:25 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/27/2024 04:52:27 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/27/2024 04:53:17 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/27/2024 04:53:17 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 1186239
08/27/2024 04:53:17 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 1024
08/27/2024 04:53:17 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 432
08/27/2024 04:53:17 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
08/27/2024 04:53:17 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/27/2024 04:53:17 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/27/2024 04:53:17 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/27/2024 04:53:17 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 10000
08/27/2024 04:57:03 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   500/ 10000] |  499/18535 batches | ms/batch 387.3087 | train_loss 1.280985e-01 | lr 2.500000e-05
08/27/2024 05:00:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1000/ 10000] |  999/18535 batches | ms/batch 378.2803 | train_loss 1.020442e-01 | lr 5.000000e-05
08/27/2024 05:03:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1500/ 10000] | 1499/18535 batches | ms/batch 378.6370 | train_loss 9.995328e-02 | lr 4.722222e-05
08/27/2024 05:07:16 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2000/ 10000] | 1999/18535 batches | ms/batch 383.4449 | train_loss 9.775454e-02 | lr 4.444444e-05
08/27/2024 05:10:39 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2500/ 10000] | 2499/18535 batches | ms/batch 379.3732 | train_loss 9.655321e-02 | lr 4.166667e-05
08/27/2024 05:14:04 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3000/ 10000] | 2999/18535 batches | ms/batch 381.9234 | train_loss 9.503536e-02 | lr 3.888889e-05
08/27/2024 05:17:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3500/ 10000] | 3499/18535 batches | ms/batch 377.5920 | train_loss 9.405298e-02 | lr 3.611111e-05
08/27/2024 05:20:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4000/ 10000] | 3999/18535 batches | ms/batch 375.9624 | train_loss 9.355326e-02 | lr 3.333333e-05
08/27/2024 05:24:10 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4500/ 10000] | 4499/18535 batches | ms/batch 377.7867 | train_loss 9.283880e-02 | lr 3.055556e-05
08/27/2024 05:27:33 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  5000/ 10000] | 4999/18535 batches | ms/batch 378.0361 | train_loss 9.218585e-02 | lr 2.777778e-05
08/27/2024 05:27:33 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmp5p3mfpx3 at global_step 5000 ****
08/27/2024 05:27:35 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/27/2024 05:30:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  5500/ 10000] | 5499/18535 batches | ms/batch 368.1421 | train_loss 9.185523e-02 | lr 2.500000e-05
08/27/2024 05:34:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  6000/ 10000] | 5999/18535 batches | ms/batch 368.8016 | train_loss 9.149660e-02 | lr 2.222222e-05
08/27/2024 05:37:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  6500/ 10000] | 6499/18535 batches | ms/batch 367.3150 | train_loss 9.116129e-02 | lr 1.944444e-05
08/27/2024 05:40:43 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  7000/ 10000] | 6999/18535 batches | ms/batch 366.5634 | train_loss 9.070390e-02 | lr 1.666667e-05
08/27/2024 05:43:59 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  7500/ 10000] | 7499/18535 batches | ms/batch 366.7987 | train_loss 9.022297e-02 | lr 1.388889e-05
08/27/2024 05:47:15 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  8000/ 10000] | 7999/18535 batches | ms/batch 366.4804 | train_loss 9.005751e-02 | lr 1.111111e-05
08/27/2024 05:50:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  8500/ 10000] | 8499/18535 batches | ms/batch 367.7144 | train_loss 8.991679e-02 | lr 8.333333e-06
08/27/2024 05:53:49 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  9000/ 10000] | 8999/18535 batches | ms/batch 366.6605 | train_loss 8.974591e-02 | lr 5.555556e-06
08/27/2024 05:57:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  9500/ 10000] | 9499/18535 batches | ms/batch 366.6842 | train_loss 8.950356e-02 | lr 2.777778e-06
08/27/2024 06:00:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][ 10000/ 10000] | 9999/18535 batches | ms/batch 369.1103 | train_loss 8.951425e-02 | lr 0.000000e+00
08/27/2024 06:00:23 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmp5p3mfpx3 at global_step 10000 ****
08/27/2024 06:00:23 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/27/2024 06:00:26 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968514/tmp5p3mfpx3
08/27/2024 06:00:29 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((1186239, 1024)) with avr_nnz=400.0
08/27/2024 06:00:29 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
08/27/2024 06:44:45 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/27/2024 06:45:09 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/27/2024 06:54:24 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/27/2024 06:55:03 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=13330, avr_M_nnz=50.05081859557813
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/27/2024 06:55:16 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
08/27/2024 06:56:56 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23968514/tmp6lm02apt/X_trn.pt
08/27/2024 06:56:56 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/27/2024 07:16:36 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/27/2024 07:16:37 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/27/2024 07:16:39 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/27/2024 07:16:44 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/27/2024 07:17:47 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/27/2024 07:17:47 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 1186239
08/27/2024 07:17:47 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 13330
08/27/2024 07:17:47 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 806
08/27/2024 07:17:47 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
08/27/2024 07:17:47 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/27/2024 07:17:47 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/27/2024 07:17:47 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/27/2024 07:17:47 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 30000
08/27/2024 07:21:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   500/ 30000] |  499/18535 batches | ms/batch 380.0466 | train_loss 2.310994e-01 | lr 2.500000e-05
08/27/2024 07:24:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  1000/ 30000] |  999/18535 batches | ms/batch 369.3283 | train_loss 2.221369e-01 | lr 5.000000e-05
08/27/2024 07:28:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  1500/ 30000] | 1499/18535 batches | ms/batch 369.3746 | train_loss 2.204817e-01 | lr 4.913793e-05
08/27/2024 07:31:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  2000/ 30000] | 1999/18535 batches | ms/batch 370.2719 | train_loss 2.173995e-01 | lr 4.827586e-05
08/27/2024 07:34:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  2500/ 30000] | 2499/18535 batches | ms/batch 369.5868 | train_loss 2.156830e-01 | lr 4.741379e-05
08/27/2024 07:38:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  3000/ 30000] | 2999/18535 batches | ms/batch 369.2275 | train_loss 2.140926e-01 | lr 4.655172e-05
08/27/2024 07:41:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  3500/ 30000] | 3499/18535 batches | ms/batch 371.1318 | train_loss 2.130543e-01 | lr 4.568966e-05
08/27/2024 07:44:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  4000/ 30000] | 3999/18535 batches | ms/batch 369.4354 | train_loss 2.121378e-01 | lr 4.482759e-05
08/27/2024 07:48:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  4500/ 30000] | 4499/18535 batches | ms/batch 370.4296 | train_loss 2.115538e-01 | lr 4.396552e-05
08/27/2024 07:51:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  5000/ 30000] | 4999/18535 batches | ms/batch 369.1108 | train_loss 2.108018e-01 | lr 4.310345e-05
08/27/2024 07:54:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  5500/ 30000] | 5499/18535 batches | ms/batch 371.0397 | train_loss 2.102890e-01 | lr 4.224138e-05
08/27/2024 07:58:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  6000/ 30000] | 5999/18535 batches | ms/batch 369.4640 | train_loss 2.097214e-01 | lr 4.137931e-05
08/27/2024 08:01:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  6500/ 30000] | 6499/18535 batches | ms/batch 369.7038 | train_loss 2.093323e-01 | lr 4.051724e-05
08/27/2024 08:04:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  7000/ 30000] | 6999/18535 batches | ms/batch 369.8288 | train_loss 2.091004e-01 | lr 3.965517e-05
08/27/2024 08:08:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  7500/ 30000] | 7499/18535 batches | ms/batch 369.2515 | train_loss 2.089246e-01 | lr 3.879310e-05
08/27/2024 08:11:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  8000/ 30000] | 7999/18535 batches | ms/batch 369.2030 | train_loss 2.083631e-01 | lr 3.793103e-05
08/27/2024 08:14:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  8500/ 30000] | 8499/18535 batches | ms/batch 368.9671 | train_loss 2.083229e-01 | lr 3.706897e-05
08/27/2024 08:18:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  9000/ 30000] | 8999/18535 batches | ms/batch 368.9319 | train_loss 2.079242e-01 | lr 3.620690e-05
08/27/2024 08:21:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  9500/ 30000] | 9499/18535 batches | ms/batch 368.8679 | train_loss 2.076661e-01 | lr 3.534483e-05
08/27/2024 08:25:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 10000/ 30000] | 9999/18535 batches | ms/batch 387.4592 | train_loss 2.072826e-01 | lr 3.448276e-05
08/27/2024 08:25:05 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmpcci9inan at global_step 10000 ****
08/27/2024 08:25:09 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/27/2024 08:28:28 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 10500/ 30000] | 10499/18535 batches | ms/batch 369.6377 | train_loss 2.072101e-01 | lr 3.362069e-05
08/27/2024 08:31:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 11000/ 30000] | 10999/18535 batches | ms/batch 369.1755 | train_loss 2.077414e-01 | lr 3.275862e-05
08/27/2024 08:35:08 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 11500/ 30000] | 11499/18535 batches | ms/batch 369.1224 | train_loss 2.066836e-01 | lr 3.189655e-05
08/27/2024 08:38:28 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 12000/ 30000] | 11999/18535 batches | ms/batch 369.1910 | train_loss 2.066740e-01 | lr 3.103448e-05
08/27/2024 08:41:49 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 12500/ 30000] | 12499/18535 batches | ms/batch 370.9365 | train_loss 2.065976e-01 | lr 3.017241e-05
08/27/2024 08:45:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 13000/ 30000] | 12999/18535 batches | ms/batch 369.8479 | train_loss 2.063630e-01 | lr 2.931034e-05
08/27/2024 08:48:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 13500/ 30000] | 13499/18535 batches | ms/batch 368.7466 | train_loss 2.062341e-01 | lr 2.844828e-05
08/27/2024 08:51:49 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 14000/ 30000] | 13999/18535 batches | ms/batch 369.4544 | train_loss 2.058842e-01 | lr 2.758621e-05
08/27/2024 08:55:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 14500/ 30000] | 14499/18535 batches | ms/batch 369.1138 | train_loss 2.057699e-01 | lr 2.672414e-05
08/27/2024 08:58:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 15000/ 30000] | 14999/18535 batches | ms/batch 369.2432 | train_loss 2.056434e-01 | lr 2.586207e-05
08/27/2024 09:01:49 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 15500/ 30000] | 15499/18535 batches | ms/batch 369.2644 | train_loss 2.054446e-01 | lr 2.500000e-05
08/27/2024 09:05:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 16000/ 30000] | 15999/18535 batches | ms/batch 369.9480 | train_loss 2.052695e-01 | lr 2.413793e-05
08/27/2024 09:08:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 16500/ 30000] | 16499/18535 batches | ms/batch 369.3930 | train_loss 2.052816e-01 | lr 2.327586e-05
08/27/2024 09:11:49 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 17000/ 30000] | 16999/18535 batches | ms/batch 369.0860 | train_loss 2.051599e-01 | lr 2.241379e-05
08/27/2024 09:15:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 17500/ 30000] | 17499/18535 batches | ms/batch 369.3287 | train_loss 2.050607e-01 | lr 2.155172e-05
08/27/2024 09:18:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 18000/ 30000] | 17999/18535 batches | ms/batch 369.6049 | train_loss 2.049044e-01 | lr 2.068966e-05
08/27/2024 09:21:49 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 18500/ 30000] | 18499/18535 batches | ms/batch 369.1055 | train_loss 2.047783e-01 | lr 1.982759e-05
08/27/2024 09:25:30 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 19000/ 30000] |  464/18535 batches | ms/batch 369.4243 | train_loss 2.043055e-01 | lr 1.896552e-05
08/27/2024 09:28:51 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 19500/ 30000] |  964/18535 batches | ms/batch 371.9391 | train_loss 2.041986e-01 | lr 1.810345e-05
08/27/2024 09:32:21 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 20000/ 30000] | 1464/18535 batches | ms/batch 381.4477 | train_loss 2.040302e-01 | lr 1.724138e-05
08/27/2024 09:32:21 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmpcci9inan at global_step 20000 ****
08/27/2024 09:32:24 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/27/2024 09:35:44 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 20500/ 30000] | 1964/18535 batches | ms/batch 369.4068 | train_loss 2.040004e-01 | lr 1.637931e-05
08/27/2024 09:39:04 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 21000/ 30000] | 2464/18535 batches | ms/batch 368.9819 | train_loss 2.040869e-01 | lr 1.551724e-05
08/27/2024 09:42:24 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 21500/ 30000] | 2964/18535 batches | ms/batch 369.3685 | train_loss 2.039286e-01 | lr 1.465517e-05
08/27/2024 09:45:44 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 22000/ 30000] | 3464/18535 batches | ms/batch 368.9964 | train_loss 2.038688e-01 | lr 1.379310e-05
08/27/2024 09:49:04 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 22500/ 30000] | 3964/18535 batches | ms/batch 369.4946 | train_loss 2.037930e-01 | lr 1.293103e-05
08/27/2024 09:52:24 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 23000/ 30000] | 4464/18535 batches | ms/batch 369.4853 | train_loss 2.035425e-01 | lr 1.206897e-05
08/27/2024 09:55:44 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 23500/ 30000] | 4964/18535 batches | ms/batch 369.3421 | train_loss 2.037437e-01 | lr 1.120690e-05
08/27/2024 09:59:04 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 24000/ 30000] | 5464/18535 batches | ms/batch 368.9098 | train_loss 2.035541e-01 | lr 1.034483e-05
08/27/2024 10:02:24 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 24500/ 30000] | 5964/18535 batches | ms/batch 369.3118 | train_loss 2.033917e-01 | lr 9.482759e-06
08/27/2024 10:05:44 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 25000/ 30000] | 6464/18535 batches | ms/batch 369.2242 | train_loss 2.034535e-01 | lr 8.620690e-06
08/27/2024 10:09:04 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 25500/ 30000] | 6964/18535 batches | ms/batch 370.0692 | train_loss 2.032729e-01 | lr 7.758621e-06
08/27/2024 10:12:24 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 26000/ 30000] | 7464/18535 batches | ms/batch 370.1497 | train_loss 2.033041e-01 | lr 6.896552e-06
08/27/2024 10:15:44 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 26500/ 30000] | 7964/18535 batches | ms/batch 369.2283 | train_loss 2.033166e-01 | lr 6.034483e-06
08/27/2024 10:19:04 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 27000/ 30000] | 8464/18535 batches | ms/batch 369.0275 | train_loss 2.031967e-01 | lr 5.172414e-06
08/27/2024 10:22:24 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 27500/ 30000] | 8964/18535 batches | ms/batch 369.9272 | train_loss 2.031632e-01 | lr 4.310345e-06
08/27/2024 10:25:44 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 28000/ 30000] | 9464/18535 batches | ms/batch 370.9654 | train_loss 2.031362e-01 | lr 3.448276e-06
08/27/2024 10:29:04 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 28500/ 30000] | 9964/18535 batches | ms/batch 370.7892 | train_loss 2.030430e-01 | lr 2.586207e-06
08/27/2024 10:32:26 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 29000/ 30000] | 10464/18535 batches | ms/batch 372.4608 | train_loss 2.029814e-01 | lr 1.724138e-06
08/27/2024 10:35:52 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 29500/ 30000] | 10964/18535 batches | ms/batch 378.7418 | train_loss 2.029626e-01 | lr 8.620690e-07
08/27/2024 10:39:12 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 30000/ 30000] | 11464/18535 batches | ms/batch 368.8663 | train_loss 2.029645e-01 | lr 0.000000e+00
08/27/2024 10:39:12 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmpcci9inan at global_step 30000 ****
08/27/2024 10:39:15 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/27/2024 10:39:18 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968514/tmpcci9inan
08/27/2024 10:39:25 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((1186239, 13330)) with avr_nnz=650.7023298003185
08/27/2024 10:39:26 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
08/27/2024 11:24:15 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(1186239, 204650)
08/27/2024 11:24:22 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [128, 1024, 13330]
08/27/2024 11:24:22 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
08/27/2024 11:24:24 - INFO - pecos.xmc.base - Training Layer 0 of 3 Layers in HierarchicalMLModel, neg_mining=tfn..
08/27/2024 11:26:32 - INFO - pecos.xmc.base - Training Layer 1 of 3 Layers in HierarchicalMLModel, neg_mining=tfn..
08/27/2024 11:27:54 - INFO - pecos.xmc.base - Training Layer 2 of 3 Layers in HierarchicalMLModel, neg_mining=tfn+man..
08/27/2024 11:51:32 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/amazoncat-13k/roberta/param.json
08/27/2024 11:51:36 - INFO - pecos.xmc.xtransformer.model - Model saved to models/amazoncat-13k/roberta
08/27/2024 12:06:01 - INFO - __main__ - Setting random seed 0
08/27/2024 12:06:02 - INFO - __main__ - Loaded training feature matrix with shape=(1186239, 203882)
08/27/2024 12:06:02 - INFO - __main__ - Loaded training label matrix with shape=(1186239, 13330)
08/27/2024 12:06:05 - INFO - __main__ - Loaded 1186239 training sequences
08/27/2024 12:06:08 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 1024, 13330]
08/27/2024 12:06:08 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 1024, 13330]
08/27/2024 12:06:08 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
08/27/2024 12:06:11 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
08/27/2024 12:06:11 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=1186239 truncation=256*****
08/27/2024 12:08:43 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=152.5935115814209 *****
08/27/2024 12:10:05 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23968514/tmpnfkwqv4z/X_trn.pt
08/27/2024 12:10:08 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/27/2024 12:10:09 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/27/2024 12:10:09 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/27/2024 12:10:09 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/27/2024 12:10:09 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 1186239
08/27/2024 12:10:09 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
08/27/2024 12:10:09 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
08/27/2024 12:10:09 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/27/2024 12:10:09 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/27/2024 12:10:09 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/27/2024 12:10:09 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 5000
08/27/2024 12:15:17 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   500/  5000] |  499/18535 batches | ms/batch 568.4010 | train_loss 2.299479e-01 | lr 2.500000e-05
08/27/2024 12:20:11 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1000/  5000] |  999/18535 batches | ms/batch 563.5972 | train_loss 5.812078e-02 | lr 5.000000e-05
08/27/2024 12:25:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1500/  5000] | 1499/18535 batches | ms/batch 563.0959 | train_loss 4.818074e-02 | lr 4.375000e-05
08/27/2024 12:29:59 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2000/  5000] | 1999/18535 batches | ms/batch 563.3302 | train_loss 4.356235e-02 | lr 3.750000e-05
08/27/2024 12:34:53 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2500/  5000] | 2499/18535 batches | ms/batch 563.2371 | train_loss 4.047618e-02 | lr 3.125000e-05
08/27/2024 12:39:47 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3000/  5000] | 2999/18535 batches | ms/batch 563.3079 | train_loss 3.901479e-02 | lr 2.500000e-05
08/27/2024 12:44:41 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3500/  5000] | 3499/18535 batches | ms/batch 563.2632 | train_loss 3.726350e-02 | lr 1.875000e-05
08/27/2024 12:49:36 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4000/  5000] | 3999/18535 batches | ms/batch 564.9265 | train_loss 3.618411e-02 | lr 1.250000e-05
08/27/2024 12:54:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4500/  5000] | 4499/18535 batches | ms/batch 563.4350 | train_loss 3.518890e-02 | lr 6.250000e-06
08/27/2024 12:59:33 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  5000/  5000] | 4999/18535 batches | ms/batch 577.0108 | train_loss 3.402053e-02 | lr 0.000000e+00
08/27/2024 12:59:33 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmpb3eygm5_ at global_step 5000 ****
08/27/2024 12:59:34 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/27/2024 12:59:36 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968514/tmpb3eygm5_
08/27/2024 12:59:37 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([1186239, 256])) in OVA mode
08/27/2024 12:59:37 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
08/27/2024 14:07:14 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/27/2024 14:07:37 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/27/2024 14:11:10 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/27/2024 14:11:48 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=1024, avr_M_nnz=50.002421097266236
08/27/2024 14:11:52 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
08/27/2024 14:14:01 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23968514/tmpnfkwqv4z/X_trn.pt
08/27/2024 14:14:01 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/27/2024 14:35:48 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/27/2024 14:35:49 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/27/2024 14:35:52 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/27/2024 14:35:54 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/27/2024 14:36:47 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/27/2024 14:36:47 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 1186239
08/27/2024 14:36:47 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 1024
08/27/2024 14:36:47 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 432
08/27/2024 14:36:47 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
08/27/2024 14:36:47 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/27/2024 14:36:47 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/27/2024 14:36:47 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/27/2024 14:36:47 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 10000
08/27/2024 14:42:20 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   500/ 10000] |  499/18535 batches | ms/batch 577.4483 | train_loss 1.459629e-01 | lr 2.500000e-05
08/27/2024 14:47:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1000/ 10000] |  999/18535 batches | ms/batch 574.5541 | train_loss 1.093036e-01 | lr 5.000000e-05
08/27/2024 14:52:24 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  1500/ 10000] | 1499/18535 batches | ms/batch 580.8974 | train_loss 1.034383e-01 | lr 4.722222e-05
08/27/2024 14:57:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2000/ 10000] | 1999/18535 batches | ms/batch 575.6272 | train_loss 1.002614e-01 | lr 4.444444e-05
08/27/2024 15:02:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  2500/ 10000] | 2499/18535 batches | ms/batch 576.7495 | train_loss 9.846706e-02 | lr 4.166667e-05
08/27/2024 15:07:28 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3000/ 10000] | 2999/18535 batches | ms/batch 575.8819 | train_loss 9.764808e-02 | lr 3.888889e-05
08/27/2024 15:12:28 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  3500/ 10000] | 3499/18535 batches | ms/batch 571.5865 | train_loss 9.607619e-02 | lr 3.611111e-05
08/27/2024 15:17:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4000/ 10000] | 3999/18535 batches | ms/batch 571.8695 | train_loss 9.544316e-02 | lr 3.333333e-05
08/27/2024 15:22:35 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  4500/ 10000] | 4499/18535 batches | ms/batch 567.8505 | train_loss 9.484437e-02 | lr 3.055556e-05
08/27/2024 15:27:38 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  5000/ 10000] | 4999/18535 batches | ms/batch 566.1029 | train_loss 9.422290e-02 | lr 2.777778e-05
08/27/2024 15:27:38 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmp2xpaj4iy at global_step 5000 ****
08/27/2024 15:27:40 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/27/2024 15:32:43 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  5500/ 10000] | 5499/18535 batches | ms/batch 566.6112 | train_loss 9.387314e-02 | lr 2.500000e-05
08/27/2024 15:37:45 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  6000/ 10000] | 5999/18535 batches | ms/batch 577.6222 | train_loss 9.312354e-02 | lr 2.222222e-05
08/27/2024 15:42:51 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  6500/ 10000] | 6499/18535 batches | ms/batch 580.9161 | train_loss 9.299101e-02 | lr 1.944444e-05
08/27/2024 15:47:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  7000/ 10000] | 6999/18535 batches | ms/batch 569.6626 | train_loss 9.296041e-02 | lr 1.666667e-05
08/27/2024 15:52:58 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  7500/ 10000] | 7499/18535 batches | ms/batch 578.8811 | train_loss 9.224411e-02 | lr 1.388889e-05
08/27/2024 15:58:00 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  8000/ 10000] | 7999/18535 batches | ms/batch 571.1669 | train_loss 9.168702e-02 | lr 1.111111e-05
08/27/2024 16:03:03 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  8500/ 10000] | 8499/18535 batches | ms/batch 568.3700 | train_loss 9.169558e-02 | lr 8.333333e-06
08/27/2024 16:08:06 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  9000/ 10000] | 8999/18535 batches | ms/batch 567.7214 | train_loss 9.151106e-02 | lr 5.555556e-06
08/27/2024 16:13:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][  9500/ 10000] | 9499/18535 batches | ms/batch 564.6501 | train_loss 9.120012e-02 | lr 2.777778e-06
08/27/2024 16:18:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][ 10000/ 10000] | 9999/18535 batches | ms/batch 564.1967 | train_loss 9.111504e-02 | lr 0.000000e+00
08/27/2024 16:18:12 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmp2xpaj4iy at global_step 10000 ****
08/27/2024 16:18:14 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/27/2024 16:18:18 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968514/tmp2xpaj4iy
08/27/2024 16:18:22 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((1186239, 1024)) with avr_nnz=400.0
08/27/2024 16:18:22 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
08/27/2024 17:27:10 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/27/2024 17:27:35 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/27/2024 17:38:49 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/27/2024 17:39:45 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=13330, avr_M_nnz=50.04837052229778
08/27/2024 17:39:55 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
08/27/2024 17:41:37 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23968514/tmpnfkwqv4z/X_trn.pt
08/27/2024 17:41:38 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/27/2024 18:04:22 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/27/2024 18:04:22 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/27/2024 18:04:28 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/27/2024 18:04:32 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/27/2024 18:05:32 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/27/2024 18:05:32 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 1186239
08/27/2024 18:05:32 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 13330
08/27/2024 18:05:32 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 796
08/27/2024 18:05:32 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
08/27/2024 18:05:32 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/27/2024 18:05:32 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/27/2024 18:05:32 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/27/2024 18:05:32 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 30000
08/27/2024 18:11:16 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   500/ 30000] |  499/18535 batches | ms/batch 597.7169 | train_loss 2.406593e-01 | lr 2.500000e-05
08/27/2024 18:16:19 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  1000/ 30000] |  999/18535 batches | ms/batch 569.3829 | train_loss 2.235422e-01 | lr 5.000000e-05
08/27/2024 18:21:46 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  1500/ 30000] | 1499/18535 batches | ms/batch 603.4278 | train_loss 2.189759e-01 | lr 4.913793e-05
08/27/2024 18:26:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  2000/ 30000] | 1999/18535 batches | ms/batch 570.4027 | train_loss 2.147547e-01 | lr 4.827586e-05
08/27/2024 18:31:53 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  2500/ 30000] | 2499/18535 batches | ms/batch 568.7253 | train_loss 2.114670e-01 | lr 4.741379e-05
08/27/2024 18:36:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  3000/ 30000] | 2999/18535 batches | ms/batch 568.9922 | train_loss 2.097549e-01 | lr 4.655172e-05
08/27/2024 18:42:15 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  3500/ 30000] | 3499/18535 batches | ms/batch 593.2491 | train_loss 2.082893e-01 | lr 4.568966e-05
08/27/2024 18:47:40 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  4000/ 30000] | 3999/18535 batches | ms/batch 603.6446 | train_loss 2.072312e-01 | lr 4.482759e-05
08/27/2024 18:52:47 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  4500/ 30000] | 4499/18535 batches | ms/batch 575.2713 | train_loss 2.063819e-01 | lr 4.396552e-05
08/27/2024 18:57:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  5000/ 30000] | 4999/18535 batches | ms/batch 575.3693 | train_loss 2.053681e-01 | lr 4.310345e-05
08/27/2024 19:03:01 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  5500/ 30000] | 5499/18535 batches | ms/batch 572.3032 | train_loss 2.042839e-01 | lr 4.224138e-05
08/27/2024 19:08:06 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  6000/ 30000] | 5999/18535 batches | ms/batch 573.2166 | train_loss 2.038360e-01 | lr 4.137931e-05
08/27/2024 19:13:24 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  6500/ 30000] | 6499/18535 batches | ms/batch 591.5559 | train_loss 2.031385e-01 | lr 4.051724e-05
08/27/2024 19:18:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  7000/ 30000] | 6999/18535 batches | ms/batch 572.9284 | train_loss 2.023566e-01 | lr 3.965517e-05
08/27/2024 19:23:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  7500/ 30000] | 7499/18535 batches | ms/batch 605.3884 | train_loss 2.019310e-01 | lr 3.879310e-05
08/27/2024 19:29:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  8000/ 30000] | 7999/18535 batches | ms/batch 609.0551 | train_loss 2.013566e-01 | lr 3.793103e-05
08/27/2024 19:34:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  8500/ 30000] | 8499/18535 batches | ms/batch 569.8246 | train_loss 2.007870e-01 | lr 3.706897e-05
08/27/2024 19:39:31 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  9000/ 30000] | 8999/18535 batches | ms/batch 571.5051 | train_loss 2.003642e-01 | lr 3.620690e-05
08/27/2024 19:44:38 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][  9500/ 30000] | 9499/18535 batches | ms/batch 574.6520 | train_loss 2.004092e-01 | lr 3.534483e-05
08/27/2024 19:49:42 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 10000/ 30000] | 9999/18535 batches | ms/batch 569.4913 | train_loss 2.006577e-01 | lr 3.448276e-05
08/27/2024 19:49:42 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmpekbf7_cu at global_step 10000 ****
08/27/2024 19:49:45 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/27/2024 19:54:49 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 10500/ 30000] | 10499/18535 batches | ms/batch 569.2379 | train_loss 1.995797e-01 | lr 3.362069e-05
08/27/2024 19:59:54 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 11000/ 30000] | 10999/18535 batches | ms/batch 573.6214 | train_loss 1.991317e-01 | lr 3.275862e-05
08/27/2024 20:04:58 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 11500/ 30000] | 11499/18535 batches | ms/batch 569.7827 | train_loss 1.988096e-01 | lr 3.189655e-05
08/27/2024 20:10:02 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 12000/ 30000] | 11999/18535 batches | ms/batch 569.0169 | train_loss 1.982792e-01 | lr 3.103448e-05
08/27/2024 20:15:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 12500/ 30000] | 12499/18535 batches | ms/batch 610.0908 | train_loss 1.981930e-01 | lr 3.017241e-05
08/27/2024 20:20:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 13000/ 30000] | 12999/18535 batches | ms/batch 591.0450 | train_loss 1.979629e-01 | lr 2.931034e-05
08/27/2024 20:26:18 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 13500/ 30000] | 13499/18535 batches | ms/batch 607.6839 | train_loss 1.977569e-01 | lr 2.844828e-05
08/27/2024 20:31:40 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 14000/ 30000] | 13999/18535 batches | ms/batch 599.5929 | train_loss 1.976672e-01 | lr 2.758621e-05
08/27/2024 20:36:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 14500/ 30000] | 14499/18535 batches | ms/batch 578.7625 | train_loss 1.974956e-01 | lr 2.672414e-05
08/27/2024 20:41:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 15000/ 30000] | 14999/18535 batches | ms/batch 572.5848 | train_loss 1.972837e-01 | lr 2.586207e-05
08/27/2024 20:46:59 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 15500/ 30000] | 15499/18535 batches | ms/batch 570.8045 | train_loss 1.971086e-01 | lr 2.500000e-05
08/27/2024 20:52:04 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 16000/ 30000] | 15999/18535 batches | ms/batch 570.8628 | train_loss 1.968216e-01 | lr 2.413793e-05
08/27/2024 20:57:11 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 16500/ 30000] | 16499/18535 batches | ms/batch 575.2551 | train_loss 1.965063e-01 | lr 2.327586e-05
08/27/2024 21:02:17 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 17000/ 30000] | 16999/18535 batches | ms/batch 571.3614 | train_loss 1.964254e-01 | lr 2.241379e-05
08/27/2024 21:07:21 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 17500/ 30000] | 17499/18535 batches | ms/batch 569.9411 | train_loss 1.964172e-01 | lr 2.155172e-05
08/27/2024 21:12:24 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 18000/ 30000] | 17999/18535 batches | ms/batch 570.4365 | train_loss 1.962264e-01 | lr 2.068966e-05
08/27/2024 21:17:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][ 18500/ 30000] | 18499/18535 batches | ms/batch 572.3348 | train_loss 1.960092e-01 | lr 1.982759e-05
08/27/2024 21:23:00 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 19000/ 30000] |  464/18535 batches | ms/batch 570.1389 | train_loss 1.953350e-01 | lr 1.896552e-05
08/27/2024 21:28:04 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 19500/ 30000] |  964/18535 batches | ms/batch 570.5330 | train_loss 1.952736e-01 | lr 1.810345e-05
08/27/2024 21:33:07 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 20000/ 30000] | 1464/18535 batches | ms/batch 568.5774 | train_loss 1.952317e-01 | lr 1.724138e-05
08/27/2024 21:33:07 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmpekbf7_cu at global_step 20000 ****
08/27/2024 21:33:09 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/27/2024 21:38:18 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 20500/ 30000] | 1964/18535 batches | ms/batch 577.7263 | train_loss 1.950383e-01 | lr 1.637931e-05
08/27/2024 21:43:21 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 21000/ 30000] | 2464/18535 batches | ms/batch 568.6556 | train_loss 1.947730e-01 | lr 1.551724e-05
08/27/2024 21:48:24 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 21500/ 30000] | 2964/18535 batches | ms/batch 568.5196 | train_loss 1.947937e-01 | lr 1.465517e-05
08/27/2024 21:53:27 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 22000/ 30000] | 3464/18535 batches | ms/batch 568.7427 | train_loss 1.946614e-01 | lr 1.379310e-05
08/27/2024 21:58:30 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 22500/ 30000] | 3964/18535 batches | ms/batch 568.6940 | train_loss 1.945536e-01 | lr 1.293103e-05
08/27/2024 22:03:34 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 23000/ 30000] | 4464/18535 batches | ms/batch 569.3805 | train_loss 1.945011e-01 | lr 1.206897e-05
08/27/2024 22:08:37 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 23500/ 30000] | 4964/18535 batches | ms/batch 568.6632 | train_loss 1.943605e-01 | lr 1.120690e-05
08/27/2024 22:13:40 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 24000/ 30000] | 5464/18535 batches | ms/batch 568.5966 | train_loss 1.941572e-01 | lr 1.034483e-05
08/27/2024 22:18:43 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 24500/ 30000] | 5964/18535 batches | ms/batch 569.1095 | train_loss 1.940894e-01 | lr 9.482759e-06
08/27/2024 22:23:50 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 25000/ 30000] | 6464/18535 batches | ms/batch 571.7036 | train_loss 1.942334e-01 | lr 8.620690e-06
08/27/2024 22:28:53 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 25500/ 30000] | 6964/18535 batches | ms/batch 568.9269 | train_loss 1.938689e-01 | lr 7.758621e-06
08/27/2024 22:33:57 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 26000/ 30000] | 7464/18535 batches | ms/batch 568.9813 | train_loss 1.938893e-01 | lr 6.896552e-06
08/27/2024 22:39:00 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 26500/ 30000] | 7964/18535 batches | ms/batch 568.3588 | train_loss 1.937643e-01 | lr 6.034483e-06
08/27/2024 22:44:18 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 27000/ 30000] | 8464/18535 batches | ms/batch 587.6763 | train_loss 1.936853e-01 | lr 5.172414e-06
08/27/2024 22:49:21 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 27500/ 30000] | 8964/18535 batches | ms/batch 568.3443 | train_loss 1.934790e-01 | lr 4.310345e-06
08/27/2024 22:54:25 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 28000/ 30000] | 9464/18535 batches | ms/batch 568.7521 | train_loss 1.934217e-01 | lr 3.448276e-06
08/27/2024 22:59:28 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 28500/ 30000] | 9964/18535 batches | ms/batch 568.0884 | train_loss 1.934567e-01 | lr 2.586207e-06
08/27/2024 23:04:31 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 29000/ 30000] | 10464/18535 batches | ms/batch 568.4137 | train_loss 1.932824e-01 | lr 1.724138e-06
08/27/2024 23:09:34 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 29500/ 30000] | 10964/18535 batches | ms/batch 568.3016 | train_loss 1.931942e-01 | lr 8.620690e-07
08/27/2024 23:14:37 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][ 30000/ 30000] | 11464/18535 batches | ms/batch 568.2731 | train_loss 1.933862e-01 | lr 0.000000e+00
08/27/2024 23:14:37 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968514/tmpekbf7_cu at global_step 30000 ****
08/27/2024 23:14:39 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/27/2024 23:14:44 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968514/tmpekbf7_cu
08/27/2024 23:14:50 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((1186239, 13330)) with avr_nnz=651.5335585830511
08/27/2024 23:14:50 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=1186239
08/28/2024 00:24:59 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(1186239, 204650)
08/28/2024 00:25:06 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [32, 256, 13330]
08/28/2024 00:25:06 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
08/28/2024 00:25:07 - INFO - pecos.xmc.base - Training Layer 0 of 3 Layers in HierarchicalMLModel, neg_mining=tfn..
08/28/2024 00:25:56 - INFO - pecos.xmc.base - Training Layer 1 of 3 Layers in HierarchicalMLModel, neg_mining=tfn..
08/28/2024 00:26:37 - INFO - pecos.xmc.base - Training Layer 2 of 3 Layers in HierarchicalMLModel, neg_mining=tfn+man..
08/28/2024 01:47:03 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/amazoncat-13k/xlnet/param.json
08/28/2024 01:47:09 - INFO - pecos.xmc.xtransformer.model - Model saved to models/amazoncat-13k/xlnet
==== evaluation results ====
param: bert
prec   = 96.36 90.56 82.91 75.35 67.23 59.76 53.58 48.46 44.17 40.55 37.46 34.79 32.47 30.42 28.62 27.00 25.56 24.25 23.08 22.00
recall = 27.58 48.66 62.81 72.66 78.33 81.55 83.74 85.32 86.49 87.39 88.10 88.67 89.14 89.52 89.84 90.11 90.34 90.53 90.70 90.85
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 81.32
param: roberta
prec   = 96.18 90.43 82.92 75.44 67.37 59.96 53.82 48.72 44.47 40.86 37.79 35.12 32.80 30.76 28.95 27.34 25.89 24.59 23.40 22.33
recall = 27.51 48.57 62.83 72.76 78.52 81.86 84.14 85.80 87.07 88.05 88.84 89.48 90.01 90.44 90.82 91.13 91.40 91.63 91.84 92.01
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 81.45
param: xlnet
prec   = 96.10 90.29 82.80 75.31 67.27 59.87 53.73 48.64 44.39 40.79 37.72 35.06 32.75 30.71 28.91 27.30 25.86 24.56 23.38 22.30
recall = 27.48 48.48 62.73 72.65 78.42 81.75 84.04 85.70 86.97 87.95 88.73 89.37 89.90 90.35 90.73 91.05 91.32 91.56 91.76 91.94
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 81.28
==== softmax_average ensemble results ====
prec   = 96.54 90.89 83.38 75.87 67.76 60.27 54.05 48.88 44.56 40.90 37.78 35.08 32.74 30.68 28.86 27.23 25.77 24.45 23.26 22.17
recall = 27.63 48.83 63.15 73.13 78.88 82.15 84.35 85.91 87.06 87.94 88.64 89.21 89.67 90.06 90.39 90.67 90.91 91.11 91.29 91.46
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 82.02

============================= JOB FEEDBACK =============================

NodeName=uc2n909
Job ID: 23968514
Cluster: uc2
User/Group: ul_ruw26/ul_student
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 32
CPU Utilized: 8-16:24:42
CPU Efficiency: 20.03% of 43-08:23:28 core-walltime
Job Wall-clock time: 1-08:30:44
Memory Utilized: 234.22 GB
Memory Efficiency: 95.94% of 244.14 GB
