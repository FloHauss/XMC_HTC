------- Ensemble run at 2024-06-28 04:52:11 for wiki10-31k ----------
rm: cannot remove 'models/wiki10-31k': No such file or directory
06/28/2024 04:55:44 - INFO - __main__ - Setting random seed 0
06/28/2024 04:55:45 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
06/28/2024 04:55:45 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
06/28/2024 04:55:46 - INFO - __main__ - Loaded 14146 training sequences
06/28/2024 04:55:50 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
06/28/2024 04:55:50 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
06/28/2024 04:55:50 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
06/28/2024 04:55:56 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
06/28/2024 04:55:56 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
06/28/2024 04:56:04 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=7.964838266372681 *****
06/28/2024 04:56:13 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23813668/tmpy4h13r90/X_trn.pt
06/28/2024 04:56:14 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 04:56:30 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 04:56:30 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 04:56:31 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 04:56:31 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 04:56:31 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
06/28/2024 04:56:31 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 5
06/28/2024 04:56:31 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 04:56:31 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 04:56:31 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 04:56:31 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 1000
06/28/2024 04:56:59 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][    50/  1000] |   49/ 222 batches | ms/batch 493.7991 | train_loss 9.564973e-01 | lr 2.500000e-05
06/28/2024 04:57:18 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   100/  1000] |   99/ 222 batches | ms/batch 354.3665 | train_loss 4.522005e-01 | lr 5.000000e-05
06/28/2024 04:57:36 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   150/  1000] |  149/ 222 batches | ms/batch 355.2638 | train_loss 3.763055e-01 | lr 4.722222e-05
06/28/2024 04:57:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   200/  1000] |  199/ 222 batches | ms/batch 356.8289 | train_loss 3.235207e-01 | lr 4.444444e-05
06/28/2024 04:57:55 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmp22b4ax6j at global_step 200 ****
06/28/2024 04:57:55 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 04:58:16 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   250/  1000] |   27/ 222 batches | ms/batch 351.5659 | train_loss 2.896383e-01 | lr 4.166667e-05
06/28/2024 04:58:35 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   300/  1000] |   77/ 222 batches | ms/batch 358.0103 | train_loss 2.786670e-01 | lr 3.888889e-05
06/28/2024 04:58:54 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   350/  1000] |  127/ 222 batches | ms/batch 358.3539 | train_loss 2.654355e-01 | lr 3.611111e-05
06/28/2024 04:59:12 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   400/  1000] |  177/ 222 batches | ms/batch 358.9035 | train_loss 2.567890e-01 | lr 3.333333e-05
06/28/2024 04:59:12 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmp22b4ax6j at global_step 400 ****
06/28/2024 04:59:13 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 04:59:34 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   450/  1000] |    5/ 222 batches | ms/batch 353.8367 | train_loss 2.515313e-01 | lr 3.055556e-05
06/28/2024 04:59:52 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   500/  1000] |   55/ 222 batches | ms/batch 359.8993 | train_loss 2.429929e-01 | lr 2.777778e-05
06/28/2024 05:00:11 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   550/  1000] |  105/ 222 batches | ms/batch 359.8509 | train_loss 2.420875e-01 | lr 2.500000e-05
06/28/2024 05:00:30 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   600/  1000] |  155/ 222 batches | ms/batch 359.8705 | train_loss 2.410729e-01 | lr 2.222222e-05
06/28/2024 05:00:30 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmp22b4ax6j at global_step 600 ****
06/28/2024 05:00:30 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:00:49 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   650/  1000] |  205/ 222 batches | ms/batch 359.8969 | train_loss 2.358214e-01 | lr 1.944444e-05
06/28/2024 05:01:10 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   700/  1000] |   33/ 222 batches | ms/batch 354.4076 | train_loss 2.326700e-01 | lr 1.666667e-05
06/28/2024 05:01:29 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   750/  1000] |   83/ 222 batches | ms/batch 360.0816 | train_loss 2.295647e-01 | lr 1.388889e-05
06/28/2024 05:01:47 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   800/  1000] |  133/ 222 batches | ms/batch 360.7734 | train_loss 2.277523e-01 | lr 1.111111e-05
06/28/2024 05:01:47 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmp22b4ax6j at global_step 800 ****
06/28/2024 05:01:48 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:02:07 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   850/  1000] |  183/ 222 batches | ms/batch 360.1654 | train_loss 2.257637e-01 | lr 8.333333e-06
06/28/2024 05:02:27 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   900/  1000] |   11/ 222 batches | ms/batch 353.8464 | train_loss 2.270423e-01 | lr 5.555556e-06
06/28/2024 05:02:46 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   950/  1000] |   61/ 222 batches | ms/batch 360.1594 | train_loss 2.212483e-01 | lr 2.777778e-06
06/28/2024 05:03:05 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][  1000/  1000] |  111/ 222 batches | ms/batch 359.9703 | train_loss 2.219468e-01 | lr 0.000000e+00
06/28/2024 05:03:05 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmp22b4ax6j at global_step 1000 ****
06/28/2024 05:03:06 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:03:06 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813668/tmp22b4ax6j
06/28/2024 05:03:07 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
06/28/2024 05:03:07 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 05:03:38 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/28/2024 05:03:38 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/28/2024 05:03:45 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/28/2024 05:03:48 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.53492153258872
06/28/2024 05:03:49 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
06/28/2024 05:03:59 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813668/tmpy4h13r90/X_trn.pt
06/28/2024 05:03:59 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/28/2024 05:04:14 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/28/2024 05:04:14 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 05:04:14 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 05:04:14 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 05:04:15 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 05:04:15 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 05:04:15 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
06/28/2024 05:04:15 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 480
06/28/2024 05:04:15 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 5
06/28/2024 05:04:15 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 05:04:15 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 05:04:15 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 05:04:15 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 1000
06/28/2024 05:04:38 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][    50/  1000] |   49/ 222 batches | ms/batch 370.9781 | train_loss 5.501302e-01 | lr 2.500000e-05
06/28/2024 05:04:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   100/  1000] |   99/ 222 batches | ms/batch 365.2321 | train_loss 4.980931e-01 | lr 5.000000e-05
06/28/2024 05:05:16 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   150/  1000] |  149/ 222 batches | ms/batch 362.6995 | train_loss 4.593542e-01 | lr 4.722222e-05
06/28/2024 05:05:35 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   200/  1000] |  199/ 222 batches | ms/batch 362.4286 | train_loss 4.464829e-01 | lr 4.444444e-05
06/28/2024 05:05:35 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpc_xh9hux at global_step 200 ****
06/28/2024 05:05:35 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:05:57 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   250/  1000] |   27/ 222 batches | ms/batch 357.1917 | train_loss 4.411320e-01 | lr 4.166667e-05
06/28/2024 05:06:16 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   300/  1000] |   77/ 222 batches | ms/batch 362.6871 | train_loss 4.351354e-01 | lr 3.888889e-05
06/28/2024 05:06:35 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   350/  1000] |  127/ 222 batches | ms/batch 362.7406 | train_loss 4.323223e-01 | lr 3.611111e-05
06/28/2024 05:06:54 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   400/  1000] |  177/ 222 batches | ms/batch 363.1000 | train_loss 4.314666e-01 | lr 3.333333e-05
06/28/2024 05:06:54 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpc_xh9hux at global_step 400 ****
06/28/2024 05:06:55 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:07:16 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   450/  1000] |    5/ 222 batches | ms/batch 356.9945 | train_loss 4.292235e-01 | lr 3.055556e-05
06/28/2024 05:07:35 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   500/  1000] |   55/ 222 batches | ms/batch 362.3193 | train_loss 4.255509e-01 | lr 2.777778e-05
06/28/2024 05:07:54 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   550/  1000] |  105/ 222 batches | ms/batch 362.1913 | train_loss 4.244602e-01 | lr 2.500000e-05
06/28/2024 05:08:13 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   600/  1000] |  155/ 222 batches | ms/batch 362.8214 | train_loss 4.235129e-01 | lr 2.222222e-05
06/28/2024 05:08:13 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpc_xh9hux at global_step 600 ****
06/28/2024 05:08:14 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:08:33 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   650/  1000] |  205/ 222 batches | ms/batch 362.8930 | train_loss 4.221066e-01 | lr 1.944444e-05
06/28/2024 05:08:55 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   700/  1000] |   33/ 222 batches | ms/batch 356.7624 | train_loss 4.199548e-01 | lr 1.666667e-05
06/28/2024 05:09:14 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   750/  1000] |   83/ 222 batches | ms/batch 363.0701 | train_loss 4.182249e-01 | lr 1.388889e-05
06/28/2024 05:09:33 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   800/  1000] |  133/ 222 batches | ms/batch 362.7864 | train_loss 4.182703e-01 | lr 1.111111e-05
06/28/2024 05:09:33 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpc_xh9hux at global_step 800 ****
06/28/2024 05:09:33 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:09:52 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   850/  1000] |  183/ 222 batches | ms/batch 363.1310 | train_loss 4.175095e-01 | lr 8.333333e-06
06/28/2024 05:10:14 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   900/  1000] |   11/ 222 batches | ms/batch 357.4039 | train_loss 4.164981e-01 | lr 5.555556e-06
06/28/2024 05:10:33 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   950/  1000] |   61/ 222 batches | ms/batch 363.3511 | train_loss 4.155050e-01 | lr 2.777778e-06
06/28/2024 05:10:52 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][  1000/  1000] |  111/ 222 batches | ms/batch 367.1745 | train_loss 4.154747e-01 | lr 0.000000e+00
06/28/2024 05:10:52 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpc_xh9hux at global_step 1000 ****
06/28/2024 05:10:53 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:10:54 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813668/tmpc_xh9hux
06/28/2024 05:10:54 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
06/28/2024 05:10:54 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 05:11:27 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/28/2024 05:11:27 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/28/2024 05:11:40 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/28/2024 05:11:43 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=25.939488194542626
06/28/2024 05:11:45 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
06/28/2024 05:11:55 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813668/tmpy4h13r90/X_trn.pt
06/28/2024 05:11:55 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/28/2024 05:12:07 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/28/2024 05:12:07 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 05:12:07 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 05:12:07 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 05:12:08 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 05:12:08 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 05:12:08 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
06/28/2024 05:12:08 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 607
06/28/2024 05:12:08 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
06/28/2024 05:12:08 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 05:12:08 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 05:12:08 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 05:12:08 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
06/28/2024 05:12:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 362.6100 | train_loss 4.301003e-01 | lr 2.500000e-05
06/28/2024 05:12:49 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 361.9235 | train_loss 4.260902e-01 | lr 5.000000e-05
06/28/2024 05:13:08 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 361.8409 | train_loss 4.244524e-01 | lr 4.166667e-05
06/28/2024 05:13:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 362.6268 | train_loss 4.234818e-01 | lr 3.333333e-05
06/28/2024 05:13:27 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpp1np17z9 at global_step 200 ****
06/28/2024 05:13:28 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:13:50 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 358.0757 | train_loss 4.202434e-01 | lr 2.500000e-05
06/28/2024 05:14:09 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 363.8266 | train_loss 4.146254e-01 | lr 1.666667e-05
06/28/2024 05:14:28 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 363.5890 | train_loss 4.140655e-01 | lr 8.333333e-06
06/28/2024 05:14:47 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 363.7911 | train_loss 4.102792e-01 | lr 0.000000e+00
06/28/2024 05:14:47 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpp1np17z9 at global_step 400 ****
06/28/2024 05:14:48 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:14:49 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813668/tmpp1np17z9
06/28/2024 05:14:49 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.25222677788776
06/28/2024 05:14:49 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 05:15:22 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
06/28/2024 05:15:27 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
06/28/2024 05:15:27 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
06/28/2024 05:15:27 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 05:15:28 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 05:15:31 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 05:15:40 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
06/28/2024 05:16:42 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/wiki10-31k/bert/param.json
06/28/2024 05:16:46 - INFO - pecos.xmc.xtransformer.model - Model saved to models/wiki10-31k/bert
06/28/2024 05:19:55 - INFO - __main__ - Setting random seed 0
06/28/2024 05:19:56 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
06/28/2024 05:19:56 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
06/28/2024 05:19:56 - INFO - __main__ - Loaded 14146 training sequences
06/28/2024 05:20:00 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
06/28/2024 05:20:00 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
06/28/2024 05:20:00 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/28/2024 05:20:05 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
06/28/2024 05:20:05 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
06/28/2024 05:20:16 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=11.435423135757446 *****
06/28/2024 05:20:25 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23813668/tmpgiersj3h/X_trn.pt
06/28/2024 05:20:25 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 05:20:26 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 05:20:26 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 05:20:26 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 05:20:26 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 05:20:26 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
06/28/2024 05:20:26 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 3
06/28/2024 05:20:26 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 05:20:26 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 05:20:26 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 05:20:26 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 600
06/28/2024 05:20:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][    50/   600] |   49/ 222 batches | ms/batch 398.8005 | train_loss 8.438361e-01 | lr 2.500000e-05
06/28/2024 05:21:07 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   100/   600] |   99/ 222 batches | ms/batch 361.6622 | train_loss 4.244485e-01 | lr 5.000000e-05
06/28/2024 05:21:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   150/   600] |  149/ 222 batches | ms/batch 362.7370 | train_loss 3.231967e-01 | lr 4.500000e-05
06/28/2024 05:21:45 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   200/   600] |  199/ 222 batches | ms/batch 363.9343 | train_loss 2.821491e-01 | lr 4.000000e-05
06/28/2024 05:21:45 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpscv53gf6 at global_step 200 ****
06/28/2024 05:21:46 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:22:06 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   250/   600] |   27/ 222 batches | ms/batch 358.2618 | train_loss 2.643365e-01 | lr 3.500000e-05
06/28/2024 05:22:25 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   300/   600] |   77/ 222 batches | ms/batch 365.4604 | train_loss 2.501131e-01 | lr 3.000000e-05
06/28/2024 05:22:44 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   350/   600] |  127/ 222 batches | ms/batch 365.4022 | train_loss 2.476592e-01 | lr 2.500000e-05
06/28/2024 05:23:03 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   400/   600] |  177/ 222 batches | ms/batch 366.1299 | train_loss 2.401966e-01 | lr 2.000000e-05
06/28/2024 05:23:03 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpscv53gf6 at global_step 400 ****
06/28/2024 05:23:04 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:23:25 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   450/   600] |    5/ 222 batches | ms/batch 360.2442 | train_loss 2.416480e-01 | lr 1.500000e-05
06/28/2024 05:23:44 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   500/   600] |   55/ 222 batches | ms/batch 366.4874 | train_loss 2.291316e-01 | lr 1.000000e-05
06/28/2024 05:24:03 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   550/   600] |  105/ 222 batches | ms/batch 366.3340 | train_loss 2.287876e-01 | lr 5.000000e-06
06/28/2024 05:24:22 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   600/   600] |  155/ 222 batches | ms/batch 365.9886 | train_loss 2.300686e-01 | lr 0.000000e+00
06/28/2024 05:24:22 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpscv53gf6 at global_step 600 ****
06/28/2024 05:24:23 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:24:23 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813668/tmpscv53gf6
06/28/2024 05:24:24 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
06/28/2024 05:24:24 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 05:24:55 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/28/2024 05:24:55 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/28/2024 05:25:02 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/28/2024 05:25:04 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.42747066308497
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/28/2024 05:25:05 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
06/28/2024 05:25:16 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813668/tmpgiersj3h/X_trn.pt
06/28/2024 05:25:16 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/28/2024 05:25:30 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/28/2024 05:25:30 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 05:25:30 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 05:25:30 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 05:25:31 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 05:25:31 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 05:25:31 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
06/28/2024 05:25:31 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 448
06/28/2024 05:25:31 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
06/28/2024 05:25:31 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 05:25:31 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 05:25:31 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 05:25:31 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
06/28/2024 05:25:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 367.9337 | train_loss 5.004413e-01 | lr 2.500000e-05
06/28/2024 05:26:12 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 367.4554 | train_loss 4.297051e-01 | lr 5.000000e-05
06/28/2024 05:26:31 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 368.2891 | train_loss 4.215094e-01 | lr 4.166667e-05
06/28/2024 05:26:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 368.4223 | train_loss 4.149169e-01 | lr 3.333333e-05
06/28/2024 05:26:50 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmp87yl86xv at global_step 200 ****
06/28/2024 05:26:51 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:27:13 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 362.9392 | train_loss 4.080080e-01 | lr 2.500000e-05
06/28/2024 05:27:32 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 368.5558 | train_loss 4.001976e-01 | lr 1.666667e-05
06/28/2024 05:27:52 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 368.6868 | train_loss 3.990909e-01 | lr 8.333333e-06
06/28/2024 05:28:11 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 369.1481 | train_loss 3.960316e-01 | lr 0.000000e+00
06/28/2024 05:28:11 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmp87yl86xv at global_step 400 ****
06/28/2024 05:28:12 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:28:12 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813668/tmp87yl86xv
06/28/2024 05:28:13 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
06/28/2024 05:28:13 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 05:28:45 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/28/2024 05:28:46 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/28/2024 05:28:59 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/28/2024 05:29:01 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=26.07005513926198
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/28/2024 05:29:04 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
06/28/2024 05:29:15 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813668/tmpgiersj3h/X_trn.pt
06/28/2024 05:29:15 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/28/2024 05:29:27 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/28/2024 05:29:27 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 05:29:27 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 05:29:27 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 05:29:28 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 05:29:28 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 05:29:28 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
06/28/2024 05:29:28 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 621
06/28/2024 05:29:28 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
06/28/2024 05:29:28 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 05:29:28 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 05:29:28 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 05:29:28 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 200
06/28/2024 05:29:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   200] |   49/ 222 batches | ms/batch 371.7740 | train_loss 4.483331e-01 | lr 2.500000e-05
06/28/2024 05:30:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   200] |   99/ 222 batches | ms/batch 369.1478 | train_loss 4.428938e-01 | lr 5.000000e-05
06/28/2024 05:30:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   200] |  149/ 222 batches | ms/batch 369.7886 | train_loss 4.443955e-01 | lr 2.500000e-05
06/28/2024 05:30:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   200] |  199/ 222 batches | ms/batch 371.0907 | train_loss 4.404747e-01 | lr 0.000000e+00
06/28/2024 05:30:48 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpf0qk1hws at global_step 200 ****
06/28/2024 05:30:49 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:30:50 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813668/tmpf0qk1hws
06/28/2024 05:30:51 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.12370988265235
06/28/2024 05:30:51 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 05:31:24 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
06/28/2024 05:31:28 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
06/28/2024 05:31:28 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
06/28/2024 05:31:28 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 05:31:29 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 05:31:32 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 05:31:40 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
06/28/2024 05:32:41 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/wiki10-31k/roberta/param.json
06/28/2024 05:32:45 - INFO - pecos.xmc.xtransformer.model - Model saved to models/wiki10-31k/roberta
06/28/2024 05:33:53 - INFO - __main__ - Setting random seed 0
06/28/2024 05:33:53 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
06/28/2024 05:33:53 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
06/28/2024 05:33:54 - INFO - __main__ - Loaded 14146 training sequences
06/28/2024 05:33:57 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
06/28/2024 05:33:57 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
06/28/2024 05:33:57 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
06/28/2024 05:33:59 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
06/28/2024 05:33:59 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
06/28/2024 05:34:13 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=14.312870025634766 *****
06/28/2024 05:34:22 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23813668/tmpfp1x40hl/X_trn.pt
06/28/2024 05:34:23 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 05:34:24 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 05:34:24 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 05:34:24 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 05:34:24 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 05:34:24 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
06/28/2024 05:34:24 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
06/28/2024 05:34:24 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 05:34:24 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 05:34:24 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 05:34:24 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
06/28/2024 05:34:58 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 616.1957 | train_loss 7.426081e-01 | lr 2.500000e-05
06/28/2024 05:35:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 560.4426 | train_loss 4.033739e-01 | lr 5.000000e-05
06/28/2024 05:35:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 561.5183 | train_loss 3.116383e-01 | lr 4.166667e-05
06/28/2024 05:36:24 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 561.5011 | train_loss 2.876156e-01 | lr 3.333333e-05
06/28/2024 05:36:24 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpxrg4qtne at global_step 200 ****
06/28/2024 05:36:25 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:36:55 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 553.9817 | train_loss 2.694991e-01 | lr 2.500000e-05
06/28/2024 05:37:24 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 562.9279 | train_loss 2.656873e-01 | lr 1.666667e-05
06/28/2024 05:37:53 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 564.2729 | train_loss 2.594246e-01 | lr 8.333333e-06
06/28/2024 05:38:22 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 563.5441 | train_loss 2.556829e-01 | lr 0.000000e+00
06/28/2024 05:38:22 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpxrg4qtne at global_step 400 ****
06/28/2024 05:38:23 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:38:24 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813668/tmpxrg4qtne
06/28/2024 05:38:24 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
06/28/2024 05:38:24 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 05:39:13 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/28/2024 05:39:13 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/28/2024 05:39:19 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/28/2024 05:39:22 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.443234836702956
06/28/2024 05:39:24 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
06/28/2024 05:39:36 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813668/tmpfp1x40hl/X_trn.pt
06/28/2024 05:39:36 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/28/2024 05:39:48 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/28/2024 05:39:48 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 05:39:49 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 05:39:49 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 05:39:50 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 05:39:50 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 05:39:50 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
06/28/2024 05:39:50 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 448
06/28/2024 05:39:50 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
06/28/2024 05:39:50 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 05:39:50 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 05:39:50 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 05:39:50 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
06/28/2024 05:40:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 565.7442 | train_loss 6.265204e-01 | lr 2.500000e-05
06/28/2024 05:40:51 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 565.8393 | train_loss 5.100898e-01 | lr 5.000000e-05
06/28/2024 05:41:21 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 566.9535 | train_loss 4.734766e-01 | lr 4.166667e-05
06/28/2024 05:41:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 566.9281 | train_loss 4.557305e-01 | lr 3.333333e-05
06/28/2024 05:41:50 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpp1spenyo at global_step 200 ****
06/28/2024 05:41:51 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:42:23 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 566.7457 | train_loss 4.442823e-01 | lr 2.500000e-05
06/28/2024 05:42:52 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 567.4121 | train_loss 4.355974e-01 | lr 1.666667e-05
06/28/2024 05:43:22 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 566.7440 | train_loss 4.298155e-01 | lr 8.333333e-06
06/28/2024 05:43:51 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 566.8170 | train_loss 4.277860e-01 | lr 0.000000e+00
06/28/2024 05:43:51 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpp1spenyo at global_step 400 ****
06/28/2024 05:43:52 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:43:53 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813668/tmpp1spenyo
06/28/2024 05:43:53 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
06/28/2024 05:43:53 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 05:44:43 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/28/2024 05:44:44 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/28/2024 05:44:57 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/28/2024 05:45:00 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=26.09416089353881
06/28/2024 05:45:02 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
06/28/2024 05:45:14 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813668/tmpfp1x40hl/X_trn.pt
06/28/2024 05:45:14 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/28/2024 05:45:27 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/28/2024 05:45:27 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 05:45:27 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 05:45:27 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 05:45:29 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 05:45:29 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 05:45:29 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
06/28/2024 05:45:29 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 648
06/28/2024 05:45:29 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
06/28/2024 05:45:29 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 05:45:29 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 05:45:29 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 05:45:29 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 200
06/28/2024 05:46:01 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   200] |   49/ 222 batches | ms/batch 566.6007 | train_loss 5.175586e-01 | lr 2.500000e-05
06/28/2024 05:46:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   200] |   99/ 222 batches | ms/batch 566.5838 | train_loss 5.056437e-01 | lr 5.000000e-05
06/28/2024 05:46:59 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   200] |  149/ 222 batches | ms/batch 566.7700 | train_loss 5.025583e-01 | lr 2.500000e-05
06/28/2024 05:47:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   200] |  199/ 222 batches | ms/batch 567.0508 | train_loss 4.898114e-01 | lr 0.000000e+00
06/28/2024 05:47:29 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813668/tmpwkmwt5d0 at global_step 200 ****
06/28/2024 05:47:30 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 05:47:31 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813668/tmpwkmwt5d0
06/28/2024 05:47:31 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.09698854799944
06/28/2024 05:47:31 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 05:48:22 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
06/28/2024 05:48:26 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
06/28/2024 05:48:26 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
06/28/2024 05:48:26 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 05:48:27 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 05:48:30 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 05:48:38 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
06/28/2024 05:49:41 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/wiki10-31k/xlnet/param.json
06/28/2024 05:49:45 - INFO - pecos.xmc.xtransformer.model - Model saved to models/wiki10-31k/xlnet
==== evaluation results ====
param: bert
[  169  3177  7108  7398  8858  9026 15158 15286 15584 17273]
17
prec   = 87.94 84.42 79.80 74.54 69.64 65.31 61.50 58.08 54.97 52.32 49.79 47.55 45.51 43.67 41.96 40.38 38.95 37.65 36.44 35.34
recall = 5.24 10.00 14.04 17.27 20.03 22.41 24.49 26.29 27.89 29.38 30.67 31.85 32.95 33.98 34.91 35.79 36.60 37.38 38.14 38.86
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 37.59
param: roberta
[  169  3177  7108  7398  8858  9026 15158 15286 15584 17273]
17
prec   = 87.97 84.42 79.48 74.12 69.17 64.77 60.87 57.47 54.36 51.66 49.22 47.09 45.00 43.21 41.57 40.05 38.68 37.41 36.23 35.14
recall = 5.25 9.99 13.96 17.19 19.90 22.20 24.20 26.02 27.57 29.03 30.34 31.58 32.62 33.66 34.61 35.51 36.36 37.17 37.93 38.67
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 37.35
param: xlnet
[  169  3177  7108  7398  8858  9026 15158 15286 15584 17273]
17
prec   = 87.97 84.39 79.50 74.44 69.70 65.36 61.46 58.15 55.02 52.26 49.85 47.65 45.58 43.73 42.05 40.48 39.04 37.74 36.59 35.45
recall = 5.25 9.98 13.98 17.28 20.07 22.44 24.49 26.36 27.95 29.39 30.73 31.93 32.99 34.02 34.97 35.84 36.66 37.46 38.29 39.00
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 37.70
==== rank_average ensemble results ====
[  169  3177  7108  7398  8858  9026 15158 15286 15584 17273]
17
prec   = 88.12 84.87 80.39 75.30 70.74 66.54 62.91 59.64 56.61 53.82 51.41 49.17 47.09 45.17 43.47 41.90 40.43 39.14 37.89 36.76
recall = 5.26 10.05 14.14 17.47 20.35 22.84 25.05 26.98 28.70 30.20 31.63 32.89 34.03 35.09 36.12 37.03 37.92 38.80 39.57 40.34
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 38.93

============================= JOB FEEDBACK =============================

NodeName=uc2n904
Job ID: 23813668
Cluster: uc2
User/Group: ul_ruw26/ul_student
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 32
CPU Utilized: 05:15:34
CPU Efficiency: 16.06% of 1-08:45:20 core-walltime
Job Wall-clock time: 01:01:25
Memory Utilized: 24.56 GB
Memory Efficiency: 83.84% of 29.30 GB
