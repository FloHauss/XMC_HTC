------- Ensemble run at 2024-06-28 11:33:46 for wiki10-31k ----------
06/28/2024 11:36:26 - INFO - __main__ - Setting random seed 0
06/28/2024 11:36:26 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
06/28/2024 11:36:26 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
06/28/2024 11:36:27 - INFO - __main__ - Loaded 14146 training sequences
06/28/2024 11:36:31 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
06/28/2024 11:36:31 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
06/28/2024 11:36:31 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
06/28/2024 11:36:38 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
06/28/2024 11:36:38 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
06/28/2024 11:36:48 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=9.75190019607544 *****
06/28/2024 11:36:57 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23813669/tmppiqu3zt6/X_trn.pt
06/28/2024 11:36:58 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 11:37:18 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 11:37:18 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 11:37:18 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 11:37:18 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 11:37:18 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
06/28/2024 11:37:18 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 5
06/28/2024 11:37:18 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 11:37:18 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 11:37:18 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 11:37:18 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 1000
06/28/2024 11:37:48 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][    50/  1000] |   49/ 222 batches | ms/batch 523.5389 | train_loss 9.564973e-01 | lr 2.500000e-05
06/28/2024 11:38:07 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   100/  1000] |   99/ 222 batches | ms/batch 353.7505 | train_loss 4.522005e-01 | lr 5.000000e-05
06/28/2024 11:38:25 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   150/  1000] |  149/ 222 batches | ms/batch 354.8784 | train_loss 3.763055e-01 | lr 4.722222e-05
06/28/2024 11:38:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   200/  1000] |  199/ 222 batches | ms/batch 355.3086 | train_loss 3.235207e-01 | lr 4.444444e-05
06/28/2024 11:38:44 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpubyg9_5_ at global_step 200 ****
06/28/2024 11:38:45 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 11:39:06 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   250/  1000] |   27/ 222 batches | ms/batch 351.5779 | train_loss 2.896383e-01 | lr 4.166667e-05
06/28/2024 11:39:24 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   300/  1000] |   77/ 222 batches | ms/batch 356.9595 | train_loss 2.786670e-01 | lr 3.888889e-05
06/28/2024 11:39:43 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   350/  1000] |  127/ 222 batches | ms/batch 357.8928 | train_loss 2.654355e-01 | lr 3.611111e-05
06/28/2024 11:40:02 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   400/  1000] |  177/ 222 batches | ms/batch 358.6482 | train_loss 2.567890e-01 | lr 3.333333e-05
06/28/2024 11:40:02 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpubyg9_5_ at global_step 400 ****
06/28/2024 11:40:02 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 11:40:23 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   450/  1000] |    5/ 222 batches | ms/batch 353.4404 | train_loss 2.515313e-01 | lr 3.055556e-05
06/28/2024 11:40:42 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   500/  1000] |   55/ 222 batches | ms/batch 358.8110 | train_loss 2.429929e-01 | lr 2.777778e-05
06/28/2024 11:41:01 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   550/  1000] |  105/ 222 batches | ms/batch 360.3153 | train_loss 2.420875e-01 | lr 2.500000e-05
06/28/2024 11:41:20 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   600/  1000] |  155/ 222 batches | ms/batch 360.1130 | train_loss 2.410729e-01 | lr 2.222222e-05
06/28/2024 11:41:20 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpubyg9_5_ at global_step 600 ****
06/28/2024 11:41:20 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 11:41:39 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   650/  1000] |  205/ 222 batches | ms/batch 360.2099 | train_loss 2.358214e-01 | lr 1.944444e-05
06/28/2024 11:42:00 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   700/  1000] |   33/ 222 batches | ms/batch 354.6560 | train_loss 2.326700e-01 | lr 1.666667e-05
06/28/2024 11:42:19 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   750/  1000] |   83/ 222 batches | ms/batch 360.3680 | train_loss 2.295647e-01 | lr 1.388889e-05
06/28/2024 11:42:38 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   800/  1000] |  133/ 222 batches | ms/batch 361.4945 | train_loss 2.277523e-01 | lr 1.111111e-05
06/28/2024 11:42:38 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpubyg9_5_ at global_step 800 ****
06/28/2024 11:42:38 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 11:42:57 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   850/  1000] |  183/ 222 batches | ms/batch 361.4159 | train_loss 2.257637e-01 | lr 8.333333e-06
06/28/2024 11:43:18 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   900/  1000] |   11/ 222 batches | ms/batch 354.1632 | train_loss 2.270423e-01 | lr 5.555556e-06
06/28/2024 11:43:37 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   950/  1000] |   61/ 222 batches | ms/batch 360.3184 | train_loss 2.212483e-01 | lr 2.777778e-06
06/28/2024 11:43:56 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][  1000/  1000] |  111/ 222 batches | ms/batch 360.1277 | train_loss 2.219468e-01 | lr 0.000000e+00
06/28/2024 11:43:56 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpubyg9_5_ at global_step 1000 ****
06/28/2024 11:43:57 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 11:43:57 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813669/tmpubyg9_5_
06/28/2024 11:43:58 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
06/28/2024 11:43:58 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 11:44:29 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/28/2024 11:44:30 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/28/2024 11:44:36 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/28/2024 11:44:39 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.53492153258872
06/28/2024 11:44:40 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
06/28/2024 11:44:50 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813669/tmppiqu3zt6/X_trn.pt
06/28/2024 11:44:51 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/28/2024 11:45:06 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/28/2024 11:45:06 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 11:45:06 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 11:45:06 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 11:45:07 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 11:45:07 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 11:45:07 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
06/28/2024 11:45:07 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 480
06/28/2024 11:45:07 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 5
06/28/2024 11:45:07 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 11:45:07 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 11:45:07 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 11:45:07 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 1000
06/28/2024 11:45:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][    50/  1000] |   49/ 222 batches | ms/batch 373.1872 | train_loss 5.501302e-01 | lr 2.500000e-05
06/28/2024 11:45:49 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   100/  1000] |   99/ 222 batches | ms/batch 360.2952 | train_loss 4.980931e-01 | lr 5.000000e-05
06/28/2024 11:46:08 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   150/  1000] |  149/ 222 batches | ms/batch 360.9855 | train_loss 4.593542e-01 | lr 4.722222e-05
06/28/2024 11:46:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   200/  1000] |  199/ 222 batches | ms/batch 362.4450 | train_loss 4.464829e-01 | lr 4.444444e-05
06/28/2024 11:46:27 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmplovdymw2 at global_step 200 ****
06/28/2024 11:46:27 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 11:46:49 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   250/  1000] |   27/ 222 batches | ms/batch 357.1822 | train_loss 4.411320e-01 | lr 4.166667e-05
06/28/2024 11:47:09 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   300/  1000] |   77/ 222 batches | ms/batch 366.9340 | train_loss 4.351354e-01 | lr 3.888889e-05
06/28/2024 11:47:28 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   350/  1000] |  127/ 222 batches | ms/batch 365.9004 | train_loss 4.323223e-01 | lr 3.611111e-05
06/28/2024 11:47:47 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   400/  1000] |  177/ 222 batches | ms/batch 362.4290 | train_loss 4.314666e-01 | lr 3.333333e-05
06/28/2024 11:47:47 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmplovdymw2 at global_step 400 ****
06/28/2024 11:47:47 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 11:48:10 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   450/  1000] |    5/ 222 batches | ms/batch 355.8063 | train_loss 4.292235e-01 | lr 3.055556e-05
06/28/2024 11:48:29 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   500/  1000] |   55/ 222 batches | ms/batch 361.6613 | train_loss 4.255509e-01 | lr 2.777778e-05
06/28/2024 11:48:48 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   550/  1000] |  105/ 222 batches | ms/batch 362.2151 | train_loss 4.244602e-01 | lr 2.500000e-05
06/28/2024 11:49:07 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   600/  1000] |  155/ 222 batches | ms/batch 363.0411 | train_loss 4.235129e-01 | lr 2.222222e-05
06/28/2024 11:49:07 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmplovdymw2 at global_step 600 ****
06/28/2024 11:49:07 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 11:49:26 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   650/  1000] |  205/ 222 batches | ms/batch 362.8617 | train_loss 4.221066e-01 | lr 1.944444e-05
06/28/2024 11:49:48 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   700/  1000] |   33/ 222 batches | ms/batch 357.8697 | train_loss 4.199548e-01 | lr 1.666667e-05
06/28/2024 11:50:07 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   750/  1000] |   83/ 222 batches | ms/batch 364.2482 | train_loss 4.182249e-01 | lr 1.388889e-05
06/28/2024 11:50:27 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   800/  1000] |  133/ 222 batches | ms/batch 363.6004 | train_loss 4.182703e-01 | lr 1.111111e-05
06/28/2024 11:50:27 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmplovdymw2 at global_step 800 ****
06/28/2024 11:50:27 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 11:50:46 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   850/  1000] |  183/ 222 batches | ms/batch 362.8761 | train_loss 4.175095e-01 | lr 8.333333e-06
06/28/2024 11:51:08 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   900/  1000] |   11/ 222 batches | ms/batch 356.8992 | train_loss 4.164981e-01 | lr 5.555556e-06
06/28/2024 11:51:27 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   950/  1000] |   61/ 222 batches | ms/batch 363.3023 | train_loss 4.155050e-01 | lr 2.777778e-06
06/28/2024 11:51:47 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][  1000/  1000] |  111/ 222 batches | ms/batch 363.3587 | train_loss 4.154747e-01 | lr 0.000000e+00
06/28/2024 11:51:47 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmplovdymw2 at global_step 1000 ****
06/28/2024 11:51:47 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 11:51:48 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813669/tmplovdymw2
06/28/2024 11:51:48 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
06/28/2024 11:51:48 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 11:52:22 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/28/2024 11:52:22 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/28/2024 11:52:36 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/28/2024 11:52:38 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=25.939488194542626
06/28/2024 11:52:40 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
06/28/2024 11:52:50 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813669/tmppiqu3zt6/X_trn.pt
06/28/2024 11:52:50 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/28/2024 11:53:02 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/28/2024 11:53:02 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 11:53:02 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 11:53:02 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 11:53:03 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 11:53:03 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 11:53:03 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
06/28/2024 11:53:03 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 607
06/28/2024 11:53:03 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
06/28/2024 11:53:03 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 11:53:03 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 11:53:03 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 11:53:03 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
06/28/2024 11:53:25 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 362.1440 | train_loss 4.301003e-01 | lr 2.500000e-05
06/28/2024 11:53:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 362.9917 | train_loss 4.260902e-01 | lr 5.000000e-05
06/28/2024 11:54:04 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 364.2548 | train_loss 4.244524e-01 | lr 4.166667e-05
06/28/2024 11:54:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 364.2146 | train_loss 4.234818e-01 | lr 3.333333e-05
06/28/2024 11:54:23 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmplhoehf98 at global_step 200 ****
06/28/2024 11:54:23 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 11:54:45 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 358.8310 | train_loss 4.202434e-01 | lr 2.500000e-05
06/28/2024 11:55:05 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 364.5289 | train_loss 4.146254e-01 | lr 1.666667e-05
06/28/2024 11:55:24 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 364.4819 | train_loss 4.140655e-01 | lr 8.333333e-06
06/28/2024 11:55:43 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 364.3010 | train_loss 4.102792e-01 | lr 0.000000e+00
06/28/2024 11:55:43 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmplhoehf98 at global_step 400 ****
06/28/2024 11:55:44 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 11:55:45 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813669/tmplhoehf98
06/28/2024 11:55:45 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.25222677788776
06/28/2024 11:55:45 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 11:56:19 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
06/28/2024 11:56:23 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
06/28/2024 11:56:23 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
06/28/2024 11:56:23 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 11:56:24 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 11:56:28 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 11:56:36 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
06/28/2024 11:57:38 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/wiki10-31k/bert/param.json
06/28/2024 11:57:42 - INFO - pecos.xmc.xtransformer.model - Model saved to models/wiki10-31k/bert
06/28/2024 12:00:32 - INFO - __main__ - Setting random seed 0
06/28/2024 12:00:32 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
06/28/2024 12:00:32 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
06/28/2024 12:00:32 - INFO - __main__ - Loaded 14146 training sequences
06/28/2024 12:00:36 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
06/28/2024 12:00:36 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
06/28/2024 12:00:36 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/28/2024 12:00:40 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
06/28/2024 12:00:40 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
06/28/2024 12:00:50 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=10.012670278549194 *****
06/28/2024 12:00:58 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23813669/tmprrsx69ku/X_trn.pt
06/28/2024 12:00:59 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 12:01:00 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 12:01:00 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 12:01:00 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 12:01:00 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 12:01:00 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
06/28/2024 12:01:00 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 3
06/28/2024 12:01:00 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 12:01:00 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 12:01:00 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 12:01:00 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 600
06/28/2024 12:01:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][    50/   600] |   49/ 222 batches | ms/batch 395.1613 | train_loss 8.438361e-01 | lr 2.500000e-05
06/28/2024 12:01:41 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   100/   600] |   99/ 222 batches | ms/batch 360.2435 | train_loss 4.244485e-01 | lr 5.000000e-05
06/28/2024 12:02:00 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   150/   600] |  149/ 222 batches | ms/batch 360.7755 | train_loss 3.231967e-01 | lr 4.500000e-05
06/28/2024 12:02:18 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   200/   600] |  199/ 222 batches | ms/batch 360.9391 | train_loss 2.821491e-01 | lr 4.000000e-05
06/28/2024 12:02:18 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpp4ufosrf at global_step 200 ****
06/28/2024 12:02:19 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 12:02:40 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   250/   600] |   27/ 222 batches | ms/batch 356.7629 | train_loss 2.643365e-01 | lr 3.500000e-05
06/28/2024 12:02:59 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   300/   600] |   77/ 222 batches | ms/batch 363.0149 | train_loss 2.501131e-01 | lr 3.000000e-05
06/28/2024 12:03:18 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   350/   600] |  127/ 222 batches | ms/batch 363.8271 | train_loss 2.476592e-01 | lr 2.500000e-05
06/28/2024 12:03:37 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   400/   600] |  177/ 222 batches | ms/batch 365.1811 | train_loss 2.401966e-01 | lr 2.000000e-05
06/28/2024 12:03:37 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpp4ufosrf at global_step 400 ****
06/28/2024 12:03:38 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 12:03:59 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   450/   600] |    5/ 222 batches | ms/batch 360.0410 | train_loss 2.416480e-01 | lr 1.500000e-05
06/28/2024 12:04:18 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   500/   600] |   55/ 222 batches | ms/batch 365.8075 | train_loss 2.291316e-01 | lr 1.000000e-05
06/28/2024 12:04:37 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   550/   600] |  105/ 222 batches | ms/batch 368.1029 | train_loss 2.287876e-01 | lr 5.000000e-06
06/28/2024 12:04:56 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   600/   600] |  155/ 222 batches | ms/batch 367.1286 | train_loss 2.300686e-01 | lr 0.000000e+00
06/28/2024 12:04:56 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpp4ufosrf at global_step 600 ****
06/28/2024 12:04:57 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 12:04:58 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813669/tmpp4ufosrf
06/28/2024 12:04:58 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
06/28/2024 12:04:58 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 12:05:30 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/28/2024 12:05:30 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/28/2024 12:05:36 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/28/2024 12:05:39 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.42747066308497
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/28/2024 12:05:40 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
06/28/2024 12:05:51 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813669/tmprrsx69ku/X_trn.pt
06/28/2024 12:05:51 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/28/2024 12:06:05 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/28/2024 12:06:05 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 12:06:05 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 12:06:05 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 12:06:06 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 12:06:06 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 12:06:06 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
06/28/2024 12:06:06 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 448
06/28/2024 12:06:06 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
06/28/2024 12:06:06 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 12:06:06 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 12:06:06 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 12:06:06 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
06/28/2024 12:06:28 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 366.8581 | train_loss 5.004413e-01 | lr 2.500000e-05
06/28/2024 12:06:47 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 367.7449 | train_loss 4.297051e-01 | lr 5.000000e-05
06/28/2024 12:07:06 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 368.6741 | train_loss 4.215094e-01 | lr 4.166667e-05
06/28/2024 12:07:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 369.4621 | train_loss 4.149169e-01 | lr 3.333333e-05
06/28/2024 12:07:26 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpkus5p_4i at global_step 200 ****
06/28/2024 12:07:26 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 12:07:48 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 363.6875 | train_loss 4.080080e-01 | lr 2.500000e-05
06/28/2024 12:08:08 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 369.3749 | train_loss 4.001976e-01 | lr 1.666667e-05
06/28/2024 12:08:27 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 368.3222 | train_loss 3.990909e-01 | lr 8.333333e-06
06/28/2024 12:08:46 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 368.8458 | train_loss 3.960316e-01 | lr 0.000000e+00
06/28/2024 12:08:46 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpkus5p_4i at global_step 400 ****
06/28/2024 12:08:47 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 12:08:48 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813669/tmpkus5p_4i
06/28/2024 12:08:48 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
06/28/2024 12:08:48 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 12:09:21 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/28/2024 12:09:21 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/28/2024 12:09:35 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/28/2024 12:09:37 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=26.07005513926198
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/28/2024 12:09:39 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
06/28/2024 12:09:50 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813669/tmprrsx69ku/X_trn.pt
06/28/2024 12:09:50 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/28/2024 12:10:02 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/28/2024 12:10:02 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 12:10:02 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 12:10:02 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 12:10:03 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 12:10:03 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 12:10:03 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
06/28/2024 12:10:03 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 621
06/28/2024 12:10:03 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
06/28/2024 12:10:03 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 12:10:03 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 12:10:03 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 12:10:03 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 200
06/28/2024 12:10:25 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   200] |   49/ 222 batches | ms/batch 370.7878 | train_loss 4.483331e-01 | lr 2.500000e-05
06/28/2024 12:10:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   200] |   99/ 222 batches | ms/batch 367.2361 | train_loss 4.428938e-01 | lr 5.000000e-05
06/28/2024 12:11:04 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   200] |  149/ 222 batches | ms/batch 368.8006 | train_loss 4.443955e-01 | lr 2.500000e-05
06/28/2024 12:11:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   200] |  199/ 222 batches | ms/batch 369.7850 | train_loss 4.404747e-01 | lr 0.000000e+00
06/28/2024 12:11:23 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmp6nfq8tkt at global_step 200 ****
06/28/2024 12:11:24 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 12:11:25 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813669/tmp6nfq8tkt
06/28/2024 12:11:25 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.12370988265235
06/28/2024 12:11:25 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 12:11:59 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
06/28/2024 12:12:03 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
06/28/2024 12:12:03 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
06/28/2024 12:12:03 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 12:12:04 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 12:12:07 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 12:12:16 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
06/28/2024 12:13:17 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/wiki10-31k/roberta/param.json
06/28/2024 12:13:21 - INFO - pecos.xmc.xtransformer.model - Model saved to models/wiki10-31k/roberta
06/28/2024 12:14:24 - INFO - __main__ - Setting random seed 0
06/28/2024 12:14:24 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
06/28/2024 12:14:24 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
06/28/2024 12:14:25 - INFO - __main__ - Loaded 14146 training sequences
06/28/2024 12:14:28 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
06/28/2024 12:14:28 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
06/28/2024 12:14:28 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
06/28/2024 12:14:30 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
06/28/2024 12:14:30 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
06/28/2024 12:14:48 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=17.742000818252563 *****
06/28/2024 12:14:58 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23813669/tmpniq1l5mg/X_trn.pt
06/28/2024 12:14:59 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 12:14:59 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 12:14:59 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 12:14:59 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 12:14:59 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 12:14:59 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
06/28/2024 12:14:59 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
06/28/2024 12:14:59 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 12:14:59 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 12:14:59 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 12:14:59 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
06/28/2024 12:15:33 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 616.2128 | train_loss 7.426081e-01 | lr 2.500000e-05
06/28/2024 12:16:02 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 559.3958 | train_loss 4.033739e-01 | lr 5.000000e-05
06/28/2024 12:16:31 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 561.3159 | train_loss 3.116383e-01 | lr 4.166667e-05
06/28/2024 12:17:00 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 564.4263 | train_loss 2.876156e-01 | lr 3.333333e-05
06/28/2024 12:17:00 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpc6y5shvs at global_step 200 ****
06/28/2024 12:17:00 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 12:17:32 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 557.2185 | train_loss 2.694991e-01 | lr 2.500000e-05
06/28/2024 12:18:01 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 566.8988 | train_loss 2.656873e-01 | lr 1.666667e-05
06/28/2024 12:18:30 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 565.9462 | train_loss 2.594246e-01 | lr 8.333333e-06
06/28/2024 12:18:59 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 567.1349 | train_loss 2.556829e-01 | lr 0.000000e+00
06/28/2024 12:18:59 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpc6y5shvs at global_step 400 ****
06/28/2024 12:19:00 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 12:19:01 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813669/tmpc6y5shvs
06/28/2024 12:19:01 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
06/28/2024 12:19:01 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 12:19:49 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/28/2024 12:19:50 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/28/2024 12:19:56 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/28/2024 12:19:59 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.443234836702956
06/28/2024 12:20:00 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
06/28/2024 12:20:12 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813669/tmpniq1l5mg/X_trn.pt
06/28/2024 12:20:12 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/28/2024 12:20:24 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/28/2024 12:20:24 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 12:20:25 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 12:20:25 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 12:20:25 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 12:20:25 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 12:20:25 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
06/28/2024 12:20:25 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 448
06/28/2024 12:20:25 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
06/28/2024 12:20:25 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 12:20:25 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 12:20:25 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 12:20:25 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
06/28/2024 12:20:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 564.1377 | train_loss 6.265204e-01 | lr 2.500000e-05
06/28/2024 12:21:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 567.5842 | train_loss 5.100898e-01 | lr 5.000000e-05
06/28/2024 12:21:56 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 574.3075 | train_loss 4.734766e-01 | lr 4.166667e-05
06/28/2024 12:22:25 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 566.6284 | train_loss 4.557305e-01 | lr 3.333333e-05
06/28/2024 12:22:25 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpv3f19ahw at global_step 200 ****
06/28/2024 12:22:26 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 12:22:58 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 556.8030 | train_loss 4.442823e-01 | lr 2.500000e-05
06/28/2024 12:23:27 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 565.6488 | train_loss 4.355974e-01 | lr 1.666667e-05
06/28/2024 12:23:57 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 574.0411 | train_loss 4.298155e-01 | lr 8.333333e-06
06/28/2024 12:24:26 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 567.4352 | train_loss 4.277860e-01 | lr 0.000000e+00
06/28/2024 12:24:26 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpv3f19ahw at global_step 400 ****
06/28/2024 12:24:26 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 12:24:27 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813669/tmpv3f19ahw
06/28/2024 12:24:28 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
06/28/2024 12:24:28 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 12:25:18 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/28/2024 12:25:18 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/28/2024 12:25:32 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/28/2024 12:25:34 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=26.09416089353881
06/28/2024 12:25:36 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
06/28/2024 12:25:48 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813669/tmpniq1l5mg/X_trn.pt
06/28/2024 12:25:48 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/28/2024 12:26:01 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/28/2024 12:26:01 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/28/2024 12:26:01 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/28/2024 12:26:01 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/28/2024 12:26:02 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/28/2024 12:26:02 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/28/2024 12:26:02 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
06/28/2024 12:26:02 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 648
06/28/2024 12:26:02 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
06/28/2024 12:26:02 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/28/2024 12:26:02 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/28/2024 12:26:02 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/28/2024 12:26:02 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 200
06/28/2024 12:26:34 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   200] |   49/ 222 batches | ms/batch 565.9844 | train_loss 5.175586e-01 | lr 2.500000e-05
06/28/2024 12:27:04 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   200] |   99/ 222 batches | ms/batch 566.3153 | train_loss 5.056437e-01 | lr 5.000000e-05
06/28/2024 12:27:33 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   200] |  149/ 222 batches | ms/batch 566.7358 | train_loss 5.025583e-01 | lr 2.500000e-05
06/28/2024 12:28:02 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   200] |  199/ 222 batches | ms/batch 567.1678 | train_loss 4.898114e-01 | lr 0.000000e+00
06/28/2024 12:28:02 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813669/tmpd2luv393 at global_step 200 ****
06/28/2024 12:28:03 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/28/2024 12:28:04 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813669/tmpd2luv393
06/28/2024 12:28:05 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.09698854799944
06/28/2024 12:28:05 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/28/2024 12:28:55 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
06/28/2024 12:28:59 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
06/28/2024 12:28:59 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
06/28/2024 12:28:59 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 12:29:00 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 12:29:03 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/28/2024 12:29:12 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
06/28/2024 12:30:15 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/wiki10-31k/xlnet/param.json
06/28/2024 12:30:18 - INFO - pecos.xmc.xtransformer.model - Model saved to models/wiki10-31k/xlnet
==== evaluation results ====
param: bert
[  169  3177  7108  7398  8858  9026 15158 15286 15584 17273]
17
prec   = 87.94 84.42 79.80 74.54 69.64 65.31 61.50 58.08 54.97 52.32 49.79 47.55 45.51 43.67 41.96 40.38 38.95 37.65 36.44 35.34
recall = 5.24 10.00 14.04 17.27 20.03 22.41 24.49 26.29 27.89 29.38 30.67 31.85 32.95 33.98 34.91 35.79 36.60 37.38 38.14 38.86
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 37.59
param: roberta
[  169  3177  7108  7398  8858  9026 15158 15286 15584 17273]
17
prec   = 87.97 84.42 79.48 74.12 69.17 64.77 60.87 57.47 54.36 51.66 49.22 47.09 45.00 43.21 41.57 40.05 38.68 37.41 36.23 35.14
recall = 5.25 9.99 13.96 17.19 19.90 22.20 24.20 26.02 27.57 29.03 30.34 31.58 32.62 33.66 34.61 35.51 36.36 37.17 37.93 38.67
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 37.35
param: xlnet
[  169  3177  7108  7398  8858  9026 15158 15286 15584 17273]
17
prec   = 87.97 84.39 79.50 74.44 69.70 65.36 61.46 58.15 55.02 52.26 49.85 47.65 45.58 43.73 42.05 40.48 39.04 37.74 36.59 35.45
recall = 5.25 9.98 13.98 17.28 20.07 22.44 24.49 26.36 27.95 29.39 30.73 31.93 32.99 34.02 34.97 35.84 36.66 37.46 38.29 39.00
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 37.70
==== rank_average ensemble results ====
[  169  3177  7108  7398  8858  9026 15158 15286 15584 17273]
17
prec   = 88.12 84.87 80.39 75.30 70.74 66.54 62.91 59.64 56.61 53.82 51.41 49.17 47.09 45.17 43.47 41.90 40.43 39.14 37.89 36.76
recall = 5.26 10.05 14.14 17.47 20.35 22.84 25.05 26.98 28.70 30.20 31.63 32.89 34.03 35.09 36.12 37.03 37.92 38.80 39.57 40.34
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 38.93

============================= JOB FEEDBACK =============================

NodeName=uc2n904
Job ID: 23813669
Cluster: uc2
User/Group: ul_ruw26/ul_student
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 32
CPU Utilized: 05:16:45
CPU Efficiency: 16.93% of 1-07:10:56 core-walltime
Job Wall-clock time: 00:58:28
Memory Utilized: 25.56 GB
Memory Efficiency: 87.24% of 29.30 GB
