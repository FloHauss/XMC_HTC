------- Ensemble run at 2024-06-29 22:18:38 for wiki10-31k ----------
06/29/2024 22:20:22 - INFO - __main__ - Setting random seed 0
06/29/2024 22:20:23 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
06/29/2024 22:20:23 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
06/29/2024 22:20:23 - INFO - __main__ - Loaded 14146 training sequences
06/29/2024 22:20:27 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
06/29/2024 22:20:27 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
06/29/2024 22:20:27 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
06/29/2024 22:20:34 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
06/29/2024 22:20:34 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
06/29/2024 22:20:42 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=8.280699729919434 *****
06/29/2024 22:20:51 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23813670/tmp80vnrhdg/X_trn.pt
06/29/2024 22:20:52 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/29/2024 22:21:15 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/29/2024 22:21:15 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/29/2024 22:21:15 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/29/2024 22:21:15 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/29/2024 22:21:15 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
06/29/2024 22:21:15 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 5
06/29/2024 22:21:15 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/29/2024 22:21:15 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/29/2024 22:21:15 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/29/2024 22:21:15 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 1000
06/29/2024 22:21:45 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][    50/  1000] |   49/ 222 batches | ms/batch 514.8280 | train_loss 9.564973e-01 | lr 2.500000e-05
06/29/2024 22:22:03 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   100/  1000] |   99/ 222 batches | ms/batch 351.9594 | train_loss 4.522005e-01 | lr 5.000000e-05
06/29/2024 22:22:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   150/  1000] |  149/ 222 batches | ms/batch 353.4584 | train_loss 3.763055e-01 | lr 4.722222e-05
06/29/2024 22:22:40 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   200/  1000] |  199/ 222 batches | ms/batch 354.6815 | train_loss 3.235207e-01 | lr 4.444444e-05
06/29/2024 22:22:40 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmp5d6kzd9z at global_step 200 ****
06/29/2024 22:22:41 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:23:02 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   250/  1000] |   27/ 222 batches | ms/batch 350.3130 | train_loss 2.896383e-01 | lr 4.166667e-05
06/29/2024 22:23:20 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   300/  1000] |   77/ 222 batches | ms/batch 356.4649 | train_loss 2.786670e-01 | lr 3.888889e-05
06/29/2024 22:23:39 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   350/  1000] |  127/ 222 batches | ms/batch 356.8590 | train_loss 2.654355e-01 | lr 3.611111e-05
06/29/2024 22:23:58 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   400/  1000] |  177/ 222 batches | ms/batch 357.1478 | train_loss 2.567890e-01 | lr 3.333333e-05
06/29/2024 22:23:58 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmp5d6kzd9z at global_step 400 ****
06/29/2024 22:23:58 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:24:20 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   450/  1000] |    5/ 222 batches | ms/batch 351.8780 | train_loss 2.515313e-01 | lr 3.055556e-05
06/29/2024 22:24:38 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   500/  1000] |   55/ 222 batches | ms/batch 357.6350 | train_loss 2.429929e-01 | lr 2.777778e-05
06/29/2024 22:24:57 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   550/  1000] |  105/ 222 batches | ms/batch 359.8820 | train_loss 2.420875e-01 | lr 2.500000e-05
06/29/2024 22:25:16 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   600/  1000] |  155/ 222 batches | ms/batch 360.8201 | train_loss 2.410729e-01 | lr 2.222222e-05
06/29/2024 22:25:16 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmp5d6kzd9z at global_step 600 ****
06/29/2024 22:25:16 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:25:35 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   650/  1000] |  205/ 222 batches | ms/batch 361.8569 | train_loss 2.358214e-01 | lr 1.944444e-05
06/29/2024 22:25:56 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   700/  1000] |   33/ 222 batches | ms/batch 356.0640 | train_loss 2.326700e-01 | lr 1.666667e-05
06/29/2024 22:26:15 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   750/  1000] |   83/ 222 batches | ms/batch 362.4715 | train_loss 2.295647e-01 | lr 1.388889e-05
06/29/2024 22:26:34 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   800/  1000] |  133/ 222 batches | ms/batch 361.9206 | train_loss 2.277523e-01 | lr 1.111111e-05
06/29/2024 22:26:34 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmp5d6kzd9z at global_step 800 ****
06/29/2024 22:26:35 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:26:54 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   850/  1000] |  183/ 222 batches | ms/batch 361.6045 | train_loss 2.257637e-01 | lr 8.333333e-06
06/29/2024 22:27:15 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   900/  1000] |   11/ 222 batches | ms/batch 354.6970 | train_loss 2.270423e-01 | lr 5.555556e-06
06/29/2024 22:27:34 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   950/  1000] |   61/ 222 batches | ms/batch 360.0893 | train_loss 2.212483e-01 | lr 2.777778e-06
06/29/2024 22:27:52 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][  1000/  1000] |  111/ 222 batches | ms/batch 359.7433 | train_loss 2.219468e-01 | lr 0.000000e+00
06/29/2024 22:27:52 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmp5d6kzd9z at global_step 1000 ****
06/29/2024 22:27:53 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:27:54 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813670/tmp5d6kzd9z
06/29/2024 22:27:54 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
06/29/2024 22:27:54 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/29/2024 22:28:26 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/29/2024 22:28:26 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/29/2024 22:28:33 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/29/2024 22:28:35 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.53492153258872
06/29/2024 22:28:36 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
06/29/2024 22:28:46 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813670/tmp80vnrhdg/X_trn.pt
06/29/2024 22:28:46 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/29/2024 22:29:01 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/29/2024 22:29:01 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/29/2024 22:29:02 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/29/2024 22:29:02 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/29/2024 22:29:02 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/29/2024 22:29:02 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/29/2024 22:29:02 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
06/29/2024 22:29:02 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 480
06/29/2024 22:29:02 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 5
06/29/2024 22:29:02 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/29/2024 22:29:02 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/29/2024 22:29:02 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/29/2024 22:29:02 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 1000
06/29/2024 22:29:25 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][    50/  1000] |   49/ 222 batches | ms/batch 370.6838 | train_loss 5.501302e-01 | lr 2.500000e-05
06/29/2024 22:29:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   100/  1000] |   99/ 222 batches | ms/batch 360.5524 | train_loss 4.980931e-01 | lr 5.000000e-05
06/29/2024 22:30:03 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   150/  1000] |  149/ 222 batches | ms/batch 361.1633 | train_loss 4.593542e-01 | lr 4.722222e-05
06/29/2024 22:30:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   200/  1000] |  199/ 222 batches | ms/batch 361.4472 | train_loss 4.464829e-01 | lr 4.444444e-05
06/29/2024 22:30:22 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmpp52nohn9 at global_step 200 ****
06/29/2024 22:30:23 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:30:45 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   250/  1000] |   27/ 222 batches | ms/batch 355.9326 | train_loss 4.411320e-01 | lr 4.166667e-05
06/29/2024 22:31:04 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   300/  1000] |   77/ 222 batches | ms/batch 361.3595 | train_loss 4.351354e-01 | lr 3.888889e-05
06/29/2024 22:31:23 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   350/  1000] |  127/ 222 batches | ms/batch 361.7712 | train_loss 4.323223e-01 | lr 3.611111e-05
06/29/2024 22:31:42 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   400/  1000] |  177/ 222 batches | ms/batch 361.5990 | train_loss 4.314666e-01 | lr 3.333333e-05
06/29/2024 22:31:42 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmpp52nohn9 at global_step 400 ****
06/29/2024 22:31:42 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:32:04 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   450/  1000] |    5/ 222 batches | ms/batch 355.9971 | train_loss 4.292235e-01 | lr 3.055556e-05
06/29/2024 22:32:23 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   500/  1000] |   55/ 222 batches | ms/batch 362.0893 | train_loss 4.255509e-01 | lr 2.777778e-05
06/29/2024 22:32:42 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   550/  1000] |  105/ 222 batches | ms/batch 361.4385 | train_loss 4.244602e-01 | lr 2.500000e-05
06/29/2024 22:33:01 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   600/  1000] |  155/ 222 batches | ms/batch 362.5654 | train_loss 4.235129e-01 | lr 2.222222e-05
06/29/2024 22:33:01 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmpp52nohn9 at global_step 600 ****
06/29/2024 22:33:02 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:33:21 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   650/  1000] |  205/ 222 batches | ms/batch 361.8057 | train_loss 4.221066e-01 | lr 1.944444e-05
06/29/2024 22:33:43 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   700/  1000] |   33/ 222 batches | ms/batch 360.2558 | train_loss 4.199548e-01 | lr 1.666667e-05
06/29/2024 22:34:02 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   750/  1000] |   83/ 222 batches | ms/batch 361.7343 | train_loss 4.182249e-01 | lr 1.388889e-05
06/29/2024 22:34:21 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   800/  1000] |  133/ 222 batches | ms/batch 362.3925 | train_loss 4.182703e-01 | lr 1.111111e-05
06/29/2024 22:34:21 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmpp52nohn9 at global_step 800 ****
06/29/2024 22:34:22 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:34:41 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   850/  1000] |  183/ 222 batches | ms/batch 362.0613 | train_loss 4.175095e-01 | lr 8.333333e-06
06/29/2024 22:35:04 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   900/  1000] |   11/ 222 batches | ms/batch 361.4082 | train_loss 4.164981e-01 | lr 5.555556e-06
06/29/2024 22:35:23 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   950/  1000] |   61/ 222 batches | ms/batch 363.7798 | train_loss 4.155050e-01 | lr 2.777778e-06
06/29/2024 22:35:42 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][  1000/  1000] |  111/ 222 batches | ms/batch 363.6390 | train_loss 4.154747e-01 | lr 0.000000e+00
06/29/2024 22:35:42 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmpp52nohn9 at global_step 1000 ****
06/29/2024 22:35:43 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:35:43 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813670/tmpp52nohn9
06/29/2024 22:35:44 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
06/29/2024 22:35:44 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/29/2024 22:36:17 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/29/2024 22:36:18 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/29/2024 22:36:31 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/29/2024 22:36:33 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=25.939488194542626
06/29/2024 22:36:35 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
06/29/2024 22:36:45 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813670/tmp80vnrhdg/X_trn.pt
06/29/2024 22:36:45 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/29/2024 22:36:57 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/29/2024 22:36:57 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/29/2024 22:36:57 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/29/2024 22:36:57 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/29/2024 22:36:58 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/29/2024 22:36:58 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/29/2024 22:36:58 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
06/29/2024 22:36:58 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 607
06/29/2024 22:36:58 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
06/29/2024 22:36:58 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/29/2024 22:36:58 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/29/2024 22:36:58 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/29/2024 22:36:58 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
06/29/2024 22:37:20 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 359.4831 | train_loss 4.301003e-01 | lr 2.500000e-05
06/29/2024 22:37:39 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 361.0812 | train_loss 4.260902e-01 | lr 5.000000e-05
06/29/2024 22:37:58 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 361.7492 | train_loss 4.244524e-01 | lr 4.166667e-05
06/29/2024 22:38:18 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 366.9476 | train_loss 4.234818e-01 | lr 3.333333e-05
06/29/2024 22:38:18 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmpos6b0890 at global_step 200 ****
06/29/2024 22:38:18 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:38:41 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 357.5896 | train_loss 4.202434e-01 | lr 2.500000e-05
06/29/2024 22:39:00 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 364.1588 | train_loss 4.146254e-01 | lr 1.666667e-05
06/29/2024 22:39:19 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 363.7546 | train_loss 4.140655e-01 | lr 8.333333e-06
06/29/2024 22:39:38 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 362.8433 | train_loss 4.102792e-01 | lr 0.000000e+00
06/29/2024 22:39:38 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmpos6b0890 at global_step 400 ****
06/29/2024 22:39:39 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:39:40 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813670/tmpos6b0890
06/29/2024 22:39:40 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.25222677788776
06/29/2024 22:39:40 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/29/2024 22:40:14 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
06/29/2024 22:40:18 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
06/29/2024 22:40:18 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
06/29/2024 22:40:18 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/29/2024 22:40:19 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/29/2024 22:40:22 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/29/2024 22:40:31 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
06/29/2024 22:41:32 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/wiki10-31k/bert/param.json
06/29/2024 22:41:35 - INFO - pecos.xmc.xtransformer.model - Model saved to models/wiki10-31k/bert
06/29/2024 22:43:26 - INFO - __main__ - Setting random seed 0
06/29/2024 22:43:26 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
06/29/2024 22:43:26 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
06/29/2024 22:43:26 - INFO - __main__ - Loaded 14146 training sequences
06/29/2024 22:43:30 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
06/29/2024 22:43:30 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
06/29/2024 22:43:30 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/29/2024 22:43:34 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
06/29/2024 22:43:34 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
06/29/2024 22:43:47 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=12.730083703994751 *****
06/29/2024 22:43:54 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23813670/tmpr_2gfxvu/X_trn.pt
06/29/2024 22:43:55 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/29/2024 22:43:56 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/29/2024 22:43:56 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/29/2024 22:43:56 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/29/2024 22:43:56 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/29/2024 22:43:56 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
06/29/2024 22:43:56 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 3
06/29/2024 22:43:56 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/29/2024 22:43:56 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/29/2024 22:43:56 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/29/2024 22:43:56 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 600
06/29/2024 22:44:19 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][    50/   600] |   49/ 222 batches | ms/batch 397.6742 | train_loss 8.438361e-01 | lr 2.500000e-05
06/29/2024 22:44:37 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   100/   600] |   99/ 222 batches | ms/batch 360.5255 | train_loss 4.244485e-01 | lr 5.000000e-05
06/29/2024 22:44:56 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   150/   600] |  149/ 222 batches | ms/batch 362.1526 | train_loss 3.231967e-01 | lr 4.500000e-05
06/29/2024 22:45:15 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   200/   600] |  199/ 222 batches | ms/batch 363.8505 | train_loss 2.821491e-01 | lr 4.000000e-05
06/29/2024 22:45:15 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmpebn05318 at global_step 200 ****
06/29/2024 22:45:16 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:45:37 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   250/   600] |   27/ 222 batches | ms/batch 358.7728 | train_loss 2.643365e-01 | lr 3.500000e-05
06/29/2024 22:45:56 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   300/   600] |   77/ 222 batches | ms/batch 365.6535 | train_loss 2.501131e-01 | lr 3.000000e-05
06/29/2024 22:46:15 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   350/   600] |  127/ 222 batches | ms/batch 365.3365 | train_loss 2.476592e-01 | lr 2.500000e-05
06/29/2024 22:46:34 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   400/   600] |  177/ 222 batches | ms/batch 365.4413 | train_loss 2.401966e-01 | lr 2.000000e-05
06/29/2024 22:46:34 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmpebn05318 at global_step 400 ****
06/29/2024 22:46:35 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:46:56 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   450/   600] |    5/ 222 batches | ms/batch 359.2505 | train_loss 2.416480e-01 | lr 1.500000e-05
06/29/2024 22:47:15 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   500/   600] |   55/ 222 batches | ms/batch 366.4144 | train_loss 2.291316e-01 | lr 1.000000e-05
06/29/2024 22:47:34 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   550/   600] |  105/ 222 batches | ms/batch 366.0341 | train_loss 2.287876e-01 | lr 5.000000e-06
06/29/2024 22:47:54 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   600/   600] |  155/ 222 batches | ms/batch 365.9446 | train_loss 2.300686e-01 | lr 0.000000e+00
06/29/2024 22:47:54 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmpebn05318 at global_step 600 ****
06/29/2024 22:47:54 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:47:55 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813670/tmpebn05318
06/29/2024 22:47:55 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
06/29/2024 22:47:55 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/29/2024 22:48:27 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/29/2024 22:48:28 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/29/2024 22:48:34 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/29/2024 22:48:37 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.42747066308497
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/29/2024 22:48:38 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
06/29/2024 22:48:50 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813670/tmpr_2gfxvu/X_trn.pt
06/29/2024 22:48:50 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/29/2024 22:49:03 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/29/2024 22:49:03 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/29/2024 22:49:03 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/29/2024 22:49:03 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/29/2024 22:49:04 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/29/2024 22:49:04 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/29/2024 22:49:04 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
06/29/2024 22:49:04 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 448
06/29/2024 22:49:04 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
06/29/2024 22:49:04 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/29/2024 22:49:04 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/29/2024 22:49:04 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/29/2024 22:49:04 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
06/29/2024 22:49:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 367.3759 | train_loss 5.004413e-01 | lr 2.500000e-05
06/29/2024 22:49:45 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 367.6676 | train_loss 4.297051e-01 | lr 5.000000e-05
06/29/2024 22:50:04 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 368.9975 | train_loss 4.215094e-01 | lr 4.166667e-05
06/29/2024 22:50:24 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 369.3534 | train_loss 4.149169e-01 | lr 3.333333e-05
06/29/2024 22:50:24 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmp8e357591 at global_step 200 ****
06/29/2024 22:50:24 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:50:47 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 363.2370 | train_loss 4.080080e-01 | lr 2.500000e-05
06/29/2024 22:51:06 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 368.8526 | train_loss 4.001976e-01 | lr 1.666667e-05
06/29/2024 22:51:25 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 368.6629 | train_loss 3.990909e-01 | lr 8.333333e-06
06/29/2024 22:51:45 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 368.9038 | train_loss 3.960316e-01 | lr 0.000000e+00
06/29/2024 22:51:45 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmp8e357591 at global_step 400 ****
06/29/2024 22:51:45 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:51:46 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813670/tmp8e357591
06/29/2024 22:51:47 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
06/29/2024 22:51:47 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/29/2024 22:52:20 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/29/2024 22:52:21 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/29/2024 22:52:34 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/29/2024 22:52:36 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=26.07005513926198
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/29/2024 22:52:38 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
06/29/2024 22:52:49 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813670/tmpr_2gfxvu/X_trn.pt
06/29/2024 22:52:49 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/29/2024 22:53:01 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/29/2024 22:53:01 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/29/2024 22:53:01 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/29/2024 22:53:01 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/29/2024 22:53:02 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/29/2024 22:53:02 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/29/2024 22:53:02 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
06/29/2024 22:53:02 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 621
06/29/2024 22:53:02 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
06/29/2024 22:53:02 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/29/2024 22:53:02 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/29/2024 22:53:02 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/29/2024 22:53:02 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 200
06/29/2024 22:53:24 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   200] |   49/ 222 batches | ms/batch 368.7873 | train_loss 4.483331e-01 | lr 2.500000e-05
06/29/2024 22:53:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   200] |   99/ 222 batches | ms/batch 368.6531 | train_loss 4.428938e-01 | lr 5.000000e-05
06/29/2024 22:54:03 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   200] |  149/ 222 batches | ms/batch 369.5117 | train_loss 4.443955e-01 | lr 2.500000e-05
06/29/2024 22:54:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   200] |  199/ 222 batches | ms/batch 369.1752 | train_loss 4.404747e-01 | lr 0.000000e+00
06/29/2024 22:54:23 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmphm0ze_3q at global_step 200 ****
06/29/2024 22:54:23 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 22:54:24 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813670/tmphm0ze_3q
06/29/2024 22:54:25 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.12370988265235
06/29/2024 22:54:25 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/29/2024 22:54:59 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
06/29/2024 22:55:03 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
06/29/2024 22:55:03 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
06/29/2024 22:55:03 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/29/2024 22:55:04 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/29/2024 22:55:07 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/29/2024 22:55:15 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
06/29/2024 22:56:15 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/wiki10-31k/roberta/param.json
06/29/2024 22:56:18 - INFO - pecos.xmc.xtransformer.model - Model saved to models/wiki10-31k/roberta
06/29/2024 22:57:22 - INFO - __main__ - Setting random seed 0
06/29/2024 22:57:22 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
06/29/2024 22:57:22 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
06/29/2024 22:57:22 - INFO - __main__ - Loaded 14146 training sequences
06/29/2024 22:57:26 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
06/29/2024 22:57:26 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
06/29/2024 22:57:26 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
06/29/2024 22:57:27 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
06/29/2024 22:57:27 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
06/29/2024 22:57:45 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=17.306842803955078 *****
06/29/2024 22:57:54 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23813670/tmpbrninzd9/X_trn.pt
06/29/2024 22:57:54 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/29/2024 22:57:55 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/29/2024 22:57:55 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/29/2024 22:57:55 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/29/2024 22:57:55 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/29/2024 22:57:55 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
06/29/2024 22:57:55 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
06/29/2024 22:57:55 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/29/2024 22:57:55 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/29/2024 22:57:55 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/29/2024 22:57:55 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
06/29/2024 22:58:30 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 623.9921 | train_loss 7.426081e-01 | lr 2.500000e-05
06/29/2024 22:58:58 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 559.7709 | train_loss 4.033739e-01 | lr 5.000000e-05
06/29/2024 22:59:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 558.9688 | train_loss 3.116383e-01 | lr 4.166667e-05
06/29/2024 22:59:56 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 562.7069 | train_loss 2.876156e-01 | lr 3.333333e-05
06/29/2024 22:59:56 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmpyu_i6iz6 at global_step 200 ****
06/29/2024 22:59:57 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 23:00:28 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 554.9334 | train_loss 2.694991e-01 | lr 2.500000e-05
06/29/2024 23:00:57 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 563.4684 | train_loss 2.656873e-01 | lr 1.666667e-05
06/29/2024 23:01:26 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 562.7019 | train_loss 2.594246e-01 | lr 8.333333e-06
06/29/2024 23:01:55 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 562.7120 | train_loss 2.556829e-01 | lr 0.000000e+00
06/29/2024 23:01:55 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmpyu_i6iz6 at global_step 400 ****
06/29/2024 23:01:55 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 23:01:56 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813670/tmpyu_i6iz6
06/29/2024 23:01:57 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
06/29/2024 23:01:57 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/29/2024 23:02:46 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/29/2024 23:02:46 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/29/2024 23:02:53 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/29/2024 23:02:57 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.443234836702956
06/29/2024 23:02:58 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
06/29/2024 23:03:09 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813670/tmpbrninzd9/X_trn.pt
06/29/2024 23:03:09 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/29/2024 23:03:22 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/29/2024 23:03:22 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/29/2024 23:03:22 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/29/2024 23:03:22 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/29/2024 23:03:23 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/29/2024 23:03:23 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/29/2024 23:03:23 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
06/29/2024 23:03:23 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 448
06/29/2024 23:03:23 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
06/29/2024 23:03:23 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/29/2024 23:03:23 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/29/2024 23:03:23 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/29/2024 23:03:23 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
06/29/2024 23:03:55 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 564.2872 | train_loss 6.265204e-01 | lr 2.500000e-05
06/29/2024 23:04:24 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 564.6241 | train_loss 5.100898e-01 | lr 5.000000e-05
06/29/2024 23:04:54 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 565.3411 | train_loss 4.734766e-01 | lr 4.166667e-05
06/29/2024 23:05:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 568.9806 | train_loss 4.557305e-01 | lr 3.333333e-05
06/29/2024 23:05:23 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmpfhsblnqb at global_step 200 ****
06/29/2024 23:05:24 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 23:05:57 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 556.9202 | train_loss 4.442823e-01 | lr 2.500000e-05
06/29/2024 23:06:26 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 564.8055 | train_loss 4.355974e-01 | lr 1.666667e-05
06/29/2024 23:06:55 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 567.3011 | train_loss 4.298155e-01 | lr 8.333333e-06
06/29/2024 23:07:25 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 565.5717 | train_loss 4.277860e-01 | lr 0.000000e+00
06/29/2024 23:07:25 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmpfhsblnqb at global_step 400 ****
06/29/2024 23:07:25 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 23:07:26 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813670/tmpfhsblnqb
06/29/2024 23:07:27 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
06/29/2024 23:07:27 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/29/2024 23:08:18 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
06/29/2024 23:08:19 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
06/29/2024 23:08:32 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
06/29/2024 23:08:35 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=26.09416089353881
06/29/2024 23:08:37 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
06/29/2024 23:08:49 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23813670/tmpbrninzd9/X_trn.pt
06/29/2024 23:08:49 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
06/29/2024 23:09:02 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
06/29/2024 23:09:02 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
06/29/2024 23:09:02 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
06/29/2024 23:09:02 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/29/2024 23:09:03 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
06/29/2024 23:09:03 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
06/29/2024 23:09:03 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
06/29/2024 23:09:03 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 648
06/29/2024 23:09:03 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
06/29/2024 23:09:03 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
06/29/2024 23:09:03 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
06/29/2024 23:09:03 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
06/29/2024 23:09:03 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 200
06/29/2024 23:09:36 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   200] |   49/ 222 batches | ms/batch 565.9435 | train_loss 5.175586e-01 | lr 2.500000e-05
06/29/2024 23:10:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   200] |   99/ 222 batches | ms/batch 567.4257 | train_loss 5.056437e-01 | lr 5.000000e-05
06/29/2024 23:10:34 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   200] |  149/ 222 batches | ms/batch 568.5923 | train_loss 5.025583e-01 | lr 2.500000e-05
06/29/2024 23:11:04 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   200] |  199/ 222 batches | ms/batch 568.8866 | train_loss 4.898114e-01 | lr 0.000000e+00
06/29/2024 23:11:04 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23813670/tmp19ow9n4c at global_step 200 ****
06/29/2024 23:11:04 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
06/29/2024 23:11:05 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23813670/tmp19ow9n4c
06/29/2024 23:11:06 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.09698854799944
06/29/2024 23:11:06 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
06/29/2024 23:11:57 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
06/29/2024 23:12:01 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
06/29/2024 23:12:01 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
06/29/2024 23:12:02 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/29/2024 23:12:03 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/29/2024 23:12:05 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
06/29/2024 23:12:14 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
06/29/2024 23:13:17 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/wiki10-31k/xlnet/param.json
06/29/2024 23:13:20 - INFO - pecos.xmc.xtransformer.model - Model saved to models/wiki10-31k/xlnet
==== evaluation results ====
param: bert
[  169  3177  7108  7398  8858  9026 15158 15286 15584 17273]
17
prec   = 87.94 84.42 79.80 74.54 69.64 65.31 61.50 58.08 54.97 52.32 49.79 47.55 45.51 43.67 41.96 40.38 38.95 37.65 36.44 35.34
recall = 5.24 10.00 14.04 17.27 20.03 22.41 24.49 26.29 27.89 29.38 30.67 31.85 32.95 33.98 34.91 35.79 36.60 37.38 38.14 38.86
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 37.59
param: roberta
[  169  3177  7108  7398  8858  9026 15158 15286 15584 17273]
17
prec   = 87.97 84.42 79.48 74.12 69.17 64.77 60.87 57.47 54.36 51.66 49.22 47.09 45.00 43.21 41.57 40.05 38.68 37.41 36.23 35.14
recall = 5.25 9.99 13.96 17.19 19.90 22.20 24.20 26.02 27.57 29.03 30.34 31.58 32.62 33.66 34.61 35.51 36.36 37.17 37.93 38.67
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 37.35
param: xlnet
[  169  3177  7108  7398  8858  9026 15158 15286 15584 17273]
17
prec   = 87.97 84.39 79.50 74.44 69.70 65.36 61.46 58.15 55.02 52.26 49.85 47.65 45.58 43.73 42.05 40.48 39.04 37.74 36.59 35.45
recall = 5.25 9.98 13.98 17.28 20.07 22.44 24.49 26.36 27.95 29.39 30.73 31.93 32.99 34.02 34.97 35.84 36.66 37.46 38.29 39.00
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 37.70
==== rank_average ensemble results ====
[  169  3177  7108  7398  8858  9026 15158 15286 15584 17273]
17
prec   = 88.12 84.87 80.39 75.30 70.74 66.54 62.91 59.64 56.61 53.82 51.41 49.17 47.09 45.17 43.47 41.90 40.43 39.14 37.89 36.76
recall = 5.26 10.05 14.14 17.47 20.35 22.84 25.05 26.98 28.70 30.20 31.63 32.89 34.03 35.09 36.12 37.03 37.92 38.80 39.57 40.34
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 38.93

============================= JOB FEEDBACK =============================

NodeName=uc2n904
Job ID: 23813670
Cluster: uc2
User/Group: ul_ruw26/ul_student
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 32
CPU Utilized: 05:17:49
CPU Efficiency: 17.19% of 1-06:48:32 core-walltime
Job Wall-clock time: 00:57:46
Memory Utilized: 28.69 GB
Memory Efficiency: 97.93% of 29.30 GB
