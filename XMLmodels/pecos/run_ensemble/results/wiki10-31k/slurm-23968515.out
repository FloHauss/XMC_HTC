------- Ensemble run at 2024-08-17 05:33:53 for wiki10-31k ----------
08/17/2024 05:35:55 - INFO - __main__ - Setting random seed 0
08/17/2024 05:35:56 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
08/17/2024 05:35:56 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
08/17/2024 05:35:57 - INFO - __main__ - Loaded 14146 training sequences
08/17/2024 05:36:01 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
08/17/2024 05:36:01 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
08/17/2024 05:36:01 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
08/17/2024 05:36:08 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
08/17/2024 05:36:08 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
08/17/2024 05:36:16 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=8.17378830909729 *****
08/17/2024 05:36:25 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23968515/tmph1eeddz_/X_trn.pt
08/17/2024 05:36:26 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/17/2024 05:36:48 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/17/2024 05:36:48 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/17/2024 05:36:49 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/17/2024 05:36:49 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
08/17/2024 05:36:49 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
08/17/2024 05:36:49 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 5
08/17/2024 05:36:49 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/17/2024 05:36:49 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/17/2024 05:36:49 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/17/2024 05:36:49 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 1000
08/17/2024 05:37:18 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][    50/  1000] |   49/ 222 batches | ms/batch 496.9040 | train_loss 9.564973e-01 | lr 2.500000e-05
08/17/2024 05:37:35 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   100/  1000] |   99/ 222 batches | ms/batch 323.9823 | train_loss 4.522005e-01 | lr 5.000000e-05
08/17/2024 05:37:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   150/  1000] |  149/ 222 batches | ms/batch 325.5502 | train_loss 3.763055e-01 | lr 4.722222e-05
08/17/2024 05:38:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   200/  1000] |  199/ 222 batches | ms/batch 327.3341 | train_loss 3.235207e-01 | lr 4.444444e-05
08/17/2024 05:38:09 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpjsg0fp3s at global_step 200 ****
08/17/2024 05:38:10 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 05:38:30 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   250/  1000] |   27/ 222 batches | ms/batch 324.1951 | train_loss 2.896383e-01 | lr 4.166667e-05
08/17/2024 05:38:47 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   300/  1000] |   77/ 222 batches | ms/batch 328.2306 | train_loss 2.786670e-01 | lr 3.888889e-05
08/17/2024 05:39:04 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   350/  1000] |  127/ 222 batches | ms/batch 329.0708 | train_loss 2.654355e-01 | lr 3.611111e-05
08/17/2024 05:39:21 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   400/  1000] |  177/ 222 batches | ms/batch 329.7468 | train_loss 2.567890e-01 | lr 3.333333e-05
08/17/2024 05:39:21 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpjsg0fp3s at global_step 400 ****
08/17/2024 05:39:22 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 05:39:42 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   450/  1000] |    5/ 222 batches | ms/batch 324.6066 | train_loss 2.515313e-01 | lr 3.055556e-05
08/17/2024 05:39:59 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   500/  1000] |   55/ 222 batches | ms/batch 330.4544 | train_loss 2.429929e-01 | lr 2.777778e-05
08/17/2024 05:40:16 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   550/  1000] |  105/ 222 batches | ms/batch 330.1691 | train_loss 2.420875e-01 | lr 2.500000e-05
08/17/2024 05:40:33 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   600/  1000] |  155/ 222 batches | ms/batch 330.2429 | train_loss 2.410729e-01 | lr 2.222222e-05
08/17/2024 05:40:33 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpjsg0fp3s at global_step 600 ****
08/17/2024 05:40:34 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 05:40:51 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   650/  1000] |  205/ 222 batches | ms/batch 330.6633 | train_loss 2.358214e-01 | lr 1.944444e-05
08/17/2024 05:41:11 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   700/  1000] |   33/ 222 batches | ms/batch 325.2517 | train_loss 2.326700e-01 | lr 1.666667e-05
08/17/2024 05:41:29 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   750/  1000] |   83/ 222 batches | ms/batch 332.3066 | train_loss 2.295647e-01 | lr 1.388889e-05
08/17/2024 05:41:46 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   800/  1000] |  133/ 222 batches | ms/batch 332.5771 | train_loss 2.277523e-01 | lr 1.111111e-05
08/17/2024 05:41:46 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpjsg0fp3s at global_step 800 ****
08/17/2024 05:41:47 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 05:42:04 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   850/  1000] |  183/ 222 batches | ms/batch 332.4512 | train_loss 2.257637e-01 | lr 8.333333e-06
08/17/2024 05:42:24 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   900/  1000] |   11/ 222 batches | ms/batch 326.5689 | train_loss 2.270423e-01 | lr 5.555556e-06
08/17/2024 05:42:41 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   950/  1000] |   61/ 222 batches | ms/batch 332.3368 | train_loss 2.212483e-01 | lr 2.777778e-06
08/17/2024 05:42:59 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][  1000/  1000] |  111/ 222 batches | ms/batch 331.5403 | train_loss 2.219468e-01 | lr 0.000000e+00
08/17/2024 05:42:59 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpjsg0fp3s at global_step 1000 ****
08/17/2024 05:42:59 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 05:43:00 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968515/tmpjsg0fp3s
08/17/2024 05:43:00 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
08/17/2024 05:43:00 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
08/17/2024 05:43:30 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/17/2024 05:43:30 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/17/2024 05:43:37 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/17/2024 05:43:39 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.53492153258872
08/17/2024 05:43:41 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
08/17/2024 05:43:51 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23968515/tmph1eeddz_/X_trn.pt
08/17/2024 05:43:51 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/17/2024 05:44:06 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/17/2024 05:44:06 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/17/2024 05:44:06 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/17/2024 05:44:06 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/17/2024 05:44:07 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/17/2024 05:44:07 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
08/17/2024 05:44:07 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
08/17/2024 05:44:07 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 480
08/17/2024 05:44:07 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 5
08/17/2024 05:44:07 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/17/2024 05:44:07 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/17/2024 05:44:07 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/17/2024 05:44:07 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 1000
08/17/2024 05:44:29 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][    50/  1000] |   49/ 222 batches | ms/batch 345.8599 | train_loss 5.501302e-01 | lr 2.500000e-05
08/17/2024 05:44:47 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   100/  1000] |   99/ 222 batches | ms/batch 331.9980 | train_loss 4.980931e-01 | lr 5.000000e-05
08/17/2024 05:45:04 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   150/  1000] |  149/ 222 batches | ms/batch 331.5382 | train_loss 4.593542e-01 | lr 4.722222e-05
08/17/2024 05:45:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   5][   200/  1000] |  199/ 222 batches | ms/batch 332.6833 | train_loss 4.464829e-01 | lr 4.444444e-05
08/17/2024 05:45:22 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpl33y1h1s at global_step 200 ****
08/17/2024 05:45:22 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 05:45:43 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   250/  1000] |   27/ 222 batches | ms/batch 327.6027 | train_loss 4.411320e-01 | lr 4.166667e-05
08/17/2024 05:46:01 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   300/  1000] |   77/ 222 batches | ms/batch 331.3749 | train_loss 4.351354e-01 | lr 3.888889e-05
08/17/2024 05:46:18 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   350/  1000] |  127/ 222 batches | ms/batch 331.6622 | train_loss 4.323223e-01 | lr 3.611111e-05
08/17/2024 05:46:36 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   5][   400/  1000] |  177/ 222 batches | ms/batch 332.5486 | train_loss 4.314666e-01 | lr 3.333333e-05
08/17/2024 05:46:36 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpl33y1h1s at global_step 400 ****
08/17/2024 05:46:37 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 05:46:57 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   450/  1000] |    5/ 222 batches | ms/batch 326.1619 | train_loss 4.292235e-01 | lr 3.055556e-05
08/17/2024 05:47:15 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   500/  1000] |   55/ 222 batches | ms/batch 333.1626 | train_loss 4.255509e-01 | lr 2.777778e-05
08/17/2024 05:47:33 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   550/  1000] |  105/ 222 batches | ms/batch 332.6139 | train_loss 4.244602e-01 | lr 2.500000e-05
08/17/2024 05:47:50 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   600/  1000] |  155/ 222 batches | ms/batch 333.7745 | train_loss 4.235129e-01 | lr 2.222222e-05
08/17/2024 05:47:50 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpl33y1h1s at global_step 600 ****
08/17/2024 05:47:51 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 05:48:08 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   5][   650/  1000] |  205/ 222 batches | ms/batch 334.0014 | train_loss 4.221066e-01 | lr 1.944444e-05
08/17/2024 05:48:29 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   700/  1000] |   33/ 222 batches | ms/batch 328.3388 | train_loss 4.199548e-01 | lr 1.666667e-05
08/17/2024 05:48:47 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   750/  1000] |   83/ 222 batches | ms/batch 334.0212 | train_loss 4.182249e-01 | lr 1.388889e-05
08/17/2024 05:49:04 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   800/  1000] |  133/ 222 batches | ms/batch 333.0892 | train_loss 4.182703e-01 | lr 1.111111e-05
08/17/2024 05:49:04 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpl33y1h1s at global_step 800 ****
08/17/2024 05:49:05 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 05:49:22 - INFO - pecos.xmc.xtransformer.matcher - | [   4/   5][   850/  1000] |  183/ 222 batches | ms/batch 333.7979 | train_loss 4.175095e-01 | lr 8.333333e-06
08/17/2024 05:49:43 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   900/  1000] |   11/ 222 batches | ms/batch 327.9997 | train_loss 4.164981e-01 | lr 5.555556e-06
08/17/2024 05:50:01 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][   950/  1000] |   61/ 222 batches | ms/batch 332.3644 | train_loss 4.155050e-01 | lr 2.777778e-06
08/17/2024 05:50:18 - INFO - pecos.xmc.xtransformer.matcher - | [   5/   5][  1000/  1000] |  111/ 222 batches | ms/batch 332.3184 | train_loss 4.154747e-01 | lr 0.000000e+00
08/17/2024 05:50:18 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpl33y1h1s at global_step 1000 ****
08/17/2024 05:50:19 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 05:50:19 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968515/tmpl33y1h1s
08/17/2024 05:50:20 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
08/17/2024 05:50:20 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
08/17/2024 05:50:50 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/17/2024 05:50:50 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/17/2024 05:51:04 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/17/2024 05:51:06 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=25.939488194542626
08/17/2024 05:51:08 - INFO - pecos.xmc.xtransformer.matcher - Downloaded bert-base-uncased model from s3.
08/17/2024 05:51:18 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23968515/tmph1eeddz_/X_trn.pt
08/17/2024 05:51:18 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/17/2024 05:51:30 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/17/2024 05:51:30 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/17/2024 05:51:30 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/17/2024 05:51:30 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/17/2024 05:51:31 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/17/2024 05:51:31 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
08/17/2024 05:51:31 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
08/17/2024 05:51:31 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 607
08/17/2024 05:51:31 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
08/17/2024 05:51:31 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/17/2024 05:51:31 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/17/2024 05:51:31 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/17/2024 05:51:31 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
08/17/2024 05:51:52 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 330.6947 | train_loss 4.301003e-01 | lr 2.500000e-05
08/17/2024 05:52:09 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 329.3643 | train_loss 4.260902e-01 | lr 5.000000e-05
08/17/2024 05:52:27 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 329.9536 | train_loss 4.244524e-01 | lr 4.166667e-05
08/17/2024 05:52:44 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 331.8929 | train_loss 4.234818e-01 | lr 3.333333e-05
08/17/2024 05:52:44 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpj426y610 at global_step 200 ****
08/17/2024 05:52:45 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 05:53:06 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 327.9817 | train_loss 4.202434e-01 | lr 2.500000e-05
08/17/2024 05:53:24 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 332.9939 | train_loss 4.146254e-01 | lr 1.666667e-05
08/17/2024 05:53:41 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 332.0358 | train_loss 4.140655e-01 | lr 8.333333e-06
08/17/2024 05:53:59 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 332.0090 | train_loss 4.102792e-01 | lr 0.000000e+00
08/17/2024 05:53:59 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpj426y610 at global_step 400 ****
08/17/2024 05:54:00 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 05:54:00 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968515/tmpj426y610
08/17/2024 05:54:01 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.25222677788776
08/17/2024 05:54:01 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
08/17/2024 05:54:32 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
08/17/2024 05:54:36 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
08/17/2024 05:54:36 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
08/17/2024 05:54:36 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
08/17/2024 05:54:37 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
08/17/2024 05:54:40 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
08/17/2024 05:54:49 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
08/17/2024 05:55:52 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/wiki10-31k/bert/param.json
08/17/2024 05:55:56 - INFO - pecos.xmc.xtransformer.model - Model saved to models/wiki10-31k/bert
08/17/2024 05:58:08 - INFO - __main__ - Setting random seed 0
08/17/2024 05:58:08 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
08/17/2024 05:58:08 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
08/17/2024 05:58:09 - INFO - __main__ - Loaded 14146 training sequences
08/17/2024 05:58:12 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
08/17/2024 05:58:12 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
08/17/2024 05:58:12 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/17/2024 05:58:17 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
08/17/2024 05:58:17 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
08/17/2024 05:58:29 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=12.72940468788147 *****
08/17/2024 05:58:37 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23968515/tmptf8kb1gv/X_trn.pt
08/17/2024 05:58:38 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/17/2024 05:58:38 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/17/2024 05:58:38 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/17/2024 05:58:39 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/17/2024 05:58:39 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
08/17/2024 05:58:39 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
08/17/2024 05:58:39 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 3
08/17/2024 05:58:39 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/17/2024 05:58:39 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/17/2024 05:58:39 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/17/2024 05:58:39 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 600
08/17/2024 05:59:00 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][    50/   600] |   49/ 222 batches | ms/batch 364.1308 | train_loss 8.438361e-01 | lr 2.500000e-05
08/17/2024 05:59:17 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   100/   600] |   99/ 222 batches | ms/batch 325.2388 | train_loss 4.244485e-01 | lr 5.000000e-05
08/17/2024 05:59:34 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   150/   600] |  149/ 222 batches | ms/batch 327.2806 | train_loss 3.231967e-01 | lr 4.500000e-05
08/17/2024 05:59:51 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   3][   200/   600] |  199/ 222 batches | ms/batch 328.5645 | train_loss 2.821491e-01 | lr 4.000000e-05
08/17/2024 05:59:51 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmp248vuoks at global_step 200 ****
08/17/2024 05:59:51 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 06:00:11 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   250/   600] |   27/ 222 batches | ms/batch 323.6382 | train_loss 2.643365e-01 | lr 3.500000e-05
08/17/2024 06:00:28 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   300/   600] |   77/ 222 batches | ms/batch 329.7759 | train_loss 2.501131e-01 | lr 3.000000e-05
08/17/2024 06:00:45 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   350/   600] |  127/ 222 batches | ms/batch 329.4577 | train_loss 2.476592e-01 | lr 2.500000e-05
08/17/2024 06:01:03 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   3][   400/   600] |  177/ 222 batches | ms/batch 331.6651 | train_loss 2.401966e-01 | lr 2.000000e-05
08/17/2024 06:01:03 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmp248vuoks at global_step 400 ****
08/17/2024 06:01:03 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 06:01:23 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   450/   600] |    5/ 222 batches | ms/batch 326.1653 | train_loss 2.416480e-01 | lr 1.500000e-05
08/17/2024 06:01:40 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   500/   600] |   55/ 222 batches | ms/batch 333.1273 | train_loss 2.291316e-01 | lr 1.000000e-05
08/17/2024 06:01:58 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   550/   600] |  105/ 222 batches | ms/batch 333.3195 | train_loss 2.287876e-01 | lr 5.000000e-06
08/17/2024 06:02:15 - INFO - pecos.xmc.xtransformer.matcher - | [   3/   3][   600/   600] |  155/ 222 batches | ms/batch 333.3925 | train_loss 2.300686e-01 | lr 0.000000e+00
08/17/2024 06:02:15 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmp248vuoks at global_step 600 ****
08/17/2024 06:02:16 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 06:02:17 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968515/tmp248vuoks
08/17/2024 06:02:17 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
08/17/2024 06:02:17 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
08/17/2024 06:02:46 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/17/2024 06:02:46 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/17/2024 06:02:52 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/17/2024 06:02:55 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.42747066308497
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/17/2024 06:02:56 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
08/17/2024 06:03:08 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23968515/tmptf8kb1gv/X_trn.pt
08/17/2024 06:03:08 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/17/2024 06:03:21 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/17/2024 06:03:21 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/17/2024 06:03:21 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/17/2024 06:03:21 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/17/2024 06:03:22 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/17/2024 06:03:22 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
08/17/2024 06:03:22 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
08/17/2024 06:03:22 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 448
08/17/2024 06:03:22 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
08/17/2024 06:03:22 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/17/2024 06:03:22 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/17/2024 06:03:22 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/17/2024 06:03:22 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
08/17/2024 06:03:42 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 332.9930 | train_loss 5.004413e-01 | lr 2.500000e-05
08/17/2024 06:03:59 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 330.6746 | train_loss 4.297051e-01 | lr 5.000000e-05
08/17/2024 06:04:17 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 331.8906 | train_loss 4.215094e-01 | lr 4.166667e-05
08/17/2024 06:04:34 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 332.4118 | train_loss 4.149169e-01 | lr 3.333333e-05
08/17/2024 06:04:34 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpe3ctpjig at global_step 200 ****
08/17/2024 06:04:35 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 06:04:55 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 327.1933 | train_loss 4.080080e-01 | lr 2.500000e-05
08/17/2024 06:05:13 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 333.8048 | train_loss 4.001976e-01 | lr 1.666667e-05
08/17/2024 06:05:31 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 334.3646 | train_loss 3.990909e-01 | lr 8.333333e-06
08/17/2024 06:05:48 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 334.2568 | train_loss 3.960316e-01 | lr 0.000000e+00
08/17/2024 06:05:48 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpe3ctpjig at global_step 400 ****
08/17/2024 06:05:49 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 06:05:50 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968515/tmpe3ctpjig
08/17/2024 06:05:50 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
08/17/2024 06:05:50 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
08/17/2024 06:06:20 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/17/2024 06:06:20 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/17/2024 06:06:33 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/17/2024 06:06:36 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=26.07005513926198
Some weights of RobertaForXMC were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/17/2024 06:06:38 - INFO - pecos.xmc.xtransformer.matcher - Downloaded roberta-base model from s3.
08/17/2024 06:06:48 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23968515/tmptf8kb1gv/X_trn.pt
08/17/2024 06:06:48 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/17/2024 06:07:00 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/17/2024 06:07:00 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/17/2024 06:07:00 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/17/2024 06:07:00 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/17/2024 06:07:01 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/17/2024 06:07:01 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
08/17/2024 06:07:01 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
08/17/2024 06:07:01 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 621
08/17/2024 06:07:01 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
08/17/2024 06:07:01 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/17/2024 06:07:01 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/17/2024 06:07:01 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/17/2024 06:07:01 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 200
08/17/2024 06:07:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   200] |   49/ 222 batches | ms/batch 331.0412 | train_loss 4.483331e-01 | lr 2.500000e-05
08/17/2024 06:07:39 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   200] |   99/ 222 batches | ms/batch 331.2216 | train_loss 4.428938e-01 | lr 5.000000e-05
08/17/2024 06:07:57 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   200] |  149/ 222 batches | ms/batch 331.8679 | train_loss 4.443955e-01 | lr 2.500000e-05
08/17/2024 06:08:14 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   200] |  199/ 222 batches | ms/batch 333.9054 | train_loss 4.404747e-01 | lr 0.000000e+00
08/17/2024 06:08:14 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmp8sg86im_ at global_step 200 ****
08/17/2024 06:08:15 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 06:08:16 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968515/tmp8sg86im_
08/17/2024 06:08:17 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.12370988265235
08/17/2024 06:08:17 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
08/17/2024 06:08:47 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
08/17/2024 06:08:51 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
08/17/2024 06:08:51 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
08/17/2024 06:08:51 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
08/17/2024 06:08:52 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
08/17/2024 06:08:55 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
08/17/2024 06:09:03 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
08/17/2024 06:10:04 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/wiki10-31k/roberta/param.json
08/17/2024 06:10:07 - INFO - pecos.xmc.xtransformer.model - Model saved to models/wiki10-31k/roberta
08/17/2024 06:11:53 - INFO - __main__ - Setting random seed 0
08/17/2024 06:11:53 - INFO - __main__ - Loaded training feature matrix with shape=(14146, 101938)
08/17/2024 06:11:53 - INFO - __main__ - Loaded training label matrix with shape=(14146, 30938)
08/17/2024 06:11:54 - INFO - __main__ - Loaded 14146 training sequences
08/17/2024 06:11:57 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree: [128, 2048, 30938]
08/17/2024 06:11:57 - INFO - pecos.xmc.xtransformer.model - Fine-tune Transformers with nr_labels=[128, 2048, 30938]
08/17/2024 06:11:57 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 0, nr_labels=128, avr_M_nnz=128
08/17/2024 06:11:59 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
08/17/2024 06:11:59 - INFO - pecos.xmc.xtransformer.matcher - ***** Encoding data len=14146 truncation=256*****
08/17/2024 06:12:13 - INFO - pecos.xmc.xtransformer.matcher - ***** Finished with time cost=13.83022928237915 *****
08/17/2024 06:12:22 - INFO - pecos.xmc.xtransformer.matcher - trn tensors saved to /scratch/slurm_tmpdir/job_23968515/tmp2jpm2658/X_trn.pt
08/17/2024 06:12:22 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/17/2024 06:12:23 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/17/2024 06:12:23 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/17/2024 06:12:23 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/17/2024 06:12:23 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
08/17/2024 06:12:23 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 128
08/17/2024 06:12:23 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
08/17/2024 06:12:23 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/17/2024 06:12:23 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/17/2024 06:12:23 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/17/2024 06:12:23 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
08/17/2024 06:12:56 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 585.9220 | train_loss 7.426081e-01 | lr 2.500000e-05
08/17/2024 06:13:23 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 526.7322 | train_loss 4.033739e-01 | lr 5.000000e-05
08/17/2024 06:13:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 527.3730 | train_loss 3.116383e-01 | lr 4.166667e-05
08/17/2024 06:14:17 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 527.8716 | train_loss 2.876156e-01 | lr 3.333333e-05
08/17/2024 06:14:17 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmplva24am8 at global_step 200 ****
08/17/2024 06:14:18 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 06:14:47 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 522.9182 | train_loss 2.694991e-01 | lr 2.500000e-05
08/17/2024 06:15:15 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 531.8209 | train_loss 2.656873e-01 | lr 1.666667e-05
08/17/2024 06:15:42 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 532.1568 | train_loss 2.594246e-01 | lr 8.333333e-06
08/17/2024 06:16:10 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 533.2239 | train_loss 2.556829e-01 | lr 0.000000e+00
08/17/2024 06:16:10 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmplva24am8 at global_step 400 ****
08/17/2024 06:16:10 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 06:16:11 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968515/tmplva24am8
08/17/2024 06:16:11 - INFO - pecos.xmc.xtransformer.matcher - Predict on input text tensors(torch.Size([14146, 256])) in OVA mode
08/17/2024 06:16:11 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
08/17/2024 06:16:57 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/17/2024 06:16:57 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/17/2024 06:17:04 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/17/2024 06:17:08 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 1, nr_labels=2048, avr_M_nnz=20.443234836702956
08/17/2024 06:17:09 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
08/17/2024 06:17:20 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23968515/tmp2jpm2658/X_trn.pt
08/17/2024 06:17:20 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/17/2024 06:17:33 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/17/2024 06:17:33 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/17/2024 06:17:33 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/17/2024 06:17:33 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/17/2024 06:17:34 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/17/2024 06:17:34 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
08/17/2024 06:17:34 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 2048
08/17/2024 06:17:34 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 448
08/17/2024 06:17:34 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 2
08/17/2024 06:17:34 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/17/2024 06:17:34 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/17/2024 06:17:34 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/17/2024 06:17:34 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 400
08/17/2024 06:18:05 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][    50/   400] |   49/ 222 batches | ms/batch 532.3452 | train_loss 6.265204e-01 | lr 2.500000e-05
08/17/2024 06:18:32 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   100/   400] |   99/ 222 batches | ms/batch 531.8388 | train_loss 5.100898e-01 | lr 5.000000e-05
08/17/2024 06:19:00 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   150/   400] |  149/ 222 batches | ms/batch 531.8051 | train_loss 4.734766e-01 | lr 4.166667e-05
08/17/2024 06:19:28 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   2][   200/   400] |  199/ 222 batches | ms/batch 532.0800 | train_loss 4.557305e-01 | lr 3.333333e-05
08/17/2024 06:19:28 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpc1in_9z5 at global_step 200 ****
08/17/2024 06:19:28 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 06:19:59 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   250/   400] |   27/ 222 batches | ms/batch 524.0551 | train_loss 4.442823e-01 | lr 2.500000e-05
08/17/2024 06:20:27 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   300/   400] |   77/ 222 batches | ms/batch 533.1542 | train_loss 4.355974e-01 | lr 1.666667e-05
08/17/2024 06:20:55 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   350/   400] |  127/ 222 batches | ms/batch 532.5109 | train_loss 4.298155e-01 | lr 8.333333e-06
08/17/2024 06:21:22 - INFO - pecos.xmc.xtransformer.matcher - | [   2/   2][   400/   400] |  177/ 222 batches | ms/batch 532.6980 | train_loss 4.277860e-01 | lr 0.000000e+00
08/17/2024 06:21:22 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpc1in_9z5 at global_step 400 ****
08/17/2024 06:21:23 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 06:21:24 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968515/tmpc1in_9z5
08/17/2024 06:21:24 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 2048)) with avr_nnz=320.0
08/17/2024 06:21:24 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
08/17/2024 06:22:11 - INFO - pecos.xmc.xtransformer.matcher - Concatenating instance embeddings with features...
08/17/2024 06:22:12 - INFO - pecos.xmc.xtransformer.matcher - Start training concat_model of transformer matcher...
08/17/2024 06:22:25 - INFO - pecos.xmc.xtransformer.matcher - Using concat-only for transformer/concat ensemble of pred_csr
08/17/2024 06:22:27 - INFO - pecos.xmc.xtransformer.model - Fine-tuning XR-Transformer with tfn+man at level 2, nr_labels=30938, avr_M_nnz=26.09416089353881
08/17/2024 06:22:30 - INFO - pecos.xmc.xtransformer.matcher - Downloaded xlnet-base-cased model from s3.
08/17/2024 06:22:42 - INFO - pecos.xmc.xtransformer.matcher - trn tensors loaded_from /scratch/slurm_tmpdir/job_23968515/tmp2jpm2658/X_trn.pt
08/17/2024 06:22:42 - INFO - pecos.xmc.xtransformer.matcher - Continue training form given text_encoder!
08/17/2024 06:22:54 - INFO - pecos.xmc.xtransformer.matcher - Initialized transformer text_model with xlinear!
08/17/2024 06:22:54 - INFO - pecos.utils.torch_util - Setting device to cuda, number of active GPUs: 2
08/17/2024 06:22:55 - INFO - pecos.xmc.xtransformer.matcher - Start fine-tuning transformer matcher...
08/17/2024 06:22:55 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
/home/ul/ul_student/ul_ruw26/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/17/2024 06:22:56 - INFO - pecos.xmc.xtransformer.matcher - ***** Running training *****
08/17/2024 06:22:56 - INFO - pecos.xmc.xtransformer.matcher -   Num examples = 14146
08/17/2024 06:22:56 - INFO - pecos.xmc.xtransformer.matcher -   Num labels = 30938
08/17/2024 06:22:56 - INFO - pecos.xmc.xtransformer.matcher -   Num active labels per instance = 648
08/17/2024 06:22:56 - INFO - pecos.xmc.xtransformer.matcher -   Num Epochs = 1
08/17/2024 06:22:56 - INFO - pecos.xmc.xtransformer.matcher -   Learning Rate Schedule = linear
08/17/2024 06:22:56 - INFO - pecos.xmc.xtransformer.matcher -   Batch size = 64
08/17/2024 06:22:56 - INFO - pecos.xmc.xtransformer.matcher -   Gradient Accumulation steps = 1
08/17/2024 06:22:56 - INFO - pecos.xmc.xtransformer.matcher -   Total optimization steps = 200
08/17/2024 06:23:26 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][    50/   200] |   49/ 222 batches | ms/batch 533.6170 | train_loss 5.175586e-01 | lr 2.500000e-05
08/17/2024 06:23:54 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   100/   200] |   99/ 222 batches | ms/batch 533.1387 | train_loss 5.056437e-01 | lr 5.000000e-05
08/17/2024 06:24:22 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   150/   200] |  149/ 222 batches | ms/batch 533.5570 | train_loss 5.025583e-01 | lr 2.500000e-05
08/17/2024 06:24:50 - INFO - pecos.xmc.xtransformer.matcher - | [   1/   1][   200/   200] |  199/ 222 batches | ms/batch 533.1048 | train_loss 4.898114e-01 | lr 0.000000e+00
08/17/2024 06:24:50 - INFO - pecos.xmc.xtransformer.matcher - | **** saving model (avg_prec=0) to /scratch/slurm_tmpdir/job_23968515/tmpx98jwmgm at global_step 200 ****
08/17/2024 06:24:50 - INFO - pecos.xmc.xtransformer.matcher - -----------------------------------------------------------------------------------------
08/17/2024 06:24:51 - INFO - pecos.xmc.xtransformer.matcher - Reload the best checkpoint from /scratch/slurm_tmpdir/job_23968515/tmpx98jwmgm
08/17/2024 06:24:52 - INFO - pecos.xmc.xtransformer.matcher - Predict with csr_codes_next((14146, 30938)) with avr_nnz=302.09698854799944
08/17/2024 06:24:52 - INFO - pecos.xmc.xtransformer.module - Constructed XMCTextTensorizer, tokenized=True, len=14146
08/17/2024 06:25:40 - INFO - pecos.xmc.xtransformer.model - Constructed instance feature matrix with shape=(14146, 102706)
08/17/2024 06:25:44 - INFO - pecos.xmc.xtransformer.model - Hierarchical label tree for ranker: [8, 128, 2048, 30938]
08/17/2024 06:25:44 - INFO - pecos.xmc.xtransformer.model - Start training ranker...
08/17/2024 06:25:44 - INFO - pecos.xmc.base - Training Layer 0 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
08/17/2024 06:25:45 - INFO - pecos.xmc.base - Training Layer 1 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
08/17/2024 06:25:48 - INFO - pecos.xmc.base - Training Layer 2 of 4 Layers in HierarchicalMLModel, neg_mining=tfn..
08/17/2024 06:25:56 - INFO - pecos.xmc.base - Training Layer 3 of 4 Layers in HierarchicalMLModel, neg_mining=tfn+man..
08/17/2024 06:26:59 - INFO - pecos.xmc.xtransformer.model - Parameters saved to models/wiki10-31k/xlnet/param.json
08/17/2024 06:27:04 - INFO - pecos.xmc.xtransformer.model - Model saved to models/wiki10-31k/xlnet
==== evaluation results ====
param: bert
prec   = 87.94 84.42 79.80 74.54 69.64 65.31 61.50 58.08 54.97 52.32 49.79 47.55 45.51 43.67 41.96 40.38 38.95 37.65 36.44 35.34
recall = 5.24 10.00 14.04 17.27 20.03 22.41 24.49 26.29 27.89 29.38 30.67 31.85 32.95 33.98 34.91 35.79 36.60 37.38 38.14 38.86
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 37.59
param: roberta
prec   = 87.97 84.42 79.48 74.12 69.17 64.77 60.87 57.47 54.36 51.66 49.22 47.09 45.00 43.21 41.57 40.05 38.68 37.41 36.23 35.14
recall = 5.25 9.99 13.96 17.19 19.90 22.20 24.20 26.02 27.57 29.03 30.34 31.58 32.62 33.66 34.61 35.51 36.36 37.17 37.93 38.67
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 37.35
param: xlnet
prec   = 87.97 84.39 79.50 74.44 69.70 65.36 61.46 58.15 55.02 52.26 49.85 47.65 45.58 43.73 42.05 40.48 39.04 37.74 36.59 35.45
recall = 5.25 9.98 13.98 17.28 20.07 22.44 24.49 26.36 27.95 29.39 30.73 31.93 32.99 34.02 34.97 35.84 36.66 37.46 38.29 39.00
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 37.70
==== rank_average ensemble results ====
prec   = 88.12 84.87 80.39 75.30 70.74 66.54 62.91 59.64 56.61 53.82 51.41 49.17 47.09 45.17 43.47 41.90 40.43 39.14 37.89 36.76
recall = 5.26 10.05 14.14 17.47 20.35 22.84 25.05 26.98 28.70 30.20 31.63 32.89 34.03 35.09 36.12 37.03 37.92 38.80 39.57 40.34
psp    = 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
r_prec = 38.93

============================= JOB FEEDBACK =============================

NodeName=uc2n910
Job ID: 23968515
Cluster: uc2
User/Group: ul_ruw26/ul_student
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 32
CPU Utilized: 05:10:55
CPU Efficiency: 17.23% of 1-06:04:16 core-walltime
Job Wall-clock time: 00:56:23
Memory Utilized: 28.21 GB
Memory Efficiency: 96.27% of 29.30 GB
